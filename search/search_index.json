{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LevelDB WIKI LevelDB WIKI is a summary of what students have studied LevelDB through the 2022 LevelDB study hosted by the DKU System Software Lab . It includes code analysis of LevelDB, benchmark experiment reports, and tuning contest reports. The Korean version can be found on the DKU System Software Lab website . If there are any errors or additions in the document content, please feel free to contribute through a Pull Request . Authors Student: 1.WAL/Manifest: Isu Kim , Seyeon Park , Suhwan Shin 2.Memtable: Taegyu Cho , Zhu Yongjie 3.Compaction: Sangwoo Kang , Seoyoung Park , Zhao Guangxun 4.SSTable: Jongki Park , Sanghyun Cho 5.Bloom Filter: Hansu Kim 6.Cache: Seungwon Ha , Subin Hong Assistant: Min-guk Choi Professor: Jongmoo Choi , Seehwan Yoo Photo Poster References 1. Documents LevelDB Document RocksDB Wiki Jongmoo Choi,\u300eKey-Value Store: Database for Unstructured Bigdata\u300f, 2021 Fenggang Wu, \u300eLevelDB Introduction\u300f, 2016 rjl493456442, \u300eleveldb-handbook (CHS)\u300f, 2022 rsy56640, \u300eread_and_analyse_levelDB (CHS)\u300f FOCUS,\u300eLevelDB fully parsed (CHS)\u300f bloomingTony, \u300eResearch on Network and Storage Technology(CHS)\u300f \u6728\u9e1f\u6742\u8bb0,\u300eTalking about LevelDB data structure (CHS)\u300f, 2021 2. Lecture Jongmoo Choi, \u300eKey-Value Store: Database for Unstructured Bigdata (KOR)\u300f, 2021 GL Tech Tutorials, \u300eLSM trees\u300f, 2021 Wei Zhou, LevelDB YouTube playlist","title":"Home"},{"location":"#leveldb-wiki","text":"LevelDB WIKI is a summary of what students have studied LevelDB through the 2022 LevelDB study hosted by the DKU System Software Lab . It includes code analysis of LevelDB, benchmark experiment reports, and tuning contest reports. The Korean version can be found on the DKU System Software Lab website . If there are any errors or additions in the document content, please feel free to contribute through a Pull Request .","title":"LevelDB WIKI"},{"location":"#authors","text":"Student: 1.WAL/Manifest: Isu Kim , Seyeon Park , Suhwan Shin 2.Memtable: Taegyu Cho , Zhu Yongjie 3.Compaction: Sangwoo Kang , Seoyoung Park , Zhao Guangxun 4.SSTable: Jongki Park , Sanghyun Cho 5.Bloom Filter: Hansu Kim 6.Cache: Seungwon Ha , Subin Hong Assistant: Min-guk Choi Professor: Jongmoo Choi , Seehwan Yoo","title":"Authors"},{"location":"#photo","text":"","title":"Photo"},{"location":"#poster","text":"","title":"Poster"},{"location":"#references","text":"","title":"References"},{"location":"#1-documents","text":"LevelDB Document RocksDB Wiki Jongmoo Choi,\u300eKey-Value Store: Database for Unstructured Bigdata\u300f, 2021 Fenggang Wu, \u300eLevelDB Introduction\u300f, 2016 rjl493456442, \u300eleveldb-handbook (CHS)\u300f, 2022 rsy56640, \u300eread_and_analyse_levelDB (CHS)\u300f FOCUS,\u300eLevelDB fully parsed (CHS)\u300f bloomingTony, \u300eResearch on Network and Storage Technology(CHS)\u300f \u6728\u9e1f\u6742\u8bb0,\u300eTalking about LevelDB data structure (CHS)\u300f, 2021","title":"1. Documents"},{"location":"#2-lecture","text":"Jongmoo Choi, \u300eKey-Value Store: Database for Unstructured Bigdata (KOR)\u300f, 2021 GL Tech Tutorials, \u300eLSM trees\u300f, 2021 Wei Zhou, LevelDB YouTube playlist","title":"2. Lecture"},{"location":"analysis/cache/","text":"Cache Contents 1. What is Cache 2. What is LRU 3. LRU Cache Structure in LevelDB 4. Overall Cache Flow 1. What is Cache Cache is hardware or software that temporarily stores data in computing environments. It is a smaller, faster, and more expensive memory used to improve the performance of recently or frequently accessed data. There are several types of caches: CPU Cache : Memory mounted next to the CPU Disk Cache : Memory built into hard disks for disk control and external interface Other Caches : These are the caches we need to understand in LevelDB. Caches other than the two mentioned above are mainly managed through software, such as page cache where the operating system's main memory is copied to the hard disk. 2. What is LRU LRU(Least Recently Used) policy is a page replacement algorithm, which means that the least recently used data will be removed. The basic assumption of this algorithm is that the data that has not been used for the longest time is unlikely to be used in the future. LRU Cache works by removing the least recently used item when the cache is full and placing a new node. LRU Cache can be implemented using double linked list . Below is a diagram explaining LRU Cache. The basic assumption is that the data closer to Head (rightmost node) is recently used and the data closer to Tail (leftmost node) is old. When accessing cached data, move the data to Head to indicate that it is recently used and remove it from the priority queue. The order of reading data is A, B, C, D, E, D, F. The numbers in parentheses indicate sorting. The smaller the number, the closer the data is to Tail. When E is read, the cache is full, so the oldest data A is removed. Continue reading D and F, and you can see that the node with the least access count is removed and replaced. LevelDB uses two different data structures for LRU Cache: HashTable (HandleTable) : Used for lookup Double Linked List : Used for removing old data 3. LRU Cache Structure in LevelDB To understand the LRU Cache structure in LevelDB accurately, we analyzed the source code of LevelDB through Git clone. (source code download) git clone --recurse-submodules https://github.com/google/leveldb.git leveldb_release Below is a rough diagram of the overall LRU Cache structure. (For accurate code analysis, please refer to leveldb_release/build/util/cache.cc .) (1) ShardedLRUCache ShardedLRUCache in LevelDB consists of 16 LRU Caches internally and is defined as the external LRUCache. Sharding Sharding means dividing data into pieces and managing them. It is a kind of load balancing technique that distributes workload requests to multiple servers or processing devices. It means dividing a large database or network system into multiple small pieces and distributing them for storage and management. Below is a diagram showing how to divide a table into different tables. LevelDB uses LRU Cache through Sharding technique. ShardedLRUCache maintains an array of LRU Caches internally. When calling LRU Caches in ShardedLRUCache, Lock is performed for each LRU Cache to prevent multiple threads from accessing the same LRU Cache object. (2) LRUCache Each Item in LRUCache is an LRUHandle, which will be described in (3) LRUHandle. LRUCache is managed by 1 HandleTable (HashTable) and 2 Double Linked Lists. LRUHandles are managed in both Double Linked List and HashTable HandleTable. Below are the three elements that make up the Cache: Double Linked List and HashTable. LRUHandle lru_; // will be described (5). LRUHandle in_use_; // will be described (6). HandleTable table_; // will be described (4). (3) LRUHandle LRUCache's object. HandleTable (HashTable) implements fast lookup by combining with Double Linked List and Double Linked List implements fast addition and deletion. LRUHandle's components: struct LRUHandle { void* value; void (*deleter)(const Slice&, void* value); // to free the key and value space LRUHandle* next_hash; // handle hash collisions and keep objects (LRUHandle) belonging to the array LRUHandle* next; // used to maintain LRU order in doubly linked list LRUHandle* prev; // points to the previous node in a doubly linked list size_t charge; size_t key_length; // length of key value bool in_cache; // whether the handle is in the cache table uint32_t refs; // the number of times the handle was referenced uint32_t hash; // hash value of key used to determine sharding and fast comparison char key_data[1]; // start of key }; (4) HandleTable It is a very simple hash table. Declare a double pointer list_ in the code to manage LRUHandle objects. uint32_t length_; // length of LRUHandle* array uint32_t elems_; // number of nodes in the table LRUHandle** list_; Below is a diagram showing LRUHandle** list_ as LRUHandle* list_ . Below are the LRUHandle methods for the HandleTable class. LRUHandle** FindPointer(const Slice& key, uint32_t hash); // use next_hash to traverse the double linked lists until find the item corresponding to the key. // if no match is found, next_hash points to the end of list_ and the value of next_hash is nullptr. // if a match is found, a double pointer (LRUHandle) pointed to by next_hash is returned. // due to the above characteristics, it is used in the first line of Lookup, Insert, and Remove function. LRUHandle* Lookup(const Slice& key, uint32_t hash); // directly read the value pointed to by the pointer (LRUHandle) by calling FindPointer LRUHandle* Insert(LRUHandle* h); // call FindPointer to put LRUHandle at the location pointed to by the pointer (next_hash). LRUHandle* Remove(const Slice& key, uint32_t hash); // directly deletes the item pointed to by the pointer (next_hash) by calling FindPointer void Resize(); // dynamically expand array as LRUHandle grows (5) in_use_ and (6) lru_ Each of them is a data structure that makes up the LRU Cache: Double Linked List. They separate hot and cold data and maintain an internal HashTable. Hot data is frequently accessed, while cold data is rarely accessed. (5) in_use_ Hot linked list, which maintains the cached object for use by the caller. (6) lru_ Cold linked list, which maintains the popularity of cached objects. Both of these double linked lists are managed by the refs variable. in_use_ has refs of 2 because it is referenced by external users, and lru_ has refs of 1 because it is not referenced. Here, lru_ and in_use_ are not actual nodes; they do not store data themselves. Data is always inserted into the lru_ list first. (Important, data is always inserted into the lru_ list first.) lru_->next (next pointer) points to the oldest data, and lru_->prev (previous pointer) points to the newest data. When a new Handle is inserted, it is inserted at the end of the double linked list, and lru_->prev (previous pointer) points to the newly inserted Handle. When a cache is accessed by external users, it goes from lru_ to next_hash to in_use_ , and refs increases from 1 to 2. 4. Overall Cache Flow LevelDB Cache Mechanism In LevelDB, Cache is checked when reading data, and the mechanism is shown in the diagram below. The function Get is called when reading data. leveldb::Status s = db->Get(leveldb::ReadOptions(), key, &value); Part of the source code of DBImpl::Get : <1> MemTable* mem = mem_; <2> MemTable* imm = imm_; <3> Version* current = versions_->current(); if (mem->Get(lkey, value, &s)) { <4> } else if (imm != nullptr && imm->Get(lkey, value, &s)) { <5> } else { <6> s = current->Get(options, lkey, value, &stats); have_stat_update = true; } Declare memtable mem_ to find the table in memtable If not found in memtable, declare memtable imm_ to find the table in immutable memtable If not found in both, declare version current to get the current version to find the table in sstable Call Memtable::Get to read the table from disk Similarly, call Memtable::Get to read the table from immutable memtable Call Version::Get to read the table from sstable Version::Get calls TableCache::Get to read the table. LevelDB Cache Usage TableCache::Get is the part where LevelDB uses Cache. TableCache::Get source code: Cache::Handle* handle = nullptr; <1> Status s = FindTable(file_number, file_size, &handle); if (s.ok()) { <2> Table* t = reinterpret_cast<TableAndFile*>(cache_->Value(handle))->table; s = t->InternalGet(options, k, arg, handle_result); cache_->Release(handle); } return s; +LevelDB has two different caches: TableCache and BlockCache . <1> TableCache Usage (FindTable) Find the table corresponding to the file in the cache. If not found, open the file first and then create the table corresponding to the file and add it to the cache. Below is the structure of TableCache. key is the file number of the SSTable. value is divided into two sections: RandomAccessFile : Pointer to the opened SSTable on disk Table : Pointer to the Table structure corresponding to the SSTable in memory, which stores the cache_id of the SSTable and the block cache <2> BlockCache Usage (InternalGet -> BlockReader) The InternalGet function that implements the logic of finding Key in the SSTable calls BlockReader. BlockCache is used when the block is found in the BlockCache; if not found, the file is opened and read, and then the block is added to the BlockCache. BlockReader Cache Usage Algorithm: Try to get the block directly from the BlockCache. If not found in the BlockCache, call ReadBlock to read from the file. If reading is successful, add the block to the BlockCache. Below is the structure of BlockCache. key is composed of the offset of each SSTable file and a unique cache_id to distinguish it from other SSTable files. value is composed of Data blocks of the opened SSTable file. After BlockReader finishes looking up in the BlockCache and inserting into the BlockCache, the read operation finishes using the Cache. Below is a rough diagram of how to use Cache in LevelDB. LevelDB Cache Creation TableCache is created using the cache_size option in ./db_bench and set using the TableCache::TableCacheSize function. TableCache::TableCache BackTrace: leveldb::Benchmark::Run leveldb::Benchmark::Open leveldb::DB::Open leveldb::DBImpl::DBImpl leveldb::TableCache::TableCache","title":"Cache"},{"location":"analysis/cache/#cache","text":"","title":"Cache"},{"location":"analysis/cache/#contents","text":"1. What is Cache 2. What is LRU 3. LRU Cache Structure in LevelDB 4. Overall Cache Flow","title":"Contents"},{"location":"analysis/cache/#1-what-is-cache","text":"Cache is hardware or software that temporarily stores data in computing environments. It is a smaller, faster, and more expensive memory used to improve the performance of recently or frequently accessed data. There are several types of caches: CPU Cache : Memory mounted next to the CPU Disk Cache : Memory built into hard disks for disk control and external interface Other Caches : These are the caches we need to understand in LevelDB. Caches other than the two mentioned above are mainly managed through software, such as page cache where the operating system's main memory is copied to the hard disk.","title":"1. What is Cache"},{"location":"analysis/cache/#2-what-is-lru","text":"LRU(Least Recently Used) policy is a page replacement algorithm, which means that the least recently used data will be removed. The basic assumption of this algorithm is that the data that has not been used for the longest time is unlikely to be used in the future. LRU Cache works by removing the least recently used item when the cache is full and placing a new node. LRU Cache can be implemented using double linked list . Below is a diagram explaining LRU Cache. The basic assumption is that the data closer to Head (rightmost node) is recently used and the data closer to Tail (leftmost node) is old. When accessing cached data, move the data to Head to indicate that it is recently used and remove it from the priority queue. The order of reading data is A, B, C, D, E, D, F. The numbers in parentheses indicate sorting. The smaller the number, the closer the data is to Tail. When E is read, the cache is full, so the oldest data A is removed. Continue reading D and F, and you can see that the node with the least access count is removed and replaced. LevelDB uses two different data structures for LRU Cache: HashTable (HandleTable) : Used for lookup Double Linked List : Used for removing old data","title":"2. What is LRU"},{"location":"analysis/cache/#3-lru-cache-structure-in-leveldb","text":"To understand the LRU Cache structure in LevelDB accurately, we analyzed the source code of LevelDB through Git clone. (source code download) git clone --recurse-submodules https://github.com/google/leveldb.git leveldb_release Below is a rough diagram of the overall LRU Cache structure. (For accurate code analysis, please refer to leveldb_release/build/util/cache.cc .)","title":"3. LRU Cache Structure in LevelDB"},{"location":"analysis/cache/#1-shardedlrucache","text":"ShardedLRUCache in LevelDB consists of 16 LRU Caches internally and is defined as the external LRUCache.","title":"(1) ShardedLRUCache"},{"location":"analysis/cache/#sharding","text":"Sharding means dividing data into pieces and managing them. It is a kind of load balancing technique that distributes workload requests to multiple servers or processing devices. It means dividing a large database or network system into multiple small pieces and distributing them for storage and management. Below is a diagram showing how to divide a table into different tables. LevelDB uses LRU Cache through Sharding technique. ShardedLRUCache maintains an array of LRU Caches internally. When calling LRU Caches in ShardedLRUCache, Lock is performed for each LRU Cache to prevent multiple threads from accessing the same LRU Cache object.","title":"Sharding"},{"location":"analysis/cache/#2-lrucache","text":"Each Item in LRUCache is an LRUHandle, which will be described in (3) LRUHandle. LRUCache is managed by 1 HandleTable (HashTable) and 2 Double Linked Lists. LRUHandles are managed in both Double Linked List and HashTable HandleTable. Below are the three elements that make up the Cache: Double Linked List and HashTable. LRUHandle lru_; // will be described (5). LRUHandle in_use_; // will be described (6). HandleTable table_; // will be described (4).","title":"(2) LRUCache"},{"location":"analysis/cache/#3-lruhandle","text":"LRUCache's object. HandleTable (HashTable) implements fast lookup by combining with Double Linked List and Double Linked List implements fast addition and deletion. LRUHandle's components: struct LRUHandle { void* value; void (*deleter)(const Slice&, void* value); // to free the key and value space LRUHandle* next_hash; // handle hash collisions and keep objects (LRUHandle) belonging to the array LRUHandle* next; // used to maintain LRU order in doubly linked list LRUHandle* prev; // points to the previous node in a doubly linked list size_t charge; size_t key_length; // length of key value bool in_cache; // whether the handle is in the cache table uint32_t refs; // the number of times the handle was referenced uint32_t hash; // hash value of key used to determine sharding and fast comparison char key_data[1]; // start of key };","title":"(3) LRUHandle"},{"location":"analysis/cache/#4-handletable","text":"It is a very simple hash table. Declare a double pointer list_ in the code to manage LRUHandle objects. uint32_t length_; // length of LRUHandle* array uint32_t elems_; // number of nodes in the table LRUHandle** list_; Below is a diagram showing LRUHandle** list_ as LRUHandle* list_ . Below are the LRUHandle methods for the HandleTable class. LRUHandle** FindPointer(const Slice& key, uint32_t hash); // use next_hash to traverse the double linked lists until find the item corresponding to the key. // if no match is found, next_hash points to the end of list_ and the value of next_hash is nullptr. // if a match is found, a double pointer (LRUHandle) pointed to by next_hash is returned. // due to the above characteristics, it is used in the first line of Lookup, Insert, and Remove function. LRUHandle* Lookup(const Slice& key, uint32_t hash); // directly read the value pointed to by the pointer (LRUHandle) by calling FindPointer LRUHandle* Insert(LRUHandle* h); // call FindPointer to put LRUHandle at the location pointed to by the pointer (next_hash). LRUHandle* Remove(const Slice& key, uint32_t hash); // directly deletes the item pointed to by the pointer (next_hash) by calling FindPointer void Resize(); // dynamically expand array as LRUHandle grows","title":"(4) HandleTable"},{"location":"analysis/cache/#5-in_use_-and-6-lru_","text":"Each of them is a data structure that makes up the LRU Cache: Double Linked List. They separate hot and cold data and maintain an internal HashTable. Hot data is frequently accessed, while cold data is rarely accessed.","title":"(5) in_use_ and (6) lru_"},{"location":"analysis/cache/#5-in_use_","text":"Hot linked list, which maintains the cached object for use by the caller.","title":"(5) in_use_"},{"location":"analysis/cache/#6-lru_","text":"Cold linked list, which maintains the popularity of cached objects. Both of these double linked lists are managed by the refs variable. in_use_ has refs of 2 because it is referenced by external users, and lru_ has refs of 1 because it is not referenced. Here, lru_ and in_use_ are not actual nodes; they do not store data themselves. Data is always inserted into the lru_ list first. (Important, data is always inserted into the lru_ list first.) lru_->next (next pointer) points to the oldest data, and lru_->prev (previous pointer) points to the newest data. When a new Handle is inserted, it is inserted at the end of the double linked list, and lru_->prev (previous pointer) points to the newly inserted Handle. When a cache is accessed by external users, it goes from lru_ to next_hash to in_use_ , and refs increases from 1 to 2.","title":"(6) lru_"},{"location":"analysis/cache/#4-overall-cache-flow","text":"LevelDB Cache Mechanism In LevelDB, Cache is checked when reading data, and the mechanism is shown in the diagram below. The function Get is called when reading data. leveldb::Status s = db->Get(leveldb::ReadOptions(), key, &value); Part of the source code of DBImpl::Get : <1> MemTable* mem = mem_; <2> MemTable* imm = imm_; <3> Version* current = versions_->current(); if (mem->Get(lkey, value, &s)) { <4> } else if (imm != nullptr && imm->Get(lkey, value, &s)) { <5> } else { <6> s = current->Get(options, lkey, value, &stats); have_stat_update = true; } Declare memtable mem_ to find the table in memtable If not found in memtable, declare memtable imm_ to find the table in immutable memtable If not found in both, declare version current to get the current version to find the table in sstable Call Memtable::Get to read the table from disk Similarly, call Memtable::Get to read the table from immutable memtable Call Version::Get to read the table from sstable Version::Get calls TableCache::Get to read the table. LevelDB Cache Usage TableCache::Get is the part where LevelDB uses Cache. TableCache::Get source code: Cache::Handle* handle = nullptr; <1> Status s = FindTable(file_number, file_size, &handle); if (s.ok()) { <2> Table* t = reinterpret_cast<TableAndFile*>(cache_->Value(handle))->table; s = t->InternalGet(options, k, arg, handle_result); cache_->Release(handle); } return s; +LevelDB has two different caches: TableCache and BlockCache .","title":"4. Overall Cache Flow"},{"location":"analysis/cache/#1-tablecache-usage-findtable","text":"Find the table corresponding to the file in the cache. If not found, open the file first and then create the table corresponding to the file and add it to the cache. Below is the structure of TableCache. key is the file number of the SSTable. value is divided into two sections: RandomAccessFile : Pointer to the opened SSTable on disk Table : Pointer to the Table structure corresponding to the SSTable in memory, which stores the cache_id of the SSTable and the block cache","title":"&lt;1&gt; TableCache Usage (FindTable)"},{"location":"analysis/cache/#2-blockcache-usage-internalget-blockreader","text":"The InternalGet function that implements the logic of finding Key in the SSTable calls BlockReader. BlockCache is used when the block is found in the BlockCache; if not found, the file is opened and read, and then the block is added to the BlockCache. BlockReader Cache Usage Algorithm: Try to get the block directly from the BlockCache. If not found in the BlockCache, call ReadBlock to read from the file. If reading is successful, add the block to the BlockCache. Below is the structure of BlockCache. key is composed of the offset of each SSTable file and a unique cache_id to distinguish it from other SSTable files. value is composed of Data blocks of the opened SSTable file. After BlockReader finishes looking up in the BlockCache and inserting into the BlockCache, the read operation finishes using the Cache. Below is a rough diagram of how to use Cache in LevelDB. LevelDB Cache Creation TableCache is created using the cache_size option in ./db_bench and set using the TableCache::TableCacheSize function. TableCache::TableCache BackTrace: leveldb::Benchmark::Run leveldb::Benchmark::Open leveldb::DB::Open leveldb::DBImpl::DBImpl leveldb::TableCache::TableCache","title":"&lt;2&gt; BlockCache Usage (InternalGet -&gt; BlockReader)"},{"location":"analysis/manifest/","text":"MANIFEST MANIFEST handles the organization of VersionSet and VersionEdit into files. It is stored with names like MANIFEST-000000 . The MANIFEST filename changes during LevelDB operations. The CURRENT file contains the name of the MANIFEST file that LevelDB will use in the current session. VersionSet and VersionEdit Both VersionSet and VersionEdit manage version-related information. VersionSet and VersionEdit are friend classes to each other. Each class has one specific function for writing to and using the MANIFEST file: VersionSet::LogAndApply : Prepares version data for storage in MANIFEST and ultimately writes the data using VersionEdit::EncodeTo to save it to the MANIFEST file. VersionEdit::EncodeTo : Writes the actual data in binary format. During DB Creation When initially creating the DB, the MANIFEST file is generated through the above process. Initially, a VersionEdit is created with the following values and written to the MANIFEST-000001 file: comparator : \"leveldb.BytewiseComparator\" log_number : 0 next_file_number : 2 last_sequence : 0 These contents are then recorded through the log::Writer function. During DB Shutdown When LevelDB shuts down, ~DBImpl is called. This deletes the existing VersionSet and writes the data about the current version to the MANIFEST file. This written MANIFEST file allows LevelDB to recover the previous version when it starts up next time.","title":"Manifest"},{"location":"analysis/manifest/#manifest","text":"MANIFEST handles the organization of VersionSet and VersionEdit into files. It is stored with names like MANIFEST-000000 . The MANIFEST filename changes during LevelDB operations. The CURRENT file contains the name of the MANIFEST file that LevelDB will use in the current session.","title":"MANIFEST"},{"location":"analysis/manifest/#versionset-and-versionedit","text":"Both VersionSet and VersionEdit manage version-related information. VersionSet and VersionEdit are friend classes to each other. Each class has one specific function for writing to and using the MANIFEST file: VersionSet::LogAndApply : Prepares version data for storage in MANIFEST and ultimately writes the data using VersionEdit::EncodeTo to save it to the MANIFEST file. VersionEdit::EncodeTo : Writes the actual data in binary format.","title":"VersionSet and VersionEdit"},{"location":"analysis/manifest/#during-db-creation","text":"When initially creating the DB, the MANIFEST file is generated through the above process. Initially, a VersionEdit is created with the following values and written to the MANIFEST-000001 file: comparator : \"leveldb.BytewiseComparator\" log_number : 0 next_file_number : 2 last_sequence : 0 These contents are then recorded through the log::Writer function.","title":"During DB Creation"},{"location":"analysis/manifest/#during-db-shutdown","text":"When LevelDB shuts down, ~DBImpl is called. This deletes the existing VersionSet and writes the data about the current version to the MANIFEST file. This written MANIFEST file allows LevelDB to recover the previous version when it starts up next time.","title":"During DB Shutdown"},{"location":"analysis/memtable/","text":"Memtable Memtable can be viewed as a memory copy of the log. Its main role is to store log data in a structured manner. (source: https://www.jianshu.com/p/0e6116f23c3d) In LevelDB, when writing data to the DB, Memtable is the space where kv data is stored. When the data written to Memtable exceeds the specified size (Options:write_buffer_size), it converts to an Immutable Memtable, and simultaneously creates a new Memtable for storing data. When compaction occurs in the background, the Immutable Memtable is dumped to disk, creating an SSTable. Structure & Operation (source: https://blog.csdn.net/redfivehit/article/details/107509884) Klength & Vlength : Varint32, maximum 5 bytes SequenceNumber + ValueType : 8 bytes The Memtable key consists of four parts: Memtable key = key length + user key + value type + sequence number Memtable can store multiple versions of the same key. KeyComparator first compares user keys in ascending order, then sequence numbers in descending order to determine entries. The user key is placed first to manipulate the same user key with sequence numbers. MemTable provides Add and Get interfaces for KV entries in memory, but there is no actual Delete operation. Deleting a key's value is inserted as an entry with a deletion tag in Memtable, with actual deletion occurring during compaction. Memtable is an interface class built on arena and skiplist. Arena manages memory, while skiplist is used for actual KV storage. Component Arena: Memory Management for Memtable (source: http://mingxinglai.com/cn/2013/01/leveldb-arena/) Memory management for usage statistics: Memtable has a size limit (write_buffer_size). Memory is allocated through a unified interface. Memory alignment guarantee: Arena requests memory in kBlockSize units (static const int kBlockSize = 4096) and provides memory address alignment for efficiency. Uses Vector to store allocated memory. Prevention of inefficiency from frequent small memory block allocations and waste from large block allocations: When memtable requests memory, if size is less than kBlockSize / 4, it allocates from the current memory block; otherwise, it makes a new request (new). When Memtable reaches its limit, it dumps to disk and Arena immediately releases used memory. Memtable uses reference counting (refs): Even after Immutable Memtable completes disk dumping, it's not deleted if the reference count isn't 0. Skiplist: Actual Structure of Memtable (source: https://en.wikipedia.org/wiki/Skip_list) Performance bottlenecks can occur here due to the high cost of insert (random write) operations in sorted structures. LevelDB uses a lightweight Skiplist instead of complex B-trees. Skiplist is an alternative data structure to binary trees. It maintains probabilistic balance, has similar time complexity of O(logN), and enables space savings. Skiplist offers better concurrency performance than binary trees. While binary trees may require rebalancing during updates, Skiplist does not. LevelDB's Skiplist doesn't require locks or node references, implementing thread synchronization through Memory Barriers. Coding & Function Analysis (source: https://zhuanlan.zhihu.com/p/25300086) Construction and destruction: Memtable's object structure requires explicit calls. Arena initialization and Skiplist structure are key elements. The Memtable class releases memory through Unref() , and LevelDB prohibits copy constructors and assignment operators. Operation: Write : void Add(SequenceNumber seq, ValueType type, const Slice& key, const Slice& value) Status DBImpl::MakeRoomForWrite(bool force) Check level0 & mem_ : MakeRoomForWrite() checks if Memtable, Immutable Memtable, and Level 0 are full. If full, it performs flush/compaction and creates a new Memtable. Encode : Encapsulates passed variables into InternalKey and encodes them with value as an entry. Insert into data structure : Calls SkipList::Insert() to insert data. Read : bool Get(const LookupKey& key, std::string* value, Status* s) Obtains memtable_key from LookupKey. Calls MemTableIterator::Seek() to return MemTableIterator. Restores MemTableIterator's key and decodes the last 8 bytes to check type. a) If kTypeValue , returns value as valid data. b) If kTypeDeletion , sets Status to NotFound. Delete : No delete function exists; instead adds an entry with ValueType = kTypeDeletion .","title":"Memtable"},{"location":"analysis/memtable/#memtable","text":"Memtable can be viewed as a memory copy of the log. Its main role is to store log data in a structured manner. (source: https://www.jianshu.com/p/0e6116f23c3d) In LevelDB, when writing data to the DB, Memtable is the space where kv data is stored. When the data written to Memtable exceeds the specified size (Options:write_buffer_size), it converts to an Immutable Memtable, and simultaneously creates a new Memtable for storing data. When compaction occurs in the background, the Immutable Memtable is dumped to disk, creating an SSTable.","title":"Memtable"},{"location":"analysis/memtable/#structure-operation","text":"(source: https://blog.csdn.net/redfivehit/article/details/107509884) Klength & Vlength : Varint32, maximum 5 bytes SequenceNumber + ValueType : 8 bytes The Memtable key consists of four parts: Memtable key = key length + user key + value type + sequence number Memtable can store multiple versions of the same key. KeyComparator first compares user keys in ascending order, then sequence numbers in descending order to determine entries. The user key is placed first to manipulate the same user key with sequence numbers. MemTable provides Add and Get interfaces for KV entries in memory, but there is no actual Delete operation. Deleting a key's value is inserted as an entry with a deletion tag in Memtable, with actual deletion occurring during compaction. Memtable is an interface class built on arena and skiplist. Arena manages memory, while skiplist is used for actual KV storage.","title":"Structure &amp; Operation"},{"location":"analysis/memtable/#component","text":"","title":"Component"},{"location":"analysis/memtable/#arena-memory-management-for-memtable","text":"(source: http://mingxinglai.com/cn/2013/01/leveldb-arena/) Memory management for usage statistics: Memtable has a size limit (write_buffer_size). Memory is allocated through a unified interface. Memory alignment guarantee: Arena requests memory in kBlockSize units (static const int kBlockSize = 4096) and provides memory address alignment for efficiency. Uses Vector to store allocated memory. Prevention of inefficiency from frequent small memory block allocations and waste from large block allocations: When memtable requests memory, if size is less than kBlockSize / 4, it allocates from the current memory block; otherwise, it makes a new request (new). When Memtable reaches its limit, it dumps to disk and Arena immediately releases used memory. Memtable uses reference counting (refs): Even after Immutable Memtable completes disk dumping, it's not deleted if the reference count isn't 0.","title":"Arena: Memory Management for Memtable"},{"location":"analysis/memtable/#skiplist-actual-structure-of-memtable","text":"(source: https://en.wikipedia.org/wiki/Skip_list) Performance bottlenecks can occur here due to the high cost of insert (random write) operations in sorted structures. LevelDB uses a lightweight Skiplist instead of complex B-trees. Skiplist is an alternative data structure to binary trees. It maintains probabilistic balance, has similar time complexity of O(logN), and enables space savings. Skiplist offers better concurrency performance than binary trees. While binary trees may require rebalancing during updates, Skiplist does not. LevelDB's Skiplist doesn't require locks or node references, implementing thread synchronization through Memory Barriers.","title":"Skiplist: Actual Structure of Memtable"},{"location":"analysis/memtable/#coding-function-analysis","text":"(source: https://zhuanlan.zhihu.com/p/25300086)","title":"Coding &amp; Function Analysis"},{"location":"analysis/memtable/#construction-and-destruction","text":"Memtable's object structure requires explicit calls. Arena initialization and Skiplist structure are key elements. The Memtable class releases memory through Unref() , and LevelDB prohibits copy constructors and assignment operators.","title":"Construction and destruction:"},{"location":"analysis/memtable/#operation","text":"Write : void Add(SequenceNumber seq, ValueType type, const Slice& key, const Slice& value) Status DBImpl::MakeRoomForWrite(bool force) Check level0 & mem_ : MakeRoomForWrite() checks if Memtable, Immutable Memtable, and Level 0 are full. If full, it performs flush/compaction and creates a new Memtable. Encode : Encapsulates passed variables into InternalKey and encodes them with value as an entry. Insert into data structure : Calls SkipList::Insert() to insert data. Read : bool Get(const LookupKey& key, std::string* value, Status* s) Obtains memtable_key from LookupKey. Calls MemTableIterator::Seek() to return MemTableIterator. Restores MemTableIterator's key and decodes the last 8 bytes to check type. a) If kTypeValue , returns value as valid data. b) If kTypeDeletion , sets Status to NotFound. Delete : No delete function exists; instead adds an entry with ValueType = kTypeDeletion .","title":"Operation:"},{"location":"analysis/wal/","text":"WAL This document provides an explanation of WAL (Write Ahead Log). Index WAL : Overview of WAL Functions : Description of WAL/MANIFEST related functions Since MANIFEST and WAL share many common functions, they are documented together in the WAL section. ETC : Additional research findings WAL Introduction LevelDB does not have an option to enable/disable WAL. To activate or deactivate WAL, you need to either use RocksDB or modify LevelDB's source code. LevelDB uses WAL (Write Ahead Log) when writing data. WAL records logs of all transactions in LevelDB and is used to prevent transaction loss. In LevelDB, data is temporarily stored in the Memtable first. However, since Memtable exists only in memory, data stored in Memtable can be lost if the system terminates abnormally or encounters an error. Analysis of this is detailed in the ETC section. To prevent data loss, LevelDB stores transactions in a separate log file, which is the WAL. Format LevelDB's WAL is stored as a .log file in binary format. The WAL file stores two things: - Transaction header - Actual transaction data (payload) Header The WAL header is stored in the following format: Header components: 1. CRC Checksum (4byte) 2. Data size (2byte) 3. Record Type (1byte) Payload After the WAL header, the payload is stored in the following format. Example: When PUT-ting 3 pairs of Key-Value data like {\"A\": \"Hello world!\", \"B\": \"Good bye world!\", \"C\": \"I am hungry\"}, the WAL file looks like this: Functions Here are the findings from analyzing WAL and MANIFEST related functions. The log::reader file contains the Reader class and several functions. Analysis focused on the bool Reader::ReadRecord(Slice* record, std::string* scratch) function. The ReadRecord() function finds the initial block position ( SkipToInitialBlock() ), and runs a while loop to get return values from ReadPhysicalRecord() function to read data. This function reads the block's record type and handles any errors. The function terminates when it reaches the end of the block. If the data spans multiple blocks, it adds data to scratch and transfers everything to record at the last block. The in_fragmented_record variable is used to determine if data spans multiple blocks. log::Writer::AddRecord Receives slice type data and writes it to WAL. Calls log::Writer::EmitPhysicalRecord function for writing. log::Writer::EmitPhysicalRecord Receives slice type data, creates a header, and writes to file. Adds the header and writes to .log file through PosixWritableFile::Append() . PosixWritableFile::Append() Implementation of WritableFile in POSIX environment. WAL files are written through PosixWritableFile . Records slice data to buffer and writes to file when buffer is full. ETC Includes experimental results of data loss with WAL enabled/disabled. Since WAL options are only supported in RocksDB, experiments were conducted using RocksDB. Summary Experiments were conducted in the following situation: - After writing data with PUT command, forcefully terminate process ( SIGINT ) - Check for data loss before termination Design The code used in experiments can be found here . Scenarios Experiments were conducted with 4 scenarios: WAL Enabled Manual Flush Scenario 1 X X Scenario 2 X O Scenario 3 O X Scenario 4 O O Results Results are as follows: Scenario Data Integrity Scenario 1 Lost all data in Memtable Scenario 2 Lost some data before Manual Flush Scenario 3 Preserved all data Scenario 4 Preserved all data Additional experiments used these conditions: - Total 1,000,000 PUT commands executed - [0] Flush every 1,000, [1] Flush every 10,000 Type Speed Data Integrity WAL Disabled 1.843s Lost all data WAL Enabled 3.604s Preserved all data Manual Flush [0] & WAL Disabled 39.128s Lost some data Manual Flush [1] & WAL Disabled 5.611s Lost some data Bottom line Enabling WAL ensures data preservation Disabling WAL risks significant data loss Using Manual Flush with WAL disabled can preserve some data","title":"WAL"},{"location":"analysis/wal/#wal","text":"This document provides an explanation of WAL (Write Ahead Log).","title":"WAL"},{"location":"analysis/wal/#index","text":"WAL : Overview of WAL Functions : Description of WAL/MANIFEST related functions Since MANIFEST and WAL share many common functions, they are documented together in the WAL section. ETC : Additional research findings","title":"Index"},{"location":"analysis/wal/#wal_1","text":"","title":"WAL"},{"location":"analysis/wal/#introduction","text":"LevelDB does not have an option to enable/disable WAL. To activate or deactivate WAL, you need to either use RocksDB or modify LevelDB's source code. LevelDB uses WAL (Write Ahead Log) when writing data. WAL records logs of all transactions in LevelDB and is used to prevent transaction loss. In LevelDB, data is temporarily stored in the Memtable first. However, since Memtable exists only in memory, data stored in Memtable can be lost if the system terminates abnormally or encounters an error. Analysis of this is detailed in the ETC section. To prevent data loss, LevelDB stores transactions in a separate log file, which is the WAL.","title":"Introduction"},{"location":"analysis/wal/#format","text":"LevelDB's WAL is stored as a .log file in binary format. The WAL file stores two things: - Transaction header - Actual transaction data (payload)","title":"Format"},{"location":"analysis/wal/#header","text":"The WAL header is stored in the following format: Header components: 1. CRC Checksum (4byte) 2. Data size (2byte) 3. Record Type (1byte)","title":"Header"},{"location":"analysis/wal/#payload","text":"After the WAL header, the payload is stored in the following format. Example: When PUT-ting 3 pairs of Key-Value data like {\"A\": \"Hello world!\", \"B\": \"Good bye world!\", \"C\": \"I am hungry\"}, the WAL file looks like this:","title":"Payload"},{"location":"analysis/wal/#functions","text":"Here are the findings from analyzing WAL and MANIFEST related functions. The log::reader file contains the Reader class and several functions. Analysis focused on the bool Reader::ReadRecord(Slice* record, std::string* scratch) function. The ReadRecord() function finds the initial block position ( SkipToInitialBlock() ), and runs a while loop to get return values from ReadPhysicalRecord() function to read data. This function reads the block's record type and handles any errors. The function terminates when it reaches the end of the block. If the data spans multiple blocks, it adds data to scratch and transfers everything to record at the last block. The in_fragmented_record variable is used to determine if data spans multiple blocks.","title":"Functions"},{"location":"analysis/wal/#logwriteraddrecord","text":"Receives slice type data and writes it to WAL. Calls log::Writer::EmitPhysicalRecord function for writing.","title":"log::Writer::AddRecord"},{"location":"analysis/wal/#logwriteremitphysicalrecord","text":"Receives slice type data, creates a header, and writes to file. Adds the header and writes to .log file through PosixWritableFile::Append() .","title":"log::Writer::EmitPhysicalRecord"},{"location":"analysis/wal/#posixwritablefileappend","text":"Implementation of WritableFile in POSIX environment. WAL files are written through PosixWritableFile . Records slice data to buffer and writes to file when buffer is full.","title":"PosixWritableFile::Append()"},{"location":"analysis/wal/#etc","text":"Includes experimental results of data loss with WAL enabled/disabled. Since WAL options are only supported in RocksDB, experiments were conducted using RocksDB.","title":"ETC"},{"location":"analysis/wal/#summary","text":"Experiments were conducted in the following situation: - After writing data with PUT command, forcefully terminate process ( SIGINT ) - Check for data loss before termination","title":"Summary"},{"location":"analysis/wal/#design","text":"The code used in experiments can be found here .","title":"Design"},{"location":"analysis/wal/#scenarios","text":"Experiments were conducted with 4 scenarios: WAL Enabled Manual Flush Scenario 1 X X Scenario 2 X O Scenario 3 O X Scenario 4 O O","title":"Scenarios"},{"location":"analysis/wal/#results","text":"Results are as follows: Scenario Data Integrity Scenario 1 Lost all data in Memtable Scenario 2 Lost some data before Manual Flush Scenario 3 Preserved all data Scenario 4 Preserved all data Additional experiments used these conditions: - Total 1,000,000 PUT commands executed - [0] Flush every 1,000, [1] Flush every 10,000 Type Speed Data Integrity WAL Disabled 1.843s Lost all data WAL Enabled 3.604s Preserved all data Manual Flush [0] & WAL Disabled 39.128s Lost some data Manual Flush [1] & WAL Disabled 5.611s Lost some data","title":"Results"},{"location":"analysis/wal/#bottom-line","text":"Enabling WAL ensures data preservation Disabling WAL risks significant data loss Using Manual Flush with WAL disabled can preserve some data","title":"Bottom line"},{"location":"analysis/bloomfilter/bloomfilter-read/","text":"Bloom Filter Read The db_bench starts from the main function in the db_bench.cc file, reads parameters using scanf , and then executes the benchmark.Run() class function. This function sequentially executes the Open() function, which handles the write process, and the RunBenchmark() function, which handles the read process. The RunBenchmark() function requires three parameters: num_threads , name , and method . Among them, num_threads indicates the number of threads and can be specified as a parameter value when starting db_bench . Next, Name is a string variable that represents the type of benchmark. It stores the values entered in the parameters, separated by commas. Method is a pointer variable that stores the address of the benchmark function corresponding to Name . After that, the address value of the method in the RunBenchmark() function is stored in the arg[] array. The StartThread function is executed with this value and the ThreadBody() function as arguments. The ThreadBody() function performs the given benchmark with the allocated thread. The code flow of the benchmark is as follows: Starting with benchmark functions like ReadRandom() or ReadHot() , it gradually narrows the search range through multiple Get() functions, from the database to the sstable, and then to each level, table, filter block, and bloom filter. In the benchmark function, after performing the functions assigned to each benchmark, the db->Get() function is used. Here, db is the address value of the database used in the DB::open() function when opening the database during the write process. The DBImpl::Get() function sequentially searches the memtable, immemtable, and sstable. The function used to search the sstable is Version::Get() . (Note that the bloom filter is only used in the sstable.) The Version::Get() function executes the ForEachOverlapping() function with the Match() function as an argument. The ForEachOverlapping() function first searches level 0 of the sstable and then searches other levels. The Match() function used in this search process checks if a specific key exists in the table and returns a boolean value based on its existence. The TableCache::Get() function called by the Match() function searches the table. Then, FilterBlockReader::KeyMayMatch() searches the filter block, and BloomFilterPolicy::KeyMayMatch() searches the bloom filter to finally check the existence of a specific key. The InternalGet() function in the middle proceeds with the read if the key being searched for exists.","title":"Bloom Filter Read"},{"location":"analysis/bloomfilter/bloomfilter-read/#bloom-filter-read","text":"The db_bench starts from the main function in the db_bench.cc file, reads parameters using scanf , and then executes the benchmark.Run() class function. This function sequentially executes the Open() function, which handles the write process, and the RunBenchmark() function, which handles the read process. The RunBenchmark() function requires three parameters: num_threads , name , and method . Among them, num_threads indicates the number of threads and can be specified as a parameter value when starting db_bench . Next, Name is a string variable that represents the type of benchmark. It stores the values entered in the parameters, separated by commas. Method is a pointer variable that stores the address of the benchmark function corresponding to Name . After that, the address value of the method in the RunBenchmark() function is stored in the arg[] array. The StartThread function is executed with this value and the ThreadBody() function as arguments. The ThreadBody() function performs the given benchmark with the allocated thread. The code flow of the benchmark is as follows: Starting with benchmark functions like ReadRandom() or ReadHot() , it gradually narrows the search range through multiple Get() functions, from the database to the sstable, and then to each level, table, filter block, and bloom filter. In the benchmark function, after performing the functions assigned to each benchmark, the db->Get() function is used. Here, db is the address value of the database used in the DB::open() function when opening the database during the write process. The DBImpl::Get() function sequentially searches the memtable, immemtable, and sstable. The function used to search the sstable is Version::Get() . (Note that the bloom filter is only used in the sstable.) The Version::Get() function executes the ForEachOverlapping() function with the Match() function as an argument. The ForEachOverlapping() function first searches level 0 of the sstable and then searches other levels. The Match() function used in this search process checks if a specific key exists in the table and returns a boolean value based on its existence. The TableCache::Get() function called by the Match() function searches the table. Then, FilterBlockReader::KeyMayMatch() searches the filter block, and BloomFilterPolicy::KeyMayMatch() searches the bloom filter to finally check the existence of a specific key. The InternalGet() function in the middle proceeds with the read if the key being searched for exists.","title":"Bloom Filter Read"},{"location":"analysis/bloomfilter/bloomfilter-write/","text":"Bloom Filter Write The entire LevelDB code contains about 100 main functions. By checking the makefile of db_bench , we can see that the main function of db_bench is located in the db_bench.cc file. This main function is largely divided into two parts. The first part is the sscanf section that reads parameters like bloom_bits and num when executing db_bench , and the second part is the benchmark.Run() class function. The benchmark.Run() is also divided into three main parts. In the Benchmark class, several class variables, including filter_policy , are declared. Subsequently, the values of the class variables are assigned in the class constructor. At this time, if the bloom_bits value read by sscanf in the main function is 0 or more, it means that a bloom filter is used, so the NewBloomFilterPolicy() function is called. If it is less than 0, it means that the bloom filter is not used, and Null is returned. The default value of bloom_bits is -1, meaning that if the value is not changed, the bloom filter is not used, and if set to 0, a minimum size bloom filter is used. Then, the Run() class function is divided into three main parts. PrintHeader() is a function that outputs information about the environment or data when running db_bench to the terminal, Open() handles the write process, and RunBenchmark() handles the read process. In the Open() function, the values of class variables, including filter_policy , are stored in a new Struct called options and passed to the DB::Open() function. The DB::Open() function also puts the values of options received as arguments into a class called impl and continues the process of passing variable values, including filter_policy , to the next function by performing functions like MaybeScheduleCompaction() . The code flow is as shown above, which involves performing compaction and creating SSTables, including FilterBlock. In this process, filter_policy only receives values from the previous function to the next function, so detailed explanation is omitted. Finally, in the GenerateFilter() function, the createFilter class function of the filter_policy that has been passed so far is called. The content of this function is determined by the NewBloomFilterPolicy() function called in the Benchmark class constructor. Bloom Filter Policy NewBloomFilterPolicy() is a function that creates and returns a \"BloomFilterPolicy\" class that overrides the \"FilterPolicy\" class. The FilterPolicy class consists of a Name() function that returns the name of the filter, a CreateFilter() function that creates a bloom filter array during write, and a KeyMayMatch() function that checks if a specific key value exists in the array during read. The BloomFilterPolicy class that overrides this is implemented as follows: class BloomFilterPolicy : public FilterPolicy { public: explicit BloomFilterPolicy(int bits_per_key) : bits_per_key_(bits_per_key) { // We intentionally round down to reduce probing cost a little bit k_ = static_cast<size_t>(bits_per_key * 0.69); // 0.69 =~ ln(2) if (k_ < 1) k_ = 1; if (k_ > 30) k_ = 30; } const char* Name() const override { return \"leveldb.BuiltinBloomFilter2\"; } The part of the code that determines the number of hash functions and limits the maximum number, and The Name() function that returns a specific name. void CreateFilter(const Slice* keys, int n, std::string* dst) const override { // Compute bloom filter size (in both bits and bytes) size_t bits = n * bits_per_key_; // For small n, we can see a very high false positive rate. Fix it // by enforcing a minimum bloom filter length. if (bits < 64) bits = 64; size_t bytes = (bits + 7) / 8; bits = bytes * 8; And the CreateFilter() function determines the size of the bloom filter array (=bits) by multiplying the bloom_bits (=bits_per_key) value and the num (=n) value. const size_t init_size = dst->size(); dst->resize(init_size + bytes, 0); dst->push_back(static_cast<char>(k_)); // Remember # of probes in filter char* array = &(*dst)[init_size]; for (int i = 0; i < n; i++) { // Use double-hashing to generate a sequence of hash values. // See analysis in [Kirsch,Mitzenmacher 2006]. uint32_t h = BloomHash(keys[i]); const uint32_t delta = (h >> 17) | (h << 15); // Rotate right 17 bits for (size_t j = 0; j < k_; j++) { const uint32_t bitpos = h % bits; array[bitpos / 8] |= (1 << (bitpos % 8)); h += delta; } } ``` It is divided into a part that handles \"double hashing\". # Double Hashing ![img1 daumcdn](https://user-images.githubusercontent.com/101636590/183426381-db205ffc-c946-49af-a75d-2b0869145737.png) ![img1 daumcdn](https://user-images.githubusercontent.com/101636590/183426385-eb7ead05-1359-4802-9bc6-0f2d892756bb.png) (Source: https://www.eecs.harvard.edu/~michaelm/postscripts/rsa2008.pdf) Referring to the paper mentioned in the comments of the function, it is mathematically proven that by using two hash functions and reusing one of them, instead of using k different hash functions, the load and computation caused by hash functions can be significantly reduced while maintaining the original performance. <br/> <br/> ![img1 daumcdn](https://user-images.githubusercontent.com/101636590/183426427-2a7c8496-b118-42aa-b6da-112205d9b581.png) In LevelDB, the first hash function is implemented as a function called `BloomHash()`, and the second hash function is implemented as a bit operation `( h>>17 ) | ( h<<15 )`. ```cpp namespace { static uint32_t BloomHash(const Slice& key) { return Hash(key.data(), key.size(), 0xbc9f1d34); } uint32_t Hash(const char* data, size_t n, uint32_t seed) { // Similar to murmur hash const uint32_t m = 0xc6a4a793; const uint32_t r = 24; const char* limit = data + n; uint32_t h = seed ^ (n * m); BloomHash takes a key value as an argument and returns a specific value, taking the form of a typical hash function. The shift operation is structured to swap the first 15 bits and the last 17 bits. (Note that the hash value uses a 32-bit data type called uint32_t .) This shift operation is presumed to be used because it satisfies the properties of a hash function while having low overhead required for computation. uint32_t h = BloomHash(key); const uint32_t delta = (h >> 17) | (h << 15); // Rotate right 17 bits for (size_t j = 0; j < k; j++) { const uint32_t bitpos = h % bits; if ((array[bitpos / 8] & (1 << (bitpos % 8))) == 0) return false; h += delta; } And notably, since one bit must be set to 1 per hash, the bloom filter array is used by dividing one byte (1 byte = 8 bits) into 8 parts. bool KeyMayMatch(const Slice& key, const Slice& bloom_filter) const override { // ... omitted uint32_t h = BloomHash(key); const uint32_t delta = (h >> 17) | (h << 15); // Rotate right 17 bits for (size_t j = 0; j < k; j++) { const uint32_t bitpos = h % bits; if ((array[bitpos / 8] & (1 << (bitpos % 8))) == 0) return false; h += delta; } return true; } Finally, the KeyMayMatch function used when reading data uses almost the same computation as CreateFilter , but it uses an if statement and an and operation instead of an or operation to check if a specific key value exists within the filter.","title":"Bloom Filter Write"},{"location":"analysis/bloomfilter/bloomfilter-write/#bloom-filter-write","text":"The entire LevelDB code contains about 100 main functions. By checking the makefile of db_bench , we can see that the main function of db_bench is located in the db_bench.cc file. This main function is largely divided into two parts. The first part is the sscanf section that reads parameters like bloom_bits and num when executing db_bench , and the second part is the benchmark.Run() class function. The benchmark.Run() is also divided into three main parts. In the Benchmark class, several class variables, including filter_policy , are declared. Subsequently, the values of the class variables are assigned in the class constructor. At this time, if the bloom_bits value read by sscanf in the main function is 0 or more, it means that a bloom filter is used, so the NewBloomFilterPolicy() function is called. If it is less than 0, it means that the bloom filter is not used, and Null is returned. The default value of bloom_bits is -1, meaning that if the value is not changed, the bloom filter is not used, and if set to 0, a minimum size bloom filter is used. Then, the Run() class function is divided into three main parts. PrintHeader() is a function that outputs information about the environment or data when running db_bench to the terminal, Open() handles the write process, and RunBenchmark() handles the read process. In the Open() function, the values of class variables, including filter_policy , are stored in a new Struct called options and passed to the DB::Open() function. The DB::Open() function also puts the values of options received as arguments into a class called impl and continues the process of passing variable values, including filter_policy , to the next function by performing functions like MaybeScheduleCompaction() . The code flow is as shown above, which involves performing compaction and creating SSTables, including FilterBlock. In this process, filter_policy only receives values from the previous function to the next function, so detailed explanation is omitted. Finally, in the GenerateFilter() function, the createFilter class function of the filter_policy that has been passed so far is called. The content of this function is determined by the NewBloomFilterPolicy() function called in the Benchmark class constructor.","title":"Bloom Filter Write"},{"location":"analysis/bloomfilter/bloomfilter-write/#bloom-filter-policy","text":"NewBloomFilterPolicy() is a function that creates and returns a \"BloomFilterPolicy\" class that overrides the \"FilterPolicy\" class. The FilterPolicy class consists of a Name() function that returns the name of the filter, a CreateFilter() function that creates a bloom filter array during write, and a KeyMayMatch() function that checks if a specific key value exists in the array during read. The BloomFilterPolicy class that overrides this is implemented as follows: class BloomFilterPolicy : public FilterPolicy { public: explicit BloomFilterPolicy(int bits_per_key) : bits_per_key_(bits_per_key) { // We intentionally round down to reduce probing cost a little bit k_ = static_cast<size_t>(bits_per_key * 0.69); // 0.69 =~ ln(2) if (k_ < 1) k_ = 1; if (k_ > 30) k_ = 30; } const char* Name() const override { return \"leveldb.BuiltinBloomFilter2\"; } The part of the code that determines the number of hash functions and limits the maximum number, and The Name() function that returns a specific name. void CreateFilter(const Slice* keys, int n, std::string* dst) const override { // Compute bloom filter size (in both bits and bytes) size_t bits = n * bits_per_key_; // For small n, we can see a very high false positive rate. Fix it // by enforcing a minimum bloom filter length. if (bits < 64) bits = 64; size_t bytes = (bits + 7) / 8; bits = bytes * 8; And the CreateFilter() function determines the size of the bloom filter array (=bits) by multiplying the bloom_bits (=bits_per_key) value and the num (=n) value. const size_t init_size = dst->size(); dst->resize(init_size + bytes, 0); dst->push_back(static_cast<char>(k_)); // Remember # of probes in filter char* array = &(*dst)[init_size]; for (int i = 0; i < n; i++) { // Use double-hashing to generate a sequence of hash values. // See analysis in [Kirsch,Mitzenmacher 2006]. uint32_t h = BloomHash(keys[i]); const uint32_t delta = (h >> 17) | (h << 15); // Rotate right 17 bits for (size_t j = 0; j < k_; j++) { const uint32_t bitpos = h % bits; array[bitpos / 8] |= (1 << (bitpos % 8)); h += delta; } } ``` It is divided into a part that handles \"double hashing\". # Double Hashing ![img1 daumcdn](https://user-images.githubusercontent.com/101636590/183426381-db205ffc-c946-49af-a75d-2b0869145737.png) ![img1 daumcdn](https://user-images.githubusercontent.com/101636590/183426385-eb7ead05-1359-4802-9bc6-0f2d892756bb.png) (Source: https://www.eecs.harvard.edu/~michaelm/postscripts/rsa2008.pdf) Referring to the paper mentioned in the comments of the function, it is mathematically proven that by using two hash functions and reusing one of them, instead of using k different hash functions, the load and computation caused by hash functions can be significantly reduced while maintaining the original performance. <br/> <br/> ![img1 daumcdn](https://user-images.githubusercontent.com/101636590/183426427-2a7c8496-b118-42aa-b6da-112205d9b581.png) In LevelDB, the first hash function is implemented as a function called `BloomHash()`, and the second hash function is implemented as a bit operation `( h>>17 ) | ( h<<15 )`. ```cpp namespace { static uint32_t BloomHash(const Slice& key) { return Hash(key.data(), key.size(), 0xbc9f1d34); } uint32_t Hash(const char* data, size_t n, uint32_t seed) { // Similar to murmur hash const uint32_t m = 0xc6a4a793; const uint32_t r = 24; const char* limit = data + n; uint32_t h = seed ^ (n * m); BloomHash takes a key value as an argument and returns a specific value, taking the form of a typical hash function. The shift operation is structured to swap the first 15 bits and the last 17 bits. (Note that the hash value uses a 32-bit data type called uint32_t .) This shift operation is presumed to be used because it satisfies the properties of a hash function while having low overhead required for computation. uint32_t h = BloomHash(key); const uint32_t delta = (h >> 17) | (h << 15); // Rotate right 17 bits for (size_t j = 0; j < k; j++) { const uint32_t bitpos = h % bits; if ((array[bitpos / 8] & (1 << (bitpos % 8))) == 0) return false; h += delta; } And notably, since one bit must be set to 1 per hash, the bloom filter array is used by dividing one byte (1 byte = 8 bits) into 8 parts. bool KeyMayMatch(const Slice& key, const Slice& bloom_filter) const override { // ... omitted uint32_t h = BloomHash(key); const uint32_t delta = (h >> 17) | (h << 15); // Rotate right 17 bits for (size_t j = 0; j < k; j++) { const uint32_t bitpos = h % bits; if ((array[bitpos / 8] & (1 << (bitpos % 8))) == 0) return false; h += delta; } return true; } Finally, the KeyMayMatch function used when reading data uses almost the same computation as CreateFilter , but it uses an if statement and an and operation instead of an or operation to check if a specific key value exists within the filter.","title":"Bloom Filter Policy"},{"location":"analysis/bloomfilter/bloomfilter/","text":"Bloom Filter Structure (Source: https://en.wikipedia.org/wiki/Bloom_filter) A Bloom filter is a probabilistic data structure that can determine whether a specific key exists in a data block. When writing data to a Bloom filter, k hash functions are performed on each key to obtain k hash values. The values corresponding to these hash values in the Bloom filter array are changed from 0 to 1, indicating that the key exists in the data block. When reading, instead of reading the entire data block, the same k hash functions are performed on the data to be read to obtain k hash values. Only the data blocks where all the corresponding array values are 1 are selected for reading, thereby improving read performance. Bloom Filter Location (Source: https://leveldb-handbook.readthedocs.io/zh/latest/) The Bloom filter exists within the SSTable, and the structure of the SSTable is as shown above. One SSTable contains n data blocks, 1 filter block, and 1 meta index block. The filter block contains n Bloom filter arrays, and the meta index block indicates which data block each Bloom filter array corresponds to. True Negative & False Positive (Source: https://www.linkedin.com/pulse/which-worse-false-positive-false-negative-miha-mozina-phd) The biggest advantage of a Bloom filter is that a True Negative never occurs. A True Negative is when data that exists in the database is judged as non-existent, which is directly related to the reliability of the filter. On the other hand, False Positives (judging non-existent data as existent) can occur frequently. This can be a cause of performance degradation, and reducing False Positives is one of the important challenges of Bloom filters. Hash Function Using the benchmarking tool db_bench provided by LevelDB, we can adjust the number of bits used per key (Bloom_bits) or the number of data (Num). The size of the generated Bloom filter array becomes Bloom_bits * Num bits. This value can be checked in the CreateFilter class function of bloom.cc . The default setting of db_bench does not use a Bloom filter, and to use it, the Bloom_bits value must be specified. The ideal Bloom_bits value in LevelDB is 10, which can improve read performance without significantly degrading write performance. Additionally, the number of hash functions K is determined using the formula K = ln(2) * (M/N) = ln(2) * B . This formula calculates the value that can minimize the False Positive rate. False Positive Probability The probability of a False Positive can be mathematically organized, and through this, the optimal k value and number of bits to minimize False Positives can be calculated. When the k value is ln(2) * (m/n) , the probability of a False Positive is expressed in the form of 1/2^k , and as the k value increases, the probability of a False Positive decreases. This is proportional to bloom_bits. Code Flow of Bloom Filter Write: Creation of Bloom Filter Read: Quickly check the existence of a specific key with Bloom Filter Reference Estimation using Probabilistic Data Structures LevelDB Github Github - db_bench.cc Github - bloom.cc Github - filter_block.cc Github - filter_policy.h LevelDB Handbook","title":"Bloom Filter Structure"},{"location":"analysis/bloomfilter/bloomfilter/#bloom-filter-structure","text":"(Source: https://en.wikipedia.org/wiki/Bloom_filter) A Bloom filter is a probabilistic data structure that can determine whether a specific key exists in a data block. When writing data to a Bloom filter, k hash functions are performed on each key to obtain k hash values. The values corresponding to these hash values in the Bloom filter array are changed from 0 to 1, indicating that the key exists in the data block. When reading, instead of reading the entire data block, the same k hash functions are performed on the data to be read to obtain k hash values. Only the data blocks where all the corresponding array values are 1 are selected for reading, thereby improving read performance.","title":"Bloom Filter Structure"},{"location":"analysis/bloomfilter/bloomfilter/#bloom-filter-location","text":"(Source: https://leveldb-handbook.readthedocs.io/zh/latest/) The Bloom filter exists within the SSTable, and the structure of the SSTable is as shown above. One SSTable contains n data blocks, 1 filter block, and 1 meta index block. The filter block contains n Bloom filter arrays, and the meta index block indicates which data block each Bloom filter array corresponds to.","title":"Bloom Filter Location"},{"location":"analysis/bloomfilter/bloomfilter/#true-negative-false-positive","text":"(Source: https://www.linkedin.com/pulse/which-worse-false-positive-false-negative-miha-mozina-phd) The biggest advantage of a Bloom filter is that a True Negative never occurs. A True Negative is when data that exists in the database is judged as non-existent, which is directly related to the reliability of the filter. On the other hand, False Positives (judging non-existent data as existent) can occur frequently. This can be a cause of performance degradation, and reducing False Positives is one of the important challenges of Bloom filters.","title":"True Negative &amp; False Positive"},{"location":"analysis/bloomfilter/bloomfilter/#hash-function","text":"Using the benchmarking tool db_bench provided by LevelDB, we can adjust the number of bits used per key (Bloom_bits) or the number of data (Num). The size of the generated Bloom filter array becomes Bloom_bits * Num bits. This value can be checked in the CreateFilter class function of bloom.cc . The default setting of db_bench does not use a Bloom filter, and to use it, the Bloom_bits value must be specified. The ideal Bloom_bits value in LevelDB is 10, which can improve read performance without significantly degrading write performance. Additionally, the number of hash functions K is determined using the formula K = ln(2) * (M/N) = ln(2) * B . This formula calculates the value that can minimize the False Positive rate.","title":"Hash Function"},{"location":"analysis/bloomfilter/bloomfilter/#false-positive-probability","text":"The probability of a False Positive can be mathematically organized, and through this, the optimal k value and number of bits to minimize False Positives can be calculated. When the k value is ln(2) * (m/n) , the probability of a False Positive is expressed in the form of 1/2^k , and as the k value increases, the probability of a False Positive decreases. This is proportional to bloom_bits.","title":"False Positive Probability"},{"location":"analysis/bloomfilter/bloomfilter/#code-flow-of-bloom-filter","text":"Write: Creation of Bloom Filter Read: Quickly check the existence of a specific key with Bloom Filter","title":"Code Flow of Bloom Filter"},{"location":"analysis/bloomfilter/bloomfilter/#reference","text":"Estimation using Probabilistic Data Structures LevelDB Github Github - db_bench.cc Github - bloom.cc Github - filter_block.cc Github - filter_policy.h LevelDB Handbook","title":"Reference"},{"location":"analysis/compaction/Major-Compaction/","text":"Major Compaction What is Major Compaction ? - This is the actual process we commonly refer to as Compaction . - Unlike Minor Compaction , which moves data from memory to disk, it merges data within the disk. - It merges each data and moves it to a lower level. Why use Major Compaction ? - The most obvious benefit of using Major Compaction is to clean up duplicate data. - If the same key exists in sst files of different levels, you can delete the data (old data) at the lower level. - Since previously written data might be needed, data to be deleted is recorded sequentially, and the latest data is updated to save disk space. - Since Level 0 may not have ordered data files, merging them into Level 1 sorts the data, making it easier to find files and improving read efficiency. When does Major Compaction occur? - It occurs when the files at each level accumulate to a threshold and there is no immutable present. Overall Major Compaction Code Flow Let's take a look at the overall Major Compaction Code Flow. When the files at each level accumulate to a threshold, MaybeScheduleCompaction determines whether a merge is needed. If a merge is necessary, it checks for the presence of immutable and, if absent, calls PickCompaction to gather information for the merge, and Major Compaction proceeds in DoCompactionWork . Let's delve into the process of Major compaction through the source code. Overall Process The number of files at a certain level reaches a threshold. MaybeScheduleCompaction determines if a merge is needed. BackgroundCompaction checks for the presence of immutable and, if absent, calls functions for Major Compaction. PickCompaction stores the information needed for the merge. With the stored information, the actual merge is carried out in DoCompactionWork . In DoCompactionWork , OpenCompactionOutputFile , FinishCompactionOutputFile , and InstallCompactionResults are executed to create sst files, insert merged records, check for errors, and place the files in the respective level. Afterward, the version is updated, and finally, CleanupCompaction is called to delete unnecessary sst files, completing the Major Compaction. MaybeScheduleCompaction Determines if a merge is needed. void DBImpl::MaybeScheduleCompaction() { mutex_.AssertHeld(); //(TeamCompaction)If a merger is already in progress, DB is being deleted, or if there is an error, nothing will happen. if (background_compaction_scheduled_) { } else if (shutting_down_.load(std::memory_order_acquire)) { } else if (!bg_error_.ok()) { //(TeamCompaction)If immutable does not exist, manual compaction do not exist, and mergers at each level are not required, nothing will happen. } else if (imm_ == nullptr && manual_compaction_ == nullptr && !versions_->NeedsCompaction()) { //(TeamCompaction)In other cases, since a merger occurs, call 'BGWork' to proceed with the merger. } else { background_compaction_scheduled_ = true; env_->Schedule(&DBImpl::BGWork, this); } } If a merge is already in progress, the DB is being deleted, or there is an error, nothing happens. If immutable does not exist, manual compaction does not exist, and compaction is not needed at each level ( NeedCompaction ), nothing happens. (Manual compaction will be explained in the BackgroundCompaction section.) In other cases, since a merge is needed, BGWork is called to proceed with the merge. BackgroundCompaction Determines Major Compaction or Minor Compaction based on the presence of immutable. Also, if the user wants manual compaction, it proceeds here. void DBImpl::BackgroundCompaction() { mutex_.AssertHeld(); //(TeamCompaction)If imutable exists, proceed with Minor Compact through the CompactMemtable function. if (imm_ != nullptr) { CompactMemTable(); return; } Compaction* c; //(TeamCompaction)If you want to merge manually, proceed with the manual merger with true (most of them are automatically merged, so they are not used well) bool is_manual = (manual_compaction_ != nullptr); InternalKey manual_end; if (is_manual) { ManualCompaction* m = manual_compaction_; c = versions_->CompactRange(m->level, m->begin, m->end); m->done = (c == nullptr); if (c != nullptr) { manual_end = c->input(0, c->num_input_files(0) - 1)->largest; } Log(options_.info_log, \"Manual compaction at level-%d from %s .. %s; will stop at %s\\n\", m->level, (m->begin ? m->begin->DebugString().c_str() : \"(begin)\"), (m->end ? m->end->DebugString().c_str() : \"(end)\"), (m->done ? \"(end)\" : manual_end.DebugString().c_str())); } //(TeamCompaction)Store information that needs to be merged if immutable does not exist else { c = versions_->PickCompaction(); } //...skipped Status status; //(TeamCompaction)Nothing happens if there is no information to merge if (c == nullptr) { } else if (!is_manual && c->IsTrivialMove()) { //...skipped (TeamCompaciton) Manual Compaction progression part } //(TeamCompaction)With information to merge, proceed with Major Compaction. else { CompactionState* compact = new CompactionState(c); status = DoCompactionWork(compact); if (!status.ok()) { RecordBackgroundError(status); } //(TeamCompaction)If there is no problem after completing the merger completely, remove the sst files (files collected after the merger) that are now unnecessary CleanupCompaction(compact); //(TeamCompaction)Remove the sst files that were saved for the merger c->ReleaseInputs(); RemoveObsoleteFiles(); } //...skipped If immutable exists, Minor Compaction (Flush) is performed through CompactMemtable . If immutable does not exist and manual compaction is not desired, information for compaction is stored through PickCompaction . If there is no information to merge, nothing happens. If there is, Major Compaction is carried out in DoCompactionWork based on that information. After the merge is completed in DoCompactionWork and the state is intact, unnecessary sst files from before the merge are removed through CleanupCompaction to finish. (Additional Explanation) Manual Compaction vs Automatic Compaction Manual Compaction - Rarely used and mainly for debugging purposes. - To use, set a key range in the benchmark and execute it, setting the manual compaction boolean to true to perform manual compaction. Automatic Compaction - Most of the compactions we use are performed automatically. DoCompactionWork The actual Major Compaction is carried out. Status DBImpl::DoCompactionWork(CompactionState* compact) { //...skipped //(TeamCompaction)Create an iter for the index block and data block of each sst file from level 0, 1 to N, and use it to find each key. //(TeamCompaction)After that, arrange the created iters so that they can be listed in the order of level 0, 1 to N, and store them in input (see Compaction-Iter.md for details) Iterator* input = versions_->MakeInputIterator(compact->compaction); // Release mutex while we're actually doing the compaction work mutex_.Unlock(); //(TeamCompaction)Position the pointer position of the created iter first input->SeekToFirst(); Status status; //(TeamCompaction)Parse the internal key and divide it into user key, sequence number, and type ParsedInternalKey ikey; std::string current_user_key; bool has_current_user_key = false; //(TeamCompaction)Set the latest key to the highest value SequenceNumber last_sequence_for_key = kMaxSequenceNumber; //(TeamCompaction)The process of repeatedly finding and processing the key/value that needs to be merged through iter stored in the input while (input->Valid() && !shutting_down_.load(std::memory_order_acquire)) { //...skipped //(TeamCompaction)Obtain the key of the current corresponding sst file Slice key = input->key(); //(TeamCompaction)Check if you need an sst file to put the key in, and if there is an sst file to put in, the merger is completed if (compact->compaction->ShouldStopBefore(key) && compact->builder != NULL) { status = FinishCompactionOutputFile(compact, input); } //(TeamCompaction)Compare different sequences for the same key to obtain the latest user key and delete records from different user keys that were the same bool drop = false; if (!ParseInternalKey(key, &ikey)) { // Do not hide error keys current_user_key.clear(); has_current_user_key = false; last_sequence_for_key = kMaxSequenceNumber; } else { if (!has_current_user_key || user_comparator()->Compare(ikey.user_key, Slice(current_user_key)) != 0) { // First occurrence of this user key current_user_key.assign(ikey.user_key.data(), ikey.user_key.size()); has_current_user_key = true; last_sequence_for_key = kMaxSequenceNumber; } if (last_sequence_for_key <= compact->smallest_snapshot) { // Hidden by an newer entry for same user key drop = true; // (A) } else if (ikey.type == kTypeDeletion && ikey.sequence <= compact->smallest_snapshot && compact->compaction->IsBaseLevelForKey(ikey.user_key)) { // For this user key: // (1) there is no data in higher levels // (2) data in lower levels will have larger sequence numbers // (3) data in layers that are being compacted here and have // smaller sequence numbers will be dropped in the next // few iterations of this loop (by rule (A) above). // Therefore this deletion marker is obsolete and can be dropped. drop = true; } last_sequence_for_key = ikey.sequence; } //...skipped if (!drop) { //(TeamCompaction)Create new sst file if necessary if (compact->builder == nullptr) { status = OpenCompactionOutputFile(compact); if (!status.ok()) { break; } } if (compact->builder->NumEntries() == 0) { compact->current_output()->smallest.DecodeFrom(key); } compact->current_output()->largest.DecodeFrom(key); //(TeamCompaction)Add records from the latest user key to the sst file compact->builder->Add(key, input->value()); //(TeamCompaction)If data accumulates in the sst file and exceeds the maximum file size, the merger of the sst file is completed if (compact->builder->FileSize() >= compact->compaction->MaxOutputFileSize()) { status = FinishCompactionOutputFile(compact, input); if (!status.ok()) { break; } } } //(TeamCompaction)Among the sst files arranged in order of level 0, 1 to N in the input variable, the next sst file is moved on input->Next(); } if (status.ok() && shutting_down_.load(std::memory_order_acquire)) { status = Status::IOError(\"Deleting DB during compaction\"); } //(TeamCompaction)If the sst file does not have an error and the sst file exists, the merger is completed if (status.ok() && compact->builder != nullptr) { status = FinishCompactionOutputFile(compact, input); } if (status.ok()) { status = input->status(); } //...skipped //(TeamCompaction)Move the merged sst file to its level if (status.ok()) { status = InstallCompactionResults(compact); } if (!status.ok()) { RecordBackgroundError(status); } VersionSet::LevelSummaryStorage tmp; Log(options_.info_log, \"compacted to: %s\", versions_->LevelSummary(&tmp)); return status; } Create an iter for the index block and data block of each sst file from level 0, 1 to N, and use it to find each key. Then, arrange the created iters so that they can be listed in the order of level 0, 1 to N, and store them in input . Position the pointer of the iter to the first position and parse the internalkey into user key, sequence, and type. Obtain the key of the current corresponding sst file using key() . Compare different sequences for the same key to obtain the latest user key's record and delete records from different user keys that were the same. If an sst file is needed, create a new one and insert the latest user key's record. If data accumulates in the sst file and exceeds the maximum file size, complete the merge and move to the next sst file. Repeat steps 2-6 to complete the merge of all sst files, check the overall status, and if there are no errors, move the merged sst file to its level. OpenCompactionOutputFile Creates a new sst file to insert the merged user key records. Status DBImpl::OpenCompactionOutputFile(CompactionState* compact) { //...skipped //(TeamCompaction)Set the number of the newly created sst file std::string fname = TableFileName(dbname_, file_number); //(TeamCompaction)Numbered and temporarily merged records in writablefile Status s = env_->NewWritableFile(fname, &compact->outfile); //(TeamCompaction)If the record is in good condition, create a sst file and add it to the builder if (s.ok()) { compact->builder = new TableBuilder(options_, compact->outfile); } return s; } Assign a number to the newly created sst file. Numbered and temporarily merged records in WriteableFile . If the record is in good condition, create an sst file and add it to the builder. FinishCompactionOutputFile Checks for errors in the merged iter and sst file. Status DBImpl::FinishCompactionOutputFile(CompactionState* compact, Iterator* input) { //...skipped //(TeamCompaction)Check for errors for iter Status s = input->status(); const uint64_t current_entries = compact->builder->NumEntries(); if (s.ok()) { s = compact->builder->Finish(); } else { compact->builder->Abandon(); } //...skipped //(TeamCompaction)Check and finalize errors for the sst file itself if (s.ok()) { s = compact->outfile->Sync(); } if (s.ok()) { s = compact->outfile->Close(); } delete compact->outfile; compact->outfile = nullptr; //...skipped } Check for errors in the iter. Check and finalize errors for the sst file itself. InstallCompactionResults Moves the merged sst file to its level and updates the version. Status DBImpl::InstallCompactionResults(CompactionState* compact) { //...skipped //(TeamCompaction) Moved merged sst files to that level compact->compaction->AddInputDeletions(compact->compaction->edit()); const int level = compact->compaction->level(); for (size_t i = 0; i < compact->outputs.size(); i++) { const CompactionState::Output& out = compact->outputs[i]; compact->compaction->edit()->AddFile(level + 1, out.number, out.file_size, out.smallest, out.largest); } //(TeamCompaction)Update version return versions_->LogAndApply(compact->compaction->edit(), &mutex_); } Move the merged sst file to its level. Update the version to complete. CleanupCompaction Removes unnecessary sst files (sst files before the merge) after the merge is completed. void DBImpl::CleanupCompaction(CompactionState* compact) { mutex_.AssertHeld(); //(TeamCompaction)Remove files if they exist in the builder that contains the sst files that need to be merged if (compact->builder != nullptr) { // May happen if we get a shutdown call in the middle of compaction compact->builder->Abandon(); delete compact->builder; } else { assert(compact->outfile == nullptr); } delete compact->outfile; for (size_t i = 0; i < compact->outputs.size(); i++) { const CompactionState::Output& out = compact->outputs[i]; pending_outputs_.erase(out.number); } delete compact; } Since the latest sst file has been updated, if there are files in the builder that contains the sst files to be merged, they are removed as they are no longer needed. Finalize the Major Compaction.","title":"Major Compaction"},{"location":"analysis/compaction/Major-Compaction/#major-compaction","text":"What is Major Compaction ? - This is the actual process we commonly refer to as Compaction . - Unlike Minor Compaction , which moves data from memory to disk, it merges data within the disk. - It merges each data and moves it to a lower level. Why use Major Compaction ? - The most obvious benefit of using Major Compaction is to clean up duplicate data. - If the same key exists in sst files of different levels, you can delete the data (old data) at the lower level. - Since previously written data might be needed, data to be deleted is recorded sequentially, and the latest data is updated to save disk space. - Since Level 0 may not have ordered data files, merging them into Level 1 sorts the data, making it easier to find files and improving read efficiency. When does Major Compaction occur? - It occurs when the files at each level accumulate to a threshold and there is no immutable present.","title":"Major Compaction"},{"location":"analysis/compaction/Major-Compaction/#overall-major-compaction-code-flow","text":"Let's take a look at the overall Major Compaction Code Flow. When the files at each level accumulate to a threshold, MaybeScheduleCompaction determines whether a merge is needed. If a merge is necessary, it checks for the presence of immutable and, if absent, calls PickCompaction to gather information for the merge, and Major Compaction proceeds in DoCompactionWork . Let's delve into the process of Major compaction through the source code.","title":"Overall Major Compaction Code Flow"},{"location":"analysis/compaction/Major-Compaction/#overall-process","text":"The number of files at a certain level reaches a threshold. MaybeScheduleCompaction determines if a merge is needed. BackgroundCompaction checks for the presence of immutable and, if absent, calls functions for Major Compaction. PickCompaction stores the information needed for the merge. With the stored information, the actual merge is carried out in DoCompactionWork . In DoCompactionWork , OpenCompactionOutputFile , FinishCompactionOutputFile , and InstallCompactionResults are executed to create sst files, insert merged records, check for errors, and place the files in the respective level. Afterward, the version is updated, and finally, CleanupCompaction is called to delete unnecessary sst files, completing the Major Compaction.","title":"Overall Process"},{"location":"analysis/compaction/Major-Compaction/#maybeschedulecompaction","text":"Determines if a merge is needed. void DBImpl::MaybeScheduleCompaction() { mutex_.AssertHeld(); //(TeamCompaction)If a merger is already in progress, DB is being deleted, or if there is an error, nothing will happen. if (background_compaction_scheduled_) { } else if (shutting_down_.load(std::memory_order_acquire)) { } else if (!bg_error_.ok()) { //(TeamCompaction)If immutable does not exist, manual compaction do not exist, and mergers at each level are not required, nothing will happen. } else if (imm_ == nullptr && manual_compaction_ == nullptr && !versions_->NeedsCompaction()) { //(TeamCompaction)In other cases, since a merger occurs, call 'BGWork' to proceed with the merger. } else { background_compaction_scheduled_ = true; env_->Schedule(&DBImpl::BGWork, this); } } If a merge is already in progress, the DB is being deleted, or there is an error, nothing happens. If immutable does not exist, manual compaction does not exist, and compaction is not needed at each level ( NeedCompaction ), nothing happens. (Manual compaction will be explained in the BackgroundCompaction section.) In other cases, since a merge is needed, BGWork is called to proceed with the merge.","title":"MaybeScheduleCompaction"},{"location":"analysis/compaction/Major-Compaction/#backgroundcompaction","text":"Determines Major Compaction or Minor Compaction based on the presence of immutable. Also, if the user wants manual compaction, it proceeds here. void DBImpl::BackgroundCompaction() { mutex_.AssertHeld(); //(TeamCompaction)If imutable exists, proceed with Minor Compact through the CompactMemtable function. if (imm_ != nullptr) { CompactMemTable(); return; } Compaction* c; //(TeamCompaction)If you want to merge manually, proceed with the manual merger with true (most of them are automatically merged, so they are not used well) bool is_manual = (manual_compaction_ != nullptr); InternalKey manual_end; if (is_manual) { ManualCompaction* m = manual_compaction_; c = versions_->CompactRange(m->level, m->begin, m->end); m->done = (c == nullptr); if (c != nullptr) { manual_end = c->input(0, c->num_input_files(0) - 1)->largest; } Log(options_.info_log, \"Manual compaction at level-%d from %s .. %s; will stop at %s\\n\", m->level, (m->begin ? m->begin->DebugString().c_str() : \"(begin)\"), (m->end ? m->end->DebugString().c_str() : \"(end)\"), (m->done ? \"(end)\" : manual_end.DebugString().c_str())); } //(TeamCompaction)Store information that needs to be merged if immutable does not exist else { c = versions_->PickCompaction(); } //...skipped Status status; //(TeamCompaction)Nothing happens if there is no information to merge if (c == nullptr) { } else if (!is_manual && c->IsTrivialMove()) { //...skipped (TeamCompaciton) Manual Compaction progression part } //(TeamCompaction)With information to merge, proceed with Major Compaction. else { CompactionState* compact = new CompactionState(c); status = DoCompactionWork(compact); if (!status.ok()) { RecordBackgroundError(status); } //(TeamCompaction)If there is no problem after completing the merger completely, remove the sst files (files collected after the merger) that are now unnecessary CleanupCompaction(compact); //(TeamCompaction)Remove the sst files that were saved for the merger c->ReleaseInputs(); RemoveObsoleteFiles(); } //...skipped If immutable exists, Minor Compaction (Flush) is performed through CompactMemtable . If immutable does not exist and manual compaction is not desired, information for compaction is stored through PickCompaction . If there is no information to merge, nothing happens. If there is, Major Compaction is carried out in DoCompactionWork based on that information. After the merge is completed in DoCompactionWork and the state is intact, unnecessary sst files from before the merge are removed through CleanupCompaction to finish.","title":"BackgroundCompaction"},{"location":"analysis/compaction/Major-Compaction/#additional-explanation","text":"Manual Compaction vs Automatic Compaction Manual Compaction - Rarely used and mainly for debugging purposes. - To use, set a key range in the benchmark and execute it, setting the manual compaction boolean to true to perform manual compaction. Automatic Compaction - Most of the compactions we use are performed automatically.","title":"(Additional Explanation)"},{"location":"analysis/compaction/Major-Compaction/#docompactionwork","text":"The actual Major Compaction is carried out. Status DBImpl::DoCompactionWork(CompactionState* compact) { //...skipped //(TeamCompaction)Create an iter for the index block and data block of each sst file from level 0, 1 to N, and use it to find each key. //(TeamCompaction)After that, arrange the created iters so that they can be listed in the order of level 0, 1 to N, and store them in input (see Compaction-Iter.md for details) Iterator* input = versions_->MakeInputIterator(compact->compaction); // Release mutex while we're actually doing the compaction work mutex_.Unlock(); //(TeamCompaction)Position the pointer position of the created iter first input->SeekToFirst(); Status status; //(TeamCompaction)Parse the internal key and divide it into user key, sequence number, and type ParsedInternalKey ikey; std::string current_user_key; bool has_current_user_key = false; //(TeamCompaction)Set the latest key to the highest value SequenceNumber last_sequence_for_key = kMaxSequenceNumber; //(TeamCompaction)The process of repeatedly finding and processing the key/value that needs to be merged through iter stored in the input while (input->Valid() && !shutting_down_.load(std::memory_order_acquire)) { //...skipped //(TeamCompaction)Obtain the key of the current corresponding sst file Slice key = input->key(); //(TeamCompaction)Check if you need an sst file to put the key in, and if there is an sst file to put in, the merger is completed if (compact->compaction->ShouldStopBefore(key) && compact->builder != NULL) { status = FinishCompactionOutputFile(compact, input); } //(TeamCompaction)Compare different sequences for the same key to obtain the latest user key and delete records from different user keys that were the same bool drop = false; if (!ParseInternalKey(key, &ikey)) { // Do not hide error keys current_user_key.clear(); has_current_user_key = false; last_sequence_for_key = kMaxSequenceNumber; } else { if (!has_current_user_key || user_comparator()->Compare(ikey.user_key, Slice(current_user_key)) != 0) { // First occurrence of this user key current_user_key.assign(ikey.user_key.data(), ikey.user_key.size()); has_current_user_key = true; last_sequence_for_key = kMaxSequenceNumber; } if (last_sequence_for_key <= compact->smallest_snapshot) { // Hidden by an newer entry for same user key drop = true; // (A) } else if (ikey.type == kTypeDeletion && ikey.sequence <= compact->smallest_snapshot && compact->compaction->IsBaseLevelForKey(ikey.user_key)) { // For this user key: // (1) there is no data in higher levels // (2) data in lower levels will have larger sequence numbers // (3) data in layers that are being compacted here and have // smaller sequence numbers will be dropped in the next // few iterations of this loop (by rule (A) above). // Therefore this deletion marker is obsolete and can be dropped. drop = true; } last_sequence_for_key = ikey.sequence; } //...skipped if (!drop) { //(TeamCompaction)Create new sst file if necessary if (compact->builder == nullptr) { status = OpenCompactionOutputFile(compact); if (!status.ok()) { break; } } if (compact->builder->NumEntries() == 0) { compact->current_output()->smallest.DecodeFrom(key); } compact->current_output()->largest.DecodeFrom(key); //(TeamCompaction)Add records from the latest user key to the sst file compact->builder->Add(key, input->value()); //(TeamCompaction)If data accumulates in the sst file and exceeds the maximum file size, the merger of the sst file is completed if (compact->builder->FileSize() >= compact->compaction->MaxOutputFileSize()) { status = FinishCompactionOutputFile(compact, input); if (!status.ok()) { break; } } } //(TeamCompaction)Among the sst files arranged in order of level 0, 1 to N in the input variable, the next sst file is moved on input->Next(); } if (status.ok() && shutting_down_.load(std::memory_order_acquire)) { status = Status::IOError(\"Deleting DB during compaction\"); } //(TeamCompaction)If the sst file does not have an error and the sst file exists, the merger is completed if (status.ok() && compact->builder != nullptr) { status = FinishCompactionOutputFile(compact, input); } if (status.ok()) { status = input->status(); } //...skipped //(TeamCompaction)Move the merged sst file to its level if (status.ok()) { status = InstallCompactionResults(compact); } if (!status.ok()) { RecordBackgroundError(status); } VersionSet::LevelSummaryStorage tmp; Log(options_.info_log, \"compacted to: %s\", versions_->LevelSummary(&tmp)); return status; } Create an iter for the index block and data block of each sst file from level 0, 1 to N, and use it to find each key. Then, arrange the created iters so that they can be listed in the order of level 0, 1 to N, and store them in input . Position the pointer of the iter to the first position and parse the internalkey into user key, sequence, and type. Obtain the key of the current corresponding sst file using key() . Compare different sequences for the same key to obtain the latest user key's record and delete records from different user keys that were the same. If an sst file is needed, create a new one and insert the latest user key's record. If data accumulates in the sst file and exceeds the maximum file size, complete the merge and move to the next sst file. Repeat steps 2-6 to complete the merge of all sst files, check the overall status, and if there are no errors, move the merged sst file to its level.","title":"DoCompactionWork"},{"location":"analysis/compaction/Major-Compaction/#opencompactionoutputfile","text":"Creates a new sst file to insert the merged user key records. Status DBImpl::OpenCompactionOutputFile(CompactionState* compact) { //...skipped //(TeamCompaction)Set the number of the newly created sst file std::string fname = TableFileName(dbname_, file_number); //(TeamCompaction)Numbered and temporarily merged records in writablefile Status s = env_->NewWritableFile(fname, &compact->outfile); //(TeamCompaction)If the record is in good condition, create a sst file and add it to the builder if (s.ok()) { compact->builder = new TableBuilder(options_, compact->outfile); } return s; } Assign a number to the newly created sst file. Numbered and temporarily merged records in WriteableFile . If the record is in good condition, create an sst file and add it to the builder.","title":"OpenCompactionOutputFile"},{"location":"analysis/compaction/Major-Compaction/#finishcompactionoutputfile","text":"Checks for errors in the merged iter and sst file. Status DBImpl::FinishCompactionOutputFile(CompactionState* compact, Iterator* input) { //...skipped //(TeamCompaction)Check for errors for iter Status s = input->status(); const uint64_t current_entries = compact->builder->NumEntries(); if (s.ok()) { s = compact->builder->Finish(); } else { compact->builder->Abandon(); } //...skipped //(TeamCompaction)Check and finalize errors for the sst file itself if (s.ok()) { s = compact->outfile->Sync(); } if (s.ok()) { s = compact->outfile->Close(); } delete compact->outfile; compact->outfile = nullptr; //...skipped } Check for errors in the iter. Check and finalize errors for the sst file itself.","title":"FinishCompactionOutputFile"},{"location":"analysis/compaction/Major-Compaction/#installcompactionresults","text":"Moves the merged sst file to its level and updates the version. Status DBImpl::InstallCompactionResults(CompactionState* compact) { //...skipped //(TeamCompaction) Moved merged sst files to that level compact->compaction->AddInputDeletions(compact->compaction->edit()); const int level = compact->compaction->level(); for (size_t i = 0; i < compact->outputs.size(); i++) { const CompactionState::Output& out = compact->outputs[i]; compact->compaction->edit()->AddFile(level + 1, out.number, out.file_size, out.smallest, out.largest); } //(TeamCompaction)Update version return versions_->LogAndApply(compact->compaction->edit(), &mutex_); } Move the merged sst file to its level. Update the version to complete.","title":"InstallCompactionResults"},{"location":"analysis/compaction/Major-Compaction/#cleanupcompaction","text":"Removes unnecessary sst files (sst files before the merge) after the merge is completed. void DBImpl::CleanupCompaction(CompactionState* compact) { mutex_.AssertHeld(); //(TeamCompaction)Remove files if they exist in the builder that contains the sst files that need to be merged if (compact->builder != nullptr) { // May happen if we get a shutdown call in the middle of compaction compact->builder->Abandon(); delete compact->builder; } else { assert(compact->outfile == nullptr); } delete compact->outfile; for (size_t i = 0; i < compact->outputs.size(); i++) { const CompactionState::Output& out = compact->outputs[i]; pending_outputs_.erase(out.number); } delete compact; } Since the latest sst file has been updated, if there are files in the builder that contains the sst files to be merged, they are removed as they are no longer needed. Finalize the Major Compaction.","title":"CleanupCompaction"},{"location":"analysis/compaction/Minor-Compaction/","text":"Minor Compaction Minor Compaction Also known as flush , this process moves data from memory (imm memtable) to disk (level 0). Overall Minor Compaction Code Flow This section only describes the Minor Compaction process. For Major Compaction, please refer to here . Overall Process MaybeScheduleCompaction checks if a Minor Compaction is needed and calls BackgroundCall if necessary. BackgroundCall invokes BackgroundCompaction and checks for the presence of an imm memtable. CompactMemTable calls the WriteLevel0Table function to execute the Minor Compaction . MaybeScheduleCompaction & BackgroundCompaction These functions are explained in the Major Compaction section, so they are omitted here. CompactMemTable This is where the actual Minor Compaction takes place. void DBImpl::CompactMemTable() { mutex_.AssertHeld(); assert(imm_ != nullptr); // Save the contents of the memtable as a new Table VersionEdit edit; Version* base = versions_->current(); base->Ref(); //(TeamCompaction)1.Main point: call the `WriteLevel0Table` function for minor compaction Status s = WriteLevel0Table(imm_, &edit, base); base->Unref(); //(TeamCompaction)2. Check if WriteLevel0Table was executed successfully if (s.ok() && shutting_down_.load(std::memory_order_acquire)) { s = Status::IOError(\"Deleting DB during memtable compaction\"); } //(TeamCompaction)3.If minor compaction is successful // Replace immutable memtable with the generated Table if (s.ok()) { edit.SetPrevLogNumber(0); edit.SetLogNumber(logfile_number_); // Earlier logs no longer needed s = versions_->LogAndApply(&edit, &mutex_); } if (s.ok()) { // Commit to the new state imm_->Unref(); imm_ = nullptr; has_imm_.store(false, std::memory_order_release); RemoveObsoleteFiles(); } else { RecordBackgroundError(s); } } To execute Minor Compaction , the WriteLevel0Table function is called with the current imm memtable and version as arguments. If an error occurs during the execution of the WriteLevel0Table function, an error message is sent. If Minor Compaction is successfully executed, the imm memtable is released. WriteLevel0Table This function moves the imm memtable to disk. Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { mutex_.AssertHeld(); const uint64_t start_micros = env_->NowMicros(); FileMetaData meta; //(TeamCompaction)1. Organizes imm memtable's meta information. meta.number = versions_->NewFileNumber(); pending_outputs_.insert(meta.number); Iterator* iter = mem->NewIterator(); Log(options_.info_log, \"Level-0 table #%llu: started\", (unsigned long long)meta.number); Status s; { mutex_.Unlock(); //(TeamCompaction)2. Change imm memtable to SST. s = BuildTable(dbname_, env_, options_, table_cache_, iter, &meta); mutex_.Lock(); } Log(options_.info_log, \"Level-0 table #%llu: %lld bytes %s\", (unsigned long long)meta.number, (unsigned long long)meta.file_size, s.ToString().c_str()); delete iter; pending_outputs_.erase(meta.number); // Note that if file_size is zero, the file has been deleted and // should not be added to the manifest. //(TeamCompaction)3. update the version int level = 0; if (s.ok() && meta.file_size > 0) { const Slice min_user_key = meta.smallest.user_key(); const Slice max_user_key = meta.largest.user_key(); if (base != nullptr) { level = base->PickLevelForMemTableOutput(min_user_key, max_user_key); } edit->AddFile(level, meta.number, meta.file_size, meta.smallest, meta.largest); } CompactionStats stats; stats.micros = env_->NowMicros() - start_micros; stats.bytes_written = meta.file_size; stats_[level].Add(stats); return s; } First, the imm memtable's information is organized. The BuildTable function is used to create an SST from the imm memtable information. If the SST is successfully created, this information is updated in the Version. Trivial Move A Trivial Move is a type of Minor Compaction where, if the newly created SST does not overlap with the keys of SSTs in each level, it is directly moved to level 2. void DBImpl::BackgroundCompaction() { // ...omitted Status status; if (c == nullptr) { // Nothing to do } else if (!is_manual && c->IsTrivialMove()) { // 1. Move file to next level assert(c->num_input_files(0) == 1); FileMetaData* f = c->input(0, 0); c->edit()->RemoveFile(c->level(), f->number); c->edit()->AddFile(c->level() + 1, f->number, f->file_size, f->smallest, f->largest); status = versions_->LogAndApply(c->edit(), &mutex_); if (!status.ok()) { RecordBackgroundError(status); } VersionSet::LevelSummaryStorage tmp; Log(options_.info_log, \"Moved #%lld to level-%d %lld bytes %s: %s\\n\", static_cast<unsigned long long>(f->number), c->level() + 1, static_cast<unsigned long long>(f->file_size), status.ToString().c_str(), versions_->LevelSummary(&tmp)); } // ...omitted } bool Compaction::IsTrivialMove() const { const VersionSet* vset = input_version_->vset_; // Avoid a move if there is lots of overlapping grandparent data. // Otherwise, the move could create a parent file that will require // a very expensive merge later on. return (num_input_files(0) == 1 && num_input_files(1) == 0 && TotalFileSize(grandparents_) <= MaxGrandParentOverlapBytes(vset->options_)); } If is_manual && c->IsTrivialMove() is true, a Trivial Move is executed.","title":"Minor Compaction"},{"location":"analysis/compaction/Minor-Compaction/#minor-compaction","text":"Minor Compaction Also known as flush , this process moves data from memory (imm memtable) to disk (level 0).","title":"Minor Compaction"},{"location":"analysis/compaction/Minor-Compaction/#overall-minor-compaction-code-flow","text":"This section only describes the Minor Compaction process. For Major Compaction, please refer to here .","title":"Overall Minor Compaction Code Flow"},{"location":"analysis/compaction/Minor-Compaction/#overall-process","text":"MaybeScheduleCompaction checks if a Minor Compaction is needed and calls BackgroundCall if necessary. BackgroundCall invokes BackgroundCompaction and checks for the presence of an imm memtable. CompactMemTable calls the WriteLevel0Table function to execute the Minor Compaction .","title":"Overall Process"},{"location":"analysis/compaction/Minor-Compaction/#maybeschedulecompaction-backgroundcompaction","text":"These functions are explained in the Major Compaction section, so they are omitted here.","title":"MaybeScheduleCompaction &amp; BackgroundCompaction"},{"location":"analysis/compaction/Minor-Compaction/#compactmemtable","text":"This is where the actual Minor Compaction takes place. void DBImpl::CompactMemTable() { mutex_.AssertHeld(); assert(imm_ != nullptr); // Save the contents of the memtable as a new Table VersionEdit edit; Version* base = versions_->current(); base->Ref(); //(TeamCompaction)1.Main point: call the `WriteLevel0Table` function for minor compaction Status s = WriteLevel0Table(imm_, &edit, base); base->Unref(); //(TeamCompaction)2. Check if WriteLevel0Table was executed successfully if (s.ok() && shutting_down_.load(std::memory_order_acquire)) { s = Status::IOError(\"Deleting DB during memtable compaction\"); } //(TeamCompaction)3.If minor compaction is successful // Replace immutable memtable with the generated Table if (s.ok()) { edit.SetPrevLogNumber(0); edit.SetLogNumber(logfile_number_); // Earlier logs no longer needed s = versions_->LogAndApply(&edit, &mutex_); } if (s.ok()) { // Commit to the new state imm_->Unref(); imm_ = nullptr; has_imm_.store(false, std::memory_order_release); RemoveObsoleteFiles(); } else { RecordBackgroundError(s); } } To execute Minor Compaction , the WriteLevel0Table function is called with the current imm memtable and version as arguments. If an error occurs during the execution of the WriteLevel0Table function, an error message is sent. If Minor Compaction is successfully executed, the imm memtable is released.","title":"CompactMemTable"},{"location":"analysis/compaction/Minor-Compaction/#writelevel0table","text":"This function moves the imm memtable to disk. Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { mutex_.AssertHeld(); const uint64_t start_micros = env_->NowMicros(); FileMetaData meta; //(TeamCompaction)1. Organizes imm memtable's meta information. meta.number = versions_->NewFileNumber(); pending_outputs_.insert(meta.number); Iterator* iter = mem->NewIterator(); Log(options_.info_log, \"Level-0 table #%llu: started\", (unsigned long long)meta.number); Status s; { mutex_.Unlock(); //(TeamCompaction)2. Change imm memtable to SST. s = BuildTable(dbname_, env_, options_, table_cache_, iter, &meta); mutex_.Lock(); } Log(options_.info_log, \"Level-0 table #%llu: %lld bytes %s\", (unsigned long long)meta.number, (unsigned long long)meta.file_size, s.ToString().c_str()); delete iter; pending_outputs_.erase(meta.number); // Note that if file_size is zero, the file has been deleted and // should not be added to the manifest. //(TeamCompaction)3. update the version int level = 0; if (s.ok() && meta.file_size > 0) { const Slice min_user_key = meta.smallest.user_key(); const Slice max_user_key = meta.largest.user_key(); if (base != nullptr) { level = base->PickLevelForMemTableOutput(min_user_key, max_user_key); } edit->AddFile(level, meta.number, meta.file_size, meta.smallest, meta.largest); } CompactionStats stats; stats.micros = env_->NowMicros() - start_micros; stats.bytes_written = meta.file_size; stats_[level].Add(stats); return s; } First, the imm memtable's information is organized. The BuildTable function is used to create an SST from the imm memtable information. If the SST is successfully created, this information is updated in the Version.","title":"WriteLevel0Table"},{"location":"analysis/compaction/Minor-Compaction/#trivial-move","text":"A Trivial Move is a type of Minor Compaction where, if the newly created SST does not overlap with the keys of SSTs in each level, it is directly moved to level 2. void DBImpl::BackgroundCompaction() { // ...omitted Status status; if (c == nullptr) { // Nothing to do } else if (!is_manual && c->IsTrivialMove()) { // 1. Move file to next level assert(c->num_input_files(0) == 1); FileMetaData* f = c->input(0, 0); c->edit()->RemoveFile(c->level(), f->number); c->edit()->AddFile(c->level() + 1, f->number, f->file_size, f->smallest, f->largest); status = versions_->LogAndApply(c->edit(), &mutex_); if (!status.ok()) { RecordBackgroundError(status); } VersionSet::LevelSummaryStorage tmp; Log(options_.info_log, \"Moved #%lld to level-%d %lld bytes %s: %s\\n\", static_cast<unsigned long long>(f->number), c->level() + 1, static_cast<unsigned long long>(f->file_size), status.ToString().c_str(), versions_->LevelSummary(&tmp)); } // ...omitted } bool Compaction::IsTrivialMove() const { const VersionSet* vset = input_version_->vset_; // Avoid a move if there is lots of overlapping grandparent data. // Otherwise, the move could create a parent file that will require // a very expensive merge later on. return (num_input_files(0) == 1 && num_input_files(1) == 0 && TotalFileSize(grandparents_) <= MaxGrandParentOverlapBytes(vset->options_)); } If is_manual && c->IsTrivialMove() is true, a Trivial Move is executed.","title":"Trivial Move"},{"location":"analysis/compaction/compaction/","text":"Compaction Compaction is one of the most complex processes in a database and can significantly impact the performance of the database. It is an internal data overlap and integration mechanism and is also an effective means of balancing read and write speeds. Compaction Type 1. Minor Compaction Trivial move 2. Major Compaction","title":"Compaction Types"},{"location":"analysis/compaction/compaction/#compaction","text":"Compaction is one of the most complex processes in a database and can significantly impact the performance of the database. It is an internal data overlap and integration mechanism and is also an effective means of balancing read and write speeds.","title":"Compaction"},{"location":"analysis/compaction/compaction/#compaction-type","text":"","title":"Compaction Type"},{"location":"analysis/compaction/compaction/#1-minor-compaction","text":"Trivial move","title":"1. Minor Compaction"},{"location":"analysis/compaction/compaction/#2-major-compaction","text":"","title":"2. Major Compaction"},{"location":"analysis/sstable/sstable-read/","text":"SSTable - Read This document explores how LevelDB finds a value for a desired key through the Get Operation, specifically focusing on the process of searching within SSTables stored in storage, using a Top-Down approach. LevelDB Get Operation LevelDB searches for a desired key in the following order: Search in MemTable If not found, search in Immutable MemTable If not found, search in storage (disk) We can see this search process in DBImpl::Get as follows: Status DBImpl::Get(const ReadOptions& options, const Slice& key, std::string* value) { // ... MemTable* mem = mem_; MemTable* imm = imm_; Version* current = versions_->current(); // ... { mutex_.Unlock(); LookupKey lkey(key, snapshot); // 1. Searching in the MemTable if (mem->Get(lkey, value, &s)) { // 2. If not in MemTable, searching in the Immutable MemTable } else if (imm != nullptr && imm->Get(lkey, value, &s)) { // 3. If not in Immutable MemTable, searching in storage(disk) } else { s = current->Get(options, lkey, value, &stats); have_stat_update = true; } mutex_.Lock(); } // ... } Process of Finding Target Key in Storage The process of finding a target key in storage begins with Version::Get and follows these steps: Select SSTables that might contain the target key from each Level Search for the target key within the selected SSTables We can see this search process in Version::Get as follows: Status Version::Get(const ReadOptions& options, const LookupKey& k, std::string* value, GetStats* stats) { // ... struct State { // ... static bool Match(void* arg, int level, FileMetaData* f) { // ... // 2. Find the target key from the selected SSTable state->s = state->vset->table_cache_->Get(*state->options, f->number, f->file_size, state->ikey, &state->saver, SaveValue); // ... } }; // ... // 1. At each level, select SSTables that may have a target key ForEachOverlapping(state.saver.user_key, state.ikey, &state, &State::Match); return state.found ? state.s : Status::NotFound(Slice()); } When calling ForEachOverlapping , Match is passed as an argument, and ForEachOverlapping performs the process of finding the target key from the selected SSTables by executing the received Match on the selected SSTables. Level 0: SSTables in Level 0 can have overlapping key ranges. Therefore, each SSTable is evaluated one by one using Linear Search. Other Levels: In levels other than Level 0, SSTable key ranges are separated. Therefore, Binary Search is used to quickly find SSTables that might contain the target key. void Version::ForEachOverlapping(Slice user_key, Slice internal_key, void* arg, bool (*func)(void*, int, FileMetaData*)) { const Comparator* ucmp = vset_->icmp_.user_comparator(); std::vector<FileMetaData*> tmp; tmp.reserve(files_[0].size()); // Level 0: Picks out SSTables via Linear Search for (uint32_t i = 0; i < files_[0].size(); i++) { FileMetaData* f = files_[0][i]; if (ucmp->Compare(user_key, f->smallest.user_key()) >= 0 && ucmp->Compare(user_key, f->largest.user_key()) <= 0) { tmp.push_back(f); } } if (!tmp.empty()) { std::sort(tmp.begin(), tmp.end(), NewestFirst); for (uint32_t i = 0; i < tmp.size(); i++) { // Perform functions received as parameter if (!(*func)(arg, 0, tmp[i])) return; } } // Ohter Levels: Picks out SSTables via Binary Search for (int level = 1; level < config::kNumLevels; level++) { size_t num_files = files_[level].size(); if (num_files == 0) continue; // FindFile : Gets index of SSTable that may have a target key via Binary search uint32_t index = FindFile(vset_->icmp_, files_[level], internal_key); if (index < num_files) { FileMetaData* f = files_[level][index]; if (ucmp->Compare(user_key, f->smallest.user_key()) < 0) { } else { // Perform functions received as parameter if (!(*func)(arg, level, f)) return; } } } } Process of Finding Target Key from Selected SSTable Starting from TableCache::Get , the process follows these steps to find the target key: Check if the SSTable object has already been cached, and if not, cache it. Search inside the SSTable to find the target key. We can see this search process in TableCache::Get as follows: Status TableCache::Get(const ReadOptions& options, uint64_t file_number, uint64_t file_size, const Slice& k, void* arg, void (*handle_result)(void*, const Slice&, const Slice&)) { Cache::Handle* handle = nullptr; // 1. Checks whether the corresponding SSTable has already been cached // If not, caches the corresponding SSTable Status s = FindTable(file_number, file_size, &handle); if (s.ok()) { Table* t = reinterpret_cast<TableAndFile*>(cache_->Value(handle))->table; // 2. Find the target key via searching inside the corresponding SSTable s = t->InternalGet(options, k, arg, handle_result); cache_->Release(handle); } return s; } When TableCache::FindTable is executed, the Table::Open method is called, which loads the Index Block and Filter Block of the corresponding SSTable into memory. Process of Searching for Target Key Inside SSTable Starting from Table::InternalGet , the process follows these steps to find the target key: Search the Index Block to identify potential Data Blocks containing the target key If using bloom filter, check if the target key exists in the identified Data Block using the bloom filter If the target key is determined to exist, create an Iterator for the identified Data Block Search the Data Block using the created Iterator If the target key is found, save its value We can see this search process in Table::InternalGet as follows: Status Table::InternalGet(const ReadOptions& options, const Slice& k, void* arg, void (*handle_result)(void*, const Slice&, const Slice&)) { Status s; // Create an Iterator for the Index Block Iterator* iiter = rep_->index_block->NewIterator(rep_->options.comparator); // 1. Search the Index Block and find the Data Block that may have a target key iiter->Seek(k); if (iiter->Valid()) { // ... // 2. If using a bloom filter, // investigate with a bloom filter if there is a target key in the found Data Block if (filter != nullptr && handle.DecodeFrom(&handle_value).ok() && !filter->KeyMayMatch(handle.offset(), k)) { // Not found } else { // 3. If it is determined that there is a target key, // reate an Iterator for the found Data Block Iterator* block_iter = BlockReader(this, options, iiter->value()); // 4. Exploring the Data Block using the generated Iterator block_iter->Seek(k); // 5. If find the target key, save the value if (block_iter->Valid()) { (*handle_result)(arg, block_iter->key(), block_iter->value()); } // ... } } // ... } Table::BlockReader Creates and returns an Iterator for the Data Block referenced by the Index Block Iterator's entry Check if the corresponding Data Block has already been cached using the Lookup method If not cached, cache the corresponding Data Block: 1) Read the contents of the corresponding Data Block using ReadBlock 2) Create a new Block object with the read contents (effectively loading the Data Block into memory) 3) Insert the loaded Data Block into the cache Create an Iterator for that Data Block If not using cache, just read the contents of the corresponding Data Block using ReadBlock and load it into memory Table::BlockReader performs the following operations: Iterator* Table::BlockReader(void* arg, const ReadOptions& options, const Slice& index_value) { // ... if (s.ok()) { BlockContents contents; if (block_cache != nullptr) { // ... // 1. Checks whether the corresponding Data Block has already been cached via Lookup cache_handle = block_cache->Lookup(key); if (cache_handle != nullptr) { block = reinterpret_cast<Block*>(block_cache->Value(cache_handle)); } else { // 2. If not, caches the corresponding Data Block // 2-1. Read the contents of the corresponding Data Block via ReadBlock s = ReadBlock(table->rep_->file, options, handle, &contents); if (s.ok()) { // 2-2. Create a new Block object with read contents // (It means loading the corresponding Data Block into memory) block = new Block(contents); if (contents.cachable && options.fill_cache) { // 2-3. Insert Loaded data block into cache cache_handle = block_cache->Insert(key, block, block->size(), &DeleteCachedBlock); } } } } else { // If do not use cache, just load the corresponding Data Block into memory s = ReadBlock(table->rep_->file, options, handle, &contents); if (s.ok()) { block = new Block(contents); } } } // 3. Create an Iterator for that Data Block Iterator* iter; if (block != nullptr) { iter = block->NewIterator(table->rep_->options.comparator); // ... } else { iter = NewErrorIterator(s); } return iter; } Block::Iter::Seek Finds the target argument within the Block Use Binary Search to find the area where the target might be located Use Linear Search to find the target within the found area Block::Iter::Seek performs the search as follows: void Seek(const Slice& target) override { // ... // 1. Find the area where the target is located via Binary Search while (left < right) { uint32_t mid = (left + right + 1) / 2; uint32_t region_offset = GetRestartPoint(mid); // ... Slice mid_key(key_ptr, non_shared); if (Compare(mid_key, target) < 0) { // if \"mid\" < \"target\" left = mid; } else { // if \"mid\" >= \"target\" right = mid - 1; } } // ... // 2. Find the target in the correspond area via Linear Search while (true) { if (!ParseNextKey()) { return; } if (Compare(key_, target) >= 0) { return; } } } Based on the described content, here's a more detailed explanation of the process of searching for a target key inside an SSTable: NewIterator : Create an Iterator for the Index Block Seek : Use the created Iterator to search the Index Block and identify potential Data Blocks containing the target key KeyMayMatch : (If using bloom filter) Use the bloom filter to check if the target key exists in the identified Data Block BlockReader : If it exists, create an Iterator for the corresponding Data Block Seek : Use the created Iterator to search within the Data Block for the target key SaveValue : If the target key is found, save its value Summary - Process of Finding Target Key in Storage Select SSTables from each Level that might contain the target key Check if each selected SSTable object has been cached, and if not, cache it During this process, the Index Block and Filter Block of the corresponding SSTable are loaded into memory Search the Index Block to identify potential Data Blocks containing the target key Use the bloom filter in the Filter Block to check if the target key exists in the identified Data Block If it exists, check if the corresponding Data Block has been cached, and if not, cache it During this process, the Data Block is loaded into memory Search within the Data Block to find the target key","title":"SSTable Read"},{"location":"analysis/sstable/sstable-read/#sstable-read","text":"This document explores how LevelDB finds a value for a desired key through the Get Operation, specifically focusing on the process of searching within SSTables stored in storage, using a Top-Down approach.","title":"SSTable - Read"},{"location":"analysis/sstable/sstable-read/#leveldb-get-operation","text":"LevelDB searches for a desired key in the following order: Search in MemTable If not found, search in Immutable MemTable If not found, search in storage (disk) We can see this search process in DBImpl::Get as follows: Status DBImpl::Get(const ReadOptions& options, const Slice& key, std::string* value) { // ... MemTable* mem = mem_; MemTable* imm = imm_; Version* current = versions_->current(); // ... { mutex_.Unlock(); LookupKey lkey(key, snapshot); // 1. Searching in the MemTable if (mem->Get(lkey, value, &s)) { // 2. If not in MemTable, searching in the Immutable MemTable } else if (imm != nullptr && imm->Get(lkey, value, &s)) { // 3. If not in Immutable MemTable, searching in storage(disk) } else { s = current->Get(options, lkey, value, &stats); have_stat_update = true; } mutex_.Lock(); } // ... }","title":"LevelDB Get Operation"},{"location":"analysis/sstable/sstable-read/#process-of-finding-target-key-in-storage","text":"The process of finding a target key in storage begins with Version::Get and follows these steps: Select SSTables that might contain the target key from each Level Search for the target key within the selected SSTables We can see this search process in Version::Get as follows: Status Version::Get(const ReadOptions& options, const LookupKey& k, std::string* value, GetStats* stats) { // ... struct State { // ... static bool Match(void* arg, int level, FileMetaData* f) { // ... // 2. Find the target key from the selected SSTable state->s = state->vset->table_cache_->Get(*state->options, f->number, f->file_size, state->ikey, &state->saver, SaveValue); // ... } }; // ... // 1. At each level, select SSTables that may have a target key ForEachOverlapping(state.saver.user_key, state.ikey, &state, &State::Match); return state.found ? state.s : Status::NotFound(Slice()); } When calling ForEachOverlapping , Match is passed as an argument, and ForEachOverlapping performs the process of finding the target key from the selected SSTables by executing the received Match on the selected SSTables. Level 0: SSTables in Level 0 can have overlapping key ranges. Therefore, each SSTable is evaluated one by one using Linear Search. Other Levels: In levels other than Level 0, SSTable key ranges are separated. Therefore, Binary Search is used to quickly find SSTables that might contain the target key. void Version::ForEachOverlapping(Slice user_key, Slice internal_key, void* arg, bool (*func)(void*, int, FileMetaData*)) { const Comparator* ucmp = vset_->icmp_.user_comparator(); std::vector<FileMetaData*> tmp; tmp.reserve(files_[0].size()); // Level 0: Picks out SSTables via Linear Search for (uint32_t i = 0; i < files_[0].size(); i++) { FileMetaData* f = files_[0][i]; if (ucmp->Compare(user_key, f->smallest.user_key()) >= 0 && ucmp->Compare(user_key, f->largest.user_key()) <= 0) { tmp.push_back(f); } } if (!tmp.empty()) { std::sort(tmp.begin(), tmp.end(), NewestFirst); for (uint32_t i = 0; i < tmp.size(); i++) { // Perform functions received as parameter if (!(*func)(arg, 0, tmp[i])) return; } } // Ohter Levels: Picks out SSTables via Binary Search for (int level = 1; level < config::kNumLevels; level++) { size_t num_files = files_[level].size(); if (num_files == 0) continue; // FindFile : Gets index of SSTable that may have a target key via Binary search uint32_t index = FindFile(vset_->icmp_, files_[level], internal_key); if (index < num_files) { FileMetaData* f = files_[level][index]; if (ucmp->Compare(user_key, f->smallest.user_key()) < 0) { } else { // Perform functions received as parameter if (!(*func)(arg, level, f)) return; } } } }","title":"Process of Finding Target Key in Storage"},{"location":"analysis/sstable/sstable-read/#process-of-finding-target-key-from-selected-sstable","text":"Starting from TableCache::Get , the process follows these steps to find the target key: Check if the SSTable object has already been cached, and if not, cache it. Search inside the SSTable to find the target key. We can see this search process in TableCache::Get as follows: Status TableCache::Get(const ReadOptions& options, uint64_t file_number, uint64_t file_size, const Slice& k, void* arg, void (*handle_result)(void*, const Slice&, const Slice&)) { Cache::Handle* handle = nullptr; // 1. Checks whether the corresponding SSTable has already been cached // If not, caches the corresponding SSTable Status s = FindTable(file_number, file_size, &handle); if (s.ok()) { Table* t = reinterpret_cast<TableAndFile*>(cache_->Value(handle))->table; // 2. Find the target key via searching inside the corresponding SSTable s = t->InternalGet(options, k, arg, handle_result); cache_->Release(handle); } return s; } When TableCache::FindTable is executed, the Table::Open method is called, which loads the Index Block and Filter Block of the corresponding SSTable into memory.","title":"Process of Finding Target Key from Selected SSTable"},{"location":"analysis/sstable/sstable-read/#process-of-searching-for-target-key-inside-sstable","text":"Starting from Table::InternalGet , the process follows these steps to find the target key: Search the Index Block to identify potential Data Blocks containing the target key If using bloom filter, check if the target key exists in the identified Data Block using the bloom filter If the target key is determined to exist, create an Iterator for the identified Data Block Search the Data Block using the created Iterator If the target key is found, save its value We can see this search process in Table::InternalGet as follows: Status Table::InternalGet(const ReadOptions& options, const Slice& k, void* arg, void (*handle_result)(void*, const Slice&, const Slice&)) { Status s; // Create an Iterator for the Index Block Iterator* iiter = rep_->index_block->NewIterator(rep_->options.comparator); // 1. Search the Index Block and find the Data Block that may have a target key iiter->Seek(k); if (iiter->Valid()) { // ... // 2. If using a bloom filter, // investigate with a bloom filter if there is a target key in the found Data Block if (filter != nullptr && handle.DecodeFrom(&handle_value).ok() && !filter->KeyMayMatch(handle.offset(), k)) { // Not found } else { // 3. If it is determined that there is a target key, // reate an Iterator for the found Data Block Iterator* block_iter = BlockReader(this, options, iiter->value()); // 4. Exploring the Data Block using the generated Iterator block_iter->Seek(k); // 5. If find the target key, save the value if (block_iter->Valid()) { (*handle_result)(arg, block_iter->key(), block_iter->value()); } // ... } } // ... }","title":"Process of Searching for Target Key Inside SSTable"},{"location":"analysis/sstable/sstable-read/#tableblockreader","text":"Creates and returns an Iterator for the Data Block referenced by the Index Block Iterator's entry Check if the corresponding Data Block has already been cached using the Lookup method If not cached, cache the corresponding Data Block: 1) Read the contents of the corresponding Data Block using ReadBlock 2) Create a new Block object with the read contents (effectively loading the Data Block into memory) 3) Insert the loaded Data Block into the cache Create an Iterator for that Data Block If not using cache, just read the contents of the corresponding Data Block using ReadBlock and load it into memory Table::BlockReader performs the following operations: Iterator* Table::BlockReader(void* arg, const ReadOptions& options, const Slice& index_value) { // ... if (s.ok()) { BlockContents contents; if (block_cache != nullptr) { // ... // 1. Checks whether the corresponding Data Block has already been cached via Lookup cache_handle = block_cache->Lookup(key); if (cache_handle != nullptr) { block = reinterpret_cast<Block*>(block_cache->Value(cache_handle)); } else { // 2. If not, caches the corresponding Data Block // 2-1. Read the contents of the corresponding Data Block via ReadBlock s = ReadBlock(table->rep_->file, options, handle, &contents); if (s.ok()) { // 2-2. Create a new Block object with read contents // (It means loading the corresponding Data Block into memory) block = new Block(contents); if (contents.cachable && options.fill_cache) { // 2-3. Insert Loaded data block into cache cache_handle = block_cache->Insert(key, block, block->size(), &DeleteCachedBlock); } } } } else { // If do not use cache, just load the corresponding Data Block into memory s = ReadBlock(table->rep_->file, options, handle, &contents); if (s.ok()) { block = new Block(contents); } } } // 3. Create an Iterator for that Data Block Iterator* iter; if (block != nullptr) { iter = block->NewIterator(table->rep_->options.comparator); // ... } else { iter = NewErrorIterator(s); } return iter; }","title":"Table::BlockReader"},{"location":"analysis/sstable/sstable-read/#blockiterseek","text":"Finds the target argument within the Block Use Binary Search to find the area where the target might be located Use Linear Search to find the target within the found area Block::Iter::Seek performs the search as follows: void Seek(const Slice& target) override { // ... // 1. Find the area where the target is located via Binary Search while (left < right) { uint32_t mid = (left + right + 1) / 2; uint32_t region_offset = GetRestartPoint(mid); // ... Slice mid_key(key_ptr, non_shared); if (Compare(mid_key, target) < 0) { // if \"mid\" < \"target\" left = mid; } else { // if \"mid\" >= \"target\" right = mid - 1; } } // ... // 2. Find the target in the correspond area via Linear Search while (true) { if (!ParseNextKey()) { return; } if (Compare(key_, target) >= 0) { return; } } } Based on the described content, here's a more detailed explanation of the process of searching for a target key inside an SSTable: NewIterator : Create an Iterator for the Index Block Seek : Use the created Iterator to search the Index Block and identify potential Data Blocks containing the target key KeyMayMatch : (If using bloom filter) Use the bloom filter to check if the target key exists in the identified Data Block BlockReader : If it exists, create an Iterator for the corresponding Data Block Seek : Use the created Iterator to search within the Data Block for the target key SaveValue : If the target key is found, save its value","title":"Block::Iter::Seek"},{"location":"analysis/sstable/sstable-read/#summary-process-of-finding-target-key-in-storage","text":"Select SSTables from each Level that might contain the target key Check if each selected SSTable object has been cached, and if not, cache it During this process, the Index Block and Filter Block of the corresponding SSTable are loaded into memory Search the Index Block to identify potential Data Blocks containing the target key Use the bloom filter in the Filter Block to check if the target key exists in the identified Data Block If it exists, check if the corresponding Data Block has been cached, and if not, cache it During this process, the Data Block is loaded into memory Search within the Data Block to find the target key","title":"Summary - Process of Finding Target Key in Storage"},{"location":"analysis/sstable/sstable-write/","text":"SSTable - Write SSTables are created in the following situations: When a Flush (Minor Compaction) occurs from the MemTable When Compaction occurs in storage This document focuses on how SSTables are created when a Flush occurs from the MemTable. When a Flush Occurs from the MemTable The process of a Flush occurring from the MemTable is as follows: CompactMemTable is called, which in turn calls WriteLevel0Table , resulting in the creation of an SSTable. At this point, BuildTable is the function that actually creates the SSTable. The flow of BuildTable is as follows: Overall Process Steps Create an instance of TableBuilder Add key-value pairs from the MemTable one by one using the Add method of TableBuilder Complete the process of creating the SSTable using the Finish method of TableBuilder Write the contents in the WritableFile to storage Load the SSTable stored in storage into the cache and check if it is available Status BuildTable(const std::string& dbname, Env* env, const Options& options, TableCache* table_cache, Iterator* iter, FileMetaData* meta) { Status s; meta->file_size = 0; // Make sure the Iterator points to the first element iter->SeekToFirst(); std::string fname = TableFileName(dbname, meta->number); if (iter->Valid()) { WritableFile* file; s = env->NewWritableFile(fname, &file); if (!s.ok()) { return s; } // 1. Create an instance of TableBuilder TableBuilder* builder = new TableBuilder(options, file); meta->smallest.DecodeFrom(iter->key()); Slice key; // 2. Add key-value pairs of MemTable one by one via TableBuilder's \"Add\" method for (; iter->Valid(); iter->Next()) { key = iter->key(); builder->Add(key, iter->value()); } if (!key.empty()) { meta->largest.DecodeFrom(key); } // 3. Complete the process of creating SSTable via TableBuilder's \"Finish\" method s = builder->Finish(); if (s.ok()) { meta->file_size = builder->FileSize(); assert(meta->file_size > 0); } delete builder; // 4. Write the contents in the WritableFile to storage if (s.ok()) { s = file->Sync(); } if (s.ok()) { s = file->Close(); } delete file; file = nullptr; if (s.ok()) { // 5. Put the SSTable stored in storage into cache and check if it is available Iterator* it = table_cache->NewIterator(ReadOptions(), meta->number, meta->file_size); s = it->status(); delete it; } } // Check for Iterator related errors if (!iter->status().ok()) { s = iter->status(); } if (s.ok() && meta->file_size > 0) { // Keep it } else { env->RemoveFile(fname); } return s; } TableBuilder::Add The role of passing the key-value pair currently referenced by the Iterator to each BlockBuilder within the TableBuilder. If the Data Block being created by the BlockBuilder is empty, meaning a new Data Block is being started, add a new entry to the Index Block. The entry added at this time is not for the newly started Data Block but for the Data Block that was being created just before. If using a Bloom Filter, update the Filter Block as well. Add data to the Data Block. If the Data Block being created is full (i.e., exceeds the block size specified by the option), call Flush . void TableBuilder::Add(const Slice& key, const Slice& value) { Rep* r = rep_; // ... // 1. If the Data Block that BlockBuilder is creating is empty, // add a new entry to the Index Block if (r->pending_index_entry) { assert(r->data_block.empty()); r->options.comparator->FindShortestSeparator(&r->last_key, key); std::string handle_encoding; r->pending_handle.EncodeTo(&handle_encoding); r->index_block.Add(r->last_key, Slice(handle_encoding)); r->pending_index_entry = false; } // 2. If using Bloom Filter, update the Filter Block as well if (r->filter_block != nullptr) { r->filter_block->AddKey(key); } r->last_key.assign(key.data(), key.size()); r->num_entries++; // 3. Add data to the Data Block r->data_block.Add(key, value); const size_t estimated_block_size = r->data_block.CurrentSizeEstimate(); // 4. If the Data Block being created is full, call \"Flush\" if (estimated_block_size >= r->options.block_size) { Flush(); } } TableBuilder::Finish Called when Add is finished for all key-value pairs in the MemTable, and it finalizes the SSTable being written. Call Flush . If using a Bloom Filter, add the Filter Block to the WritableFile using FilterBlockBuilder . Add the Meta Index Block to the WritableFile . Add the Index Block being created by BlockBuilder to the WritableFile . Add the Footer to the WritableFile . Status TableBuilder::Finish() { Rep* r = rep_; // 1. Call \"Flush\" Flush(); assert(!r->closed); r->closed = true; BlockHandle filter_block_handle, metaindex_block_handle, index_block_handle; // 2. If using Bloom Filter, add the Filter Block to the WritableFile. if (ok() && r->filter_block != nullptr) { WriteRawBlock(r->filter_block->Finish(), kNoCompression, &filter_block_handle); } // 3. Add the Meta Index Block to the WritableFile if (ok()) { BlockBuilder meta_index_block(&r->options); if (r->filter_block != nullptr) { std::string key = \"filter.\"; key.append(r->options.filter_policy->Name()); std::string handle_encoding; filter_block_handle.EncodeTo(&handle_encoding); meta_index_block.Add(key, handle_encoding); } WriteBlock(&meta_index_block, &metaindex_block_handle); } // 4. Add the Index Block to the WritableFile. if (ok()) { if (r->pending_index_entry) { r->options.comparator->FindShortSuccessor(&r->last_key); std::string handle_encoding; r->pending_handle.EncodeTo(&handle_encoding); r->index_block.Add(r->last_key, Slice(handle_encoding)); r->pending_index_entry = false; } WriteBlock(&r->index_block, &index_block_handle); } // 5. Add the Footer to the WritableFile. if (ok()) { Footer footer; footer.set_metaindex_handle(metaindex_block_handle); footer.set_index_handle(index_block_handle); std::string footer_encoding; footer.EncodeTo(&footer_encoding); r->status = r->file->Append(footer_encoding); if (r->status.ok()) { r->offset += footer_encoding.size(); } } return r->status; } TableBuilder::Flush The role of writing the Data Block being created to storage. Add the contents of the Data Block being created to the WritableFile . Write the contents of the WritableFile to storage. If using a Bloom Filter, create a new Bloom Filter. void TableBuilder::Flush() { Rep* r = rep_; // ... // 1. Add the contents of the Data Block being created to the WritableFile WriteBlock(&r->data_block, &r->pending_handle); if (ok()) { r->pending_index_entry = true; // 2. Write the contents of WritableFile to storage r->status = r->file->Flush(); } // 3. If using Bloom Filter, create a new Bloom Filter if (r->filter_block != nullptr) { r->filter_block->StartBlock(r->offset); } }","title":"SSTable Write"},{"location":"analysis/sstable/sstable-write/#sstable-write","text":"SSTables are created in the following situations: When a Flush (Minor Compaction) occurs from the MemTable When Compaction occurs in storage This document focuses on how SSTables are created when a Flush occurs from the MemTable.","title":"SSTable - Write"},{"location":"analysis/sstable/sstable-write/#when-a-flush-occurs-from-the-memtable","text":"The process of a Flush occurring from the MemTable is as follows: CompactMemTable is called, which in turn calls WriteLevel0Table , resulting in the creation of an SSTable. At this point, BuildTable is the function that actually creates the SSTable. The flow of BuildTable is as follows:","title":"When a Flush Occurs from the MemTable"},{"location":"analysis/sstable/sstable-write/#overall-process","text":"","title":"Overall Process"},{"location":"analysis/sstable/sstable-write/#steps","text":"Create an instance of TableBuilder Add key-value pairs from the MemTable one by one using the Add method of TableBuilder Complete the process of creating the SSTable using the Finish method of TableBuilder Write the contents in the WritableFile to storage Load the SSTable stored in storage into the cache and check if it is available Status BuildTable(const std::string& dbname, Env* env, const Options& options, TableCache* table_cache, Iterator* iter, FileMetaData* meta) { Status s; meta->file_size = 0; // Make sure the Iterator points to the first element iter->SeekToFirst(); std::string fname = TableFileName(dbname, meta->number); if (iter->Valid()) { WritableFile* file; s = env->NewWritableFile(fname, &file); if (!s.ok()) { return s; } // 1. Create an instance of TableBuilder TableBuilder* builder = new TableBuilder(options, file); meta->smallest.DecodeFrom(iter->key()); Slice key; // 2. Add key-value pairs of MemTable one by one via TableBuilder's \"Add\" method for (; iter->Valid(); iter->Next()) { key = iter->key(); builder->Add(key, iter->value()); } if (!key.empty()) { meta->largest.DecodeFrom(key); } // 3. Complete the process of creating SSTable via TableBuilder's \"Finish\" method s = builder->Finish(); if (s.ok()) { meta->file_size = builder->FileSize(); assert(meta->file_size > 0); } delete builder; // 4. Write the contents in the WritableFile to storage if (s.ok()) { s = file->Sync(); } if (s.ok()) { s = file->Close(); } delete file; file = nullptr; if (s.ok()) { // 5. Put the SSTable stored in storage into cache and check if it is available Iterator* it = table_cache->NewIterator(ReadOptions(), meta->number, meta->file_size); s = it->status(); delete it; } } // Check for Iterator related errors if (!iter->status().ok()) { s = iter->status(); } if (s.ok() && meta->file_size > 0) { // Keep it } else { env->RemoveFile(fname); } return s; }","title":"Steps"},{"location":"analysis/sstable/sstable-write/#tablebuilderadd","text":"The role of passing the key-value pair currently referenced by the Iterator to each BlockBuilder within the TableBuilder. If the Data Block being created by the BlockBuilder is empty, meaning a new Data Block is being started, add a new entry to the Index Block. The entry added at this time is not for the newly started Data Block but for the Data Block that was being created just before. If using a Bloom Filter, update the Filter Block as well. Add data to the Data Block. If the Data Block being created is full (i.e., exceeds the block size specified by the option), call Flush . void TableBuilder::Add(const Slice& key, const Slice& value) { Rep* r = rep_; // ... // 1. If the Data Block that BlockBuilder is creating is empty, // add a new entry to the Index Block if (r->pending_index_entry) { assert(r->data_block.empty()); r->options.comparator->FindShortestSeparator(&r->last_key, key); std::string handle_encoding; r->pending_handle.EncodeTo(&handle_encoding); r->index_block.Add(r->last_key, Slice(handle_encoding)); r->pending_index_entry = false; } // 2. If using Bloom Filter, update the Filter Block as well if (r->filter_block != nullptr) { r->filter_block->AddKey(key); } r->last_key.assign(key.data(), key.size()); r->num_entries++; // 3. Add data to the Data Block r->data_block.Add(key, value); const size_t estimated_block_size = r->data_block.CurrentSizeEstimate(); // 4. If the Data Block being created is full, call \"Flush\" if (estimated_block_size >= r->options.block_size) { Flush(); } }","title":"TableBuilder::Add"},{"location":"analysis/sstable/sstable-write/#tablebuilderfinish","text":"Called when Add is finished for all key-value pairs in the MemTable, and it finalizes the SSTable being written. Call Flush . If using a Bloom Filter, add the Filter Block to the WritableFile using FilterBlockBuilder . Add the Meta Index Block to the WritableFile . Add the Index Block being created by BlockBuilder to the WritableFile . Add the Footer to the WritableFile . Status TableBuilder::Finish() { Rep* r = rep_; // 1. Call \"Flush\" Flush(); assert(!r->closed); r->closed = true; BlockHandle filter_block_handle, metaindex_block_handle, index_block_handle; // 2. If using Bloom Filter, add the Filter Block to the WritableFile. if (ok() && r->filter_block != nullptr) { WriteRawBlock(r->filter_block->Finish(), kNoCompression, &filter_block_handle); } // 3. Add the Meta Index Block to the WritableFile if (ok()) { BlockBuilder meta_index_block(&r->options); if (r->filter_block != nullptr) { std::string key = \"filter.\"; key.append(r->options.filter_policy->Name()); std::string handle_encoding; filter_block_handle.EncodeTo(&handle_encoding); meta_index_block.Add(key, handle_encoding); } WriteBlock(&meta_index_block, &metaindex_block_handle); } // 4. Add the Index Block to the WritableFile. if (ok()) { if (r->pending_index_entry) { r->options.comparator->FindShortSuccessor(&r->last_key); std::string handle_encoding; r->pending_handle.EncodeTo(&handle_encoding); r->index_block.Add(r->last_key, Slice(handle_encoding)); r->pending_index_entry = false; } WriteBlock(&r->index_block, &index_block_handle); } // 5. Add the Footer to the WritableFile. if (ok()) { Footer footer; footer.set_metaindex_handle(metaindex_block_handle); footer.set_index_handle(index_block_handle); std::string footer_encoding; footer.EncodeTo(&footer_encoding); r->status = r->file->Append(footer_encoding); if (r->status.ok()) { r->offset += footer_encoding.size(); } } return r->status; }","title":"TableBuilder::Finish"},{"location":"analysis/sstable/sstable-write/#tablebuilderflush","text":"The role of writing the Data Block being created to storage. Add the contents of the Data Block being created to the WritableFile . Write the contents of the WritableFile to storage. If using a Bloom Filter, create a new Bloom Filter. void TableBuilder::Flush() { Rep* r = rep_; // ... // 1. Add the contents of the Data Block being created to the WritableFile WriteBlock(&r->data_block, &r->pending_handle); if (ok()) { r->pending_index_entry = true; // 2. Write the contents of WritableFile to storage r->status = r->file->Flush(); } // 3. If using Bloom Filter, create a new Bloom Filter if (r->filter_block != nullptr) { r->filter_block->StartBlock(r->offset); } }","title":"TableBuilder::Flush"},{"location":"analysis/sstable/sstable/","text":"SSTable LevelDB is based on the LSM tree (Log Structured Merge Tree), which means that when writing, data is not written directly to the disk but is first written to the Log and then to the MemTable. When the MemTable is full, it sends the data to the Immutable MemTable, and from the Immutable MemTable, it flushes the data to storage (i.e., disk). When the data in memory is flushed to storage in this way, LevelDB stores the data in a data structure called SSTable (Sorted String Table). This article deals with this SSTable. SSTable format Physically, an SSTable is divided into 4KB blocks, each containing fields for storing data, compression type (indicating whether the data stored in the block is compressed and, if so, which algorithm was used), and CRC checks for error detection. From a logical perspective, an SSTable can be viewed as having the following structure: Data Block This block stores key-value pairs. The structure is as follows: Since SSTables hold keys in a sorted manner, each key can have many overlapping parts with others. To address this issue and improve space efficiency, LevelDB is designed to store only the non-overlapping part of the key (excluding the prefix shared with the previous record) rather than the entire key value. This method reduces the memory used for storing keys, but it also has the downside of degrading read performance. Although SSTables are internally sorted, allowing for binary search, the partial key storage means binary search cannot be effectively used. To solve this problem, LevelDB introduces the concept of a Restart Point . Instead of storing only partial keys for all entries in a Data Block, it stores the full key value every k entries (default is 16). These entries with full key values are called Restart Points . As shown in the diagram, the Data Block internally stores the positions of each Restart Point . By introducing Restart Points and rewriting the full key value every k records, the entries within a Data Block form sections based on these Restart Points . Since SSTables internally maintain keys in a sorted order, the entries corresponding to each Restart Point are also sorted. Therefore, you can use binary search to find the section where the key you are looking for might be, thus improving read performance. Filter Block This block contains Bloom Filters. The structure is as follows: Filters refer to Bloom Filters, and the Filter offset contains the starting offset information for each Filter. The Filter offset's offset contains the offset information for Filter 1. To read a Bloom Filter, you first read the Filter offset's offset, then use it to read the desired Filter's offset, and finally navigate to and read the corresponding Filter. Meta Index Block This block contains the index information for the Filter Block and stores only one related record. Index Block This block contains the index information for each Data Block. The structure is as follows: Each entry contains not only the index information for the corresponding Data Block but also the size of the Data Block and the largest key value in that Data Block. Footer This block has a fixed size of 48 bytes and contains the index information for the Meta Index Block and the Index Block. The structure is as follows: Note - Block Entry Structure Data Block, Index Block, and Meta Index Block are all created through the same object called BlockBuilder, so their structures are essentially the same. This means that not only Data Blocks but also Index Blocks and Meta Index Blocks have Restart Points . Just as Data Blocks store Key-Value Pairs, Index Blocks and Meta Index Blocks also store a kind of Key-Value Pair. For example, in an Index Block, the Max key field of each entry acts as the key, while the offset and length fields act as the value. The structure of the entries that make up these blocks is as follows: Shared key length: Length of the part of the key that overlaps with the previous record Unshared key length: Length of the part of the key that does not overlap with the previous record Value length: Length of the value Unshared key content: Part of the key that does not overlap with the previous record Value: The value itself Example - restart_interval = 3 - entry 1: key = abc, value = v1 - entry 2: key = abe, value = v2 - entry 3: key = abg, value = v3 - entry 4: key = chesh, value = v4 - entry 5: key = chosh, value = v5 - entry 6: key = chush, value = v6 By setting the restart_interval to 3, you can see that a Restart Point is marked every 3 records, and the entries corresponding to these Restart Points store the full key value, unlike other records. SSTable - Write & Read Write - Process of Creating an SSTable Read - Process of Finding a Key in an SSTable","title":"SSTable Structure"},{"location":"analysis/sstable/sstable/#sstable","text":"LevelDB is based on the LSM tree (Log Structured Merge Tree), which means that when writing, data is not written directly to the disk but is first written to the Log and then to the MemTable. When the MemTable is full, it sends the data to the Immutable MemTable, and from the Immutable MemTable, it flushes the data to storage (i.e., disk). When the data in memory is flushed to storage in this way, LevelDB stores the data in a data structure called SSTable (Sorted String Table). This article deals with this SSTable.","title":"SSTable"},{"location":"analysis/sstable/sstable/#sstable-format","text":"Physically, an SSTable is divided into 4KB blocks, each containing fields for storing data, compression type (indicating whether the data stored in the block is compressed and, if so, which algorithm was used), and CRC checks for error detection. From a logical perspective, an SSTable can be viewed as having the following structure:","title":"SSTable format"},{"location":"analysis/sstable/sstable/#data-block","text":"This block stores key-value pairs. The structure is as follows: Since SSTables hold keys in a sorted manner, each key can have many overlapping parts with others. To address this issue and improve space efficiency, LevelDB is designed to store only the non-overlapping part of the key (excluding the prefix shared with the previous record) rather than the entire key value. This method reduces the memory used for storing keys, but it also has the downside of degrading read performance. Although SSTables are internally sorted, allowing for binary search, the partial key storage means binary search cannot be effectively used. To solve this problem, LevelDB introduces the concept of a Restart Point . Instead of storing only partial keys for all entries in a Data Block, it stores the full key value every k entries (default is 16). These entries with full key values are called Restart Points . As shown in the diagram, the Data Block internally stores the positions of each Restart Point . By introducing Restart Points and rewriting the full key value every k records, the entries within a Data Block form sections based on these Restart Points . Since SSTables internally maintain keys in a sorted order, the entries corresponding to each Restart Point are also sorted. Therefore, you can use binary search to find the section where the key you are looking for might be, thus improving read performance.","title":"Data Block"},{"location":"analysis/sstable/sstable/#filter-block","text":"This block contains Bloom Filters. The structure is as follows: Filters refer to Bloom Filters, and the Filter offset contains the starting offset information for each Filter. The Filter offset's offset contains the offset information for Filter 1. To read a Bloom Filter, you first read the Filter offset's offset, then use it to read the desired Filter's offset, and finally navigate to and read the corresponding Filter.","title":"Filter Block"},{"location":"analysis/sstable/sstable/#meta-index-block","text":"This block contains the index information for the Filter Block and stores only one related record.","title":"Meta Index Block"},{"location":"analysis/sstable/sstable/#index-block","text":"This block contains the index information for each Data Block. The structure is as follows: Each entry contains not only the index information for the corresponding Data Block but also the size of the Data Block and the largest key value in that Data Block.","title":"Index Block"},{"location":"analysis/sstable/sstable/#footer","text":"This block has a fixed size of 48 bytes and contains the index information for the Meta Index Block and the Index Block. The structure is as follows:","title":"Footer"},{"location":"analysis/sstable/sstable/#note-block-entry-structure","text":"Data Block, Index Block, and Meta Index Block are all created through the same object called BlockBuilder, so their structures are essentially the same. This means that not only Data Blocks but also Index Blocks and Meta Index Blocks have Restart Points . Just as Data Blocks store Key-Value Pairs, Index Blocks and Meta Index Blocks also store a kind of Key-Value Pair. For example, in an Index Block, the Max key field of each entry acts as the key, while the offset and length fields act as the value. The structure of the entries that make up these blocks is as follows: Shared key length: Length of the part of the key that overlaps with the previous record Unshared key length: Length of the part of the key that does not overlap with the previous record Value length: Length of the value Unshared key content: Part of the key that does not overlap with the previous record Value: The value itself","title":"Note - Block Entry Structure"},{"location":"analysis/sstable/sstable/#example","text":"- restart_interval = 3 - entry 1: key = abc, value = v1 - entry 2: key = abe, value = v2 - entry 3: key = abg, value = v3 - entry 4: key = chesh, value = v4 - entry 5: key = chosh, value = v5 - entry 6: key = chush, value = v6 By setting the restart_interval to 3, you can see that a Restart Point is marked every 3 records, and the entries corresponding to these Restart Points store the full key value, unlike other records.","title":"Example"},{"location":"analysis/sstable/sstable/#sstable-write-read","text":"Write - Process of Creating an SSTable Read - Process of Finding a Key in an SSTable","title":"SSTable - Write &amp; Read"},{"location":"benchmarks/bloomfilter/","text":"Bloom Filter (Source: https://en.wikipedia.org/wiki/Bloom_filter) A Bloom filter is a probabilistic data structure that quickly checks for the presence of a specific key in a data block. In db_bench , you can adjust the value of bloom_bits to create a Bloom filter of your desired size. This experiment aims to explore the performance differences based on the value of bloom_bits and the reasons behind them. Hypothesis 1 As the size of the Bloom filter increases, false positives decrease, improving performance. However, the overhead required to read and write the Bloom filter also increases, creating a trade-off. Considering this trade-off, the optimal bloom_bits value would be the default value used by LevelDB, which is 10 bits. Fillrandom Performance Measurement Bits per key Throughput Latency -1 28.21 4.327 0 28.13 4.389 1 28.01 4.472 5 27.92 4.475 10 27.56 4.486 30 26.72 4.729 50 25.49 5.292 100 25.46 5.382 1000 19.11 7.781 As the number of bits per key increases, latency increases and throughput decreases. Readrandom Performance Measurement Bits per key Latency -1 3.20 0 2.84 1 2.81 5 2.78 10 2.41 30 2.97 50 3.40 100 3.56 1000 5.69 As the number of bits per key increases, latency initially decreases but then increases again. This suggests that initially, the performance improvement from the Bloom filter outweighs the overhead, but eventually, the overhead becomes too large, reducing throughput. Additionally, the number 10 is identified as the threshold that enhances read performance without significantly degrading write performance, which is why LevelDB uses 10 bits per key as the default value. Hypothesis 2 The size of the Bloom filter is determined by the number of bits per key and the number of data items. If these values are fixed and only the data size is changed, the overhead from the Bloom filter should remain unchanged. Result When the value_size is 128 Bytes, using a Bloom filter results in a 7% decrease in write performance and a 7% increase in read performance. When the value_size is increased tenfold to 1280 Bytes, the write performance decreases by 8% and the read performance increases by 9.5%. Contrary to the hypothesis, it appears that value_size also affects the performance difference caused by the Bloom filter. This is likely because, although the overall size of the Bloom filter remains the same, each Bloom filter is divided into smaller parts, requiring additional overhead for reading and writing. Hypothesis 3 When the number of bits per key in a Bloom filter is fixed, the mathematically proven optimal number of hashes is ln2 * bloom_bits \u2248 0.69 * bloom_bits. However, since the number of hashes cannot be a fraction, if we use the default value of 10 bits in LevelDB, the number of hashes becomes 6 instead of 6.9. Therefore, using 7 hash functions, which is closer to the theoretically optimal value, should improve performance. Result The results were almost identical, but using 7 hash functions slightly decreased write performance and slightly improved read performance, though the difference was minimal. Mathematically, the difference in the probability of false positives between using 6 and 7 hash functions is 0.00024, which is negligible and likely equivalent to the additional overhead from processing one more hash during reading. Hypothesis 4 If we use 9 bits (6.21 <-> 6) or 12 bits (8.28 <-> 8), which are closer to the theoretically optimal number of hash functions, performance should improve. Result Contrary to expectations, the best results were obtained when using 10 bits. Using 12 bits and 8 hashes significantly reduced the probability of false positives compared to the default 10 bits and 6 hashes, but considering the overhead required to process 2 additional bits and 2 hashes, the difference in read performance was not significant, and write performance decreased. When using 6 hashes and 9 bits, the write performance was similar to the default case, but read performance decreased. This is because, contrary to my expectations, the optimal number of hashes is the value that minimizes false positives when the bloom_bits value is fixed. Conversely, if the number of hashes is fixed, false positives decrease as the bloom_bits value increases. Hypothesis 5 From the experimental results above, it was found that with the same number of hashes, a larger bloom_bits value results in better read performance. Considering that the bloom_bits value has a greater impact on false positives than the number of hashes, and that the overhead of the Bloom filter is largely due to the number of hashes, it is hypothesized that fixing the number of hashes and increasing the bloom_bits value will yield better performance results. Result Using 6 hash functions with 10 bits and 30 bits each resulted in a slight increase in read performance but a decrease in write performance. Despite adjusting various variables and conducting numerous measurements, it seems difficult to achieve better results than using the default 10 bits with 6 hash functions through simple numerical adjustments. Hypothesis 6 False positives can only occur if the data to be read does not exist. Therefore, in the ReadMissing benchmark, where only non-existent keys are searched, the impact of false positives will be greater than in ReadRandom. Result Unlike FillRandom, which consistently decreases in performance, and ReadRandom, which increases and then decreases around 10 bits, ReadMissing shows a significant performance increase as soon as a Bloom filter is used, and performance continues to improve as the number of bits increases. This is likely because, as hypothesized, in ReadMissing, the performance improvement from reducing false positives outweighs the overhead from increasing the Bloom filter size.","title":"Bloom Filter"},{"location":"benchmarks/bloomfilter/#bloom-filter","text":"(Source: https://en.wikipedia.org/wiki/Bloom_filter) A Bloom filter is a probabilistic data structure that quickly checks for the presence of a specific key in a data block. In db_bench , you can adjust the value of bloom_bits to create a Bloom filter of your desired size. This experiment aims to explore the performance differences based on the value of bloom_bits and the reasons behind them.","title":"Bloom Filter"},{"location":"benchmarks/bloomfilter/#hypothesis-1","text":"As the size of the Bloom filter increases, false positives decrease, improving performance. However, the overhead required to read and write the Bloom filter also increases, creating a trade-off. Considering this trade-off, the optimal bloom_bits value would be the default value used by LevelDB, which is 10 bits.","title":"Hypothesis 1"},{"location":"benchmarks/bloomfilter/#fillrandom-performance-measurement","text":"Bits per key Throughput Latency -1 28.21 4.327 0 28.13 4.389 1 28.01 4.472 5 27.92 4.475 10 27.56 4.486 30 26.72 4.729 50 25.49 5.292 100 25.46 5.382 1000 19.11 7.781 As the number of bits per key increases, latency increases and throughput decreases.","title":"Fillrandom Performance Measurement"},{"location":"benchmarks/bloomfilter/#readrandom-performance-measurement","text":"Bits per key Latency -1 3.20 0 2.84 1 2.81 5 2.78 10 2.41 30 2.97 50 3.40 100 3.56 1000 5.69 As the number of bits per key increases, latency initially decreases but then increases again. This suggests that initially, the performance improvement from the Bloom filter outweighs the overhead, but eventually, the overhead becomes too large, reducing throughput. Additionally, the number 10 is identified as the threshold that enhances read performance without significantly degrading write performance, which is why LevelDB uses 10 bits per key as the default value.","title":"Readrandom Performance Measurement"},{"location":"benchmarks/bloomfilter/#hypothesis-2","text":"The size of the Bloom filter is determined by the number of bits per key and the number of data items. If these values are fixed and only the data size is changed, the overhead from the Bloom filter should remain unchanged.","title":"Hypothesis 2"},{"location":"benchmarks/bloomfilter/#result","text":"When the value_size is 128 Bytes, using a Bloom filter results in a 7% decrease in write performance and a 7% increase in read performance. When the value_size is increased tenfold to 1280 Bytes, the write performance decreases by 8% and the read performance increases by 9.5%. Contrary to the hypothesis, it appears that value_size also affects the performance difference caused by the Bloom filter. This is likely because, although the overall size of the Bloom filter remains the same, each Bloom filter is divided into smaller parts, requiring additional overhead for reading and writing.","title":"Result"},{"location":"benchmarks/bloomfilter/#hypothesis-3","text":"When the number of bits per key in a Bloom filter is fixed, the mathematically proven optimal number of hashes is ln2 * bloom_bits \u2248 0.69 * bloom_bits. However, since the number of hashes cannot be a fraction, if we use the default value of 10 bits in LevelDB, the number of hashes becomes 6 instead of 6.9. Therefore, using 7 hash functions, which is closer to the theoretically optimal value, should improve performance.","title":"Hypothesis 3"},{"location":"benchmarks/bloomfilter/#result_1","text":"The results were almost identical, but using 7 hash functions slightly decreased write performance and slightly improved read performance, though the difference was minimal. Mathematically, the difference in the probability of false positives between using 6 and 7 hash functions is 0.00024, which is negligible and likely equivalent to the additional overhead from processing one more hash during reading.","title":"Result"},{"location":"benchmarks/bloomfilter/#hypothesis-4","text":"If we use 9 bits (6.21 <-> 6) or 12 bits (8.28 <-> 8), which are closer to the theoretically optimal number of hash functions, performance should improve.","title":"Hypothesis 4"},{"location":"benchmarks/bloomfilter/#result_2","text":"Contrary to expectations, the best results were obtained when using 10 bits. Using 12 bits and 8 hashes significantly reduced the probability of false positives compared to the default 10 bits and 6 hashes, but considering the overhead required to process 2 additional bits and 2 hashes, the difference in read performance was not significant, and write performance decreased. When using 6 hashes and 9 bits, the write performance was similar to the default case, but read performance decreased. This is because, contrary to my expectations, the optimal number of hashes is the value that minimizes false positives when the bloom_bits value is fixed. Conversely, if the number of hashes is fixed, false positives decrease as the bloom_bits value increases.","title":"Result"},{"location":"benchmarks/bloomfilter/#hypothesis-5","text":"From the experimental results above, it was found that with the same number of hashes, a larger bloom_bits value results in better read performance. Considering that the bloom_bits value has a greater impact on false positives than the number of hashes, and that the overhead of the Bloom filter is largely due to the number of hashes, it is hypothesized that fixing the number of hashes and increasing the bloom_bits value will yield better performance results.","title":"Hypothesis 5"},{"location":"benchmarks/bloomfilter/#result_3","text":"Using 6 hash functions with 10 bits and 30 bits each resulted in a slight increase in read performance but a decrease in write performance. Despite adjusting various variables and conducting numerous measurements, it seems difficult to achieve better results than using the default 10 bits with 6 hash functions through simple numerical adjustments.","title":"Result"},{"location":"benchmarks/bloomfilter/#hypothesis-6","text":"False positives can only occur if the data to be read does not exist. Therefore, in the ReadMissing benchmark, where only non-existent keys are searched, the impact of false positives will be greater than in ReadRandom.","title":"Hypothesis 6"},{"location":"benchmarks/bloomfilter/#result_4","text":"Unlike FillRandom, which consistently decreases in performance, and ReadRandom, which increases and then decreases around 10 bits, ReadMissing shows a significant performance increase as soon as a Bloom filter is used, and performance continues to improve as the number of bits increases. This is likely because, as hypothesized, in ReadMissing, the performance improvement from reducing false positives outweighs the overhead from increasing the Bloom filter size.","title":"Result"},{"location":"benchmarks/compaction/","text":"Experimental Environment We conducted experiments to investigate how the size of the Key and Value affects Compaction. Various Key Size Various Value Size 16 byte 256 byte 32 byte 1 kb 64 byte 4 kb 128 byte 16 kb For benchmarks, we utilized only fillrandom and readrandom where Compaction occurs. Experiment Setup Benchmarks version 1.23 fillrandom CPU Intel Xeon(R) readrandom ~~fillseq~~ ~~readseq~~ 1. 'fillrandom' with Variable Key Size In fillrandom with variable key size, we expected the WAF latency to gradually increase. However, contrary to our expectations, the WAF decreased as the key size decreased. 2. 'fillrandom' with Variable Value Size For variable value size, we anticipated that increasing the size by 4 times would result in a 4-fold performance difference. However, the results showed: Compaction latency increased by 5.18 times. Latency increased by 5.33 times. This seems to be because as the value size increased, Compaction occurred more frequently. 3. 'readrandom' with Variable Key/Value Size In readrandom with variable value size, latency increased by more than 100 times at 4kb and 16kb. Re-experiment Due to Unexpected Results In the fillrandom benchmark experiment, we found the rate of change in the graph inappropriate, prompting a re-experiment. We calculated the average of values obtained by repeating the same experiment three times and compared it with the previous experiment. Re-experiment-1 'fillrandom' with Variable Key Size Measured by the average of values repeated three times. As a result, the WAF continued to decrease as in the previous experiment. Further research was conducted to understand why the WAF continued to decrease contrary to expectations. Re-experiment-2 'fillrandom' with Variable Value Size Measured by the average of values repeated three times. To ensure more accurate measurements, we subdivided the intervals further. As a result, we obtained more precise results with latency decreasing from 5.33 times to 2.5 times. Similarly, we calculated the average of values repeated three times and subdivided the intervals further for comparison. As a result, we obtained more precise results with compaction latency decreasing from 5.18 times to 3.5 times. Conclusion and Discussion We conducted experiments based on the hypothesis that key/value size affects compaction. In the case of fillrandom , as the value size increased, the number of values to be compacted increased, resulting in decreased throughput and increased WAF, latency, and compaction latency. In the case of readrandom , as the value to be read at once increased, latency also increased. The results from fillrandom with increasing key size were peculiar. As the key size increased, the number of keys within that size remained the same, but the key range increased (see image below). Therefore, when compaction is triggered, the size to be loaded into memory, merge sorted, and written back to disk increased, leading to increased latency and compaction latency. However, since the number of keys within the range remained the same, the overlapping range was the same, allowing for more write processing over a larger range at once, resulting in decreased WAF and increased throughput.","title":"Compaction"},{"location":"benchmarks/compaction/#experimental-environment","text":"We conducted experiments to investigate how the size of the Key and Value affects Compaction. Various Key Size Various Value Size 16 byte 256 byte 32 byte 1 kb 64 byte 4 kb 128 byte 16 kb For benchmarks, we utilized only fillrandom and readrandom where Compaction occurs. Experiment Setup Benchmarks version 1.23 fillrandom CPU Intel Xeon(R) readrandom ~~fillseq~~ ~~readseq~~","title":"Experimental Environment"},{"location":"benchmarks/compaction/#1-fillrandom-with-variable-key-size","text":"In fillrandom with variable key size, we expected the WAF latency to gradually increase. However, contrary to our expectations, the WAF decreased as the key size decreased.","title":"1. 'fillrandom' with Variable Key Size"},{"location":"benchmarks/compaction/#2-fillrandom-with-variable-value-size","text":"For variable value size, we anticipated that increasing the size by 4 times would result in a 4-fold performance difference. However, the results showed: Compaction latency increased by 5.18 times. Latency increased by 5.33 times. This seems to be because as the value size increased, Compaction occurred more frequently.","title":"2. 'fillrandom' with Variable Value Size"},{"location":"benchmarks/compaction/#3-readrandom-with-variable-keyvalue-size","text":"In readrandom with variable value size, latency increased by more than 100 times at 4kb and 16kb.","title":"3. 'readrandom' with Variable Key/Value Size"},{"location":"benchmarks/compaction/#re-experiment-due-to-unexpected-results","text":"In the fillrandom benchmark experiment, we found the rate of change in the graph inappropriate, prompting a re-experiment. We calculated the average of values obtained by repeating the same experiment three times and compared it with the previous experiment.","title":"Re-experiment Due to Unexpected Results"},{"location":"benchmarks/compaction/#re-experiment-1-fillrandom-with-variable-key-size","text":"Measured by the average of values repeated three times. As a result, the WAF continued to decrease as in the previous experiment. Further research was conducted to understand why the WAF continued to decrease contrary to expectations.","title":"Re-experiment-1 'fillrandom' with Variable Key Size"},{"location":"benchmarks/compaction/#re-experiment-2-fillrandom-with-variable-value-size","text":"Measured by the average of values repeated three times. To ensure more accurate measurements, we subdivided the intervals further. As a result, we obtained more precise results with latency decreasing from 5.33 times to 2.5 times. Similarly, we calculated the average of values repeated three times and subdivided the intervals further for comparison. As a result, we obtained more precise results with compaction latency decreasing from 5.18 times to 3.5 times.","title":"Re-experiment-2 'fillrandom' with Variable Value Size"},{"location":"benchmarks/compaction/#conclusion-and-discussion","text":"We conducted experiments based on the hypothesis that key/value size affects compaction. In the case of fillrandom , as the value size increased, the number of values to be compacted increased, resulting in decreased throughput and increased WAF, latency, and compaction latency. In the case of readrandom , as the value to be read at once increased, latency also increased. The results from fillrandom with increasing key size were peculiar. As the key size increased, the number of keys within that size remained the same, but the key range increased (see image below). Therefore, when compaction is triggered, the size to be loaded into memory, merge sorted, and written back to disk increased, leading to increased latency and compaction latency. However, since the number of keys within the range remained the same, the overlapping range was the same, allowing for more write processing over a larger range at once, resulting in decreased WAF and increased throughput.","title":"Conclusion and Discussion"},{"location":"benchmarks/sstable/","text":"SSTable An SSTable consists of Data Blocks containing key-value pairs and a Filter Block that holds bloom filters for each Data Block. According to the LevelDB Handbook , it states the following regarding the Filter Block: If the user does not specify LevelDB to use filters, LevelDB does not store content in this block. This means that if you do not specify to use bloom filters, data is not stored in the Filter Block. This led to the thought, \"If we don't use bloom filters, wouldn't the performance improve during writes since we don't write to the Filter Block?\" Thus, I decided to conduct an experiment on this. Before conducting the experiment, I used uftrace to confirm that the leveldb::FilterBlockBuilder::Addkey function is only called when bloom filters are applied, verifying that data is not written to the Filter Block if bloom filters are not used. Hypothesis If bloom filters are not applied, data is not stored in the Filter Block, which will improve performance during write operations. Latency will decrease, and Throughput will increase. Design Controlled Variables --value_size : 2,000 --use_existing_db : 0 --compression_ratio : 1 --benchmarks : \"fillseq\" Independent Variables --bloom_bits : Not specified when bloom filters are not applied, specified as 64 when applied Dependent Variables Throughput Latency Experiment Environment CPU: 40*Intel\u00ae Xeon\u00ae Silver 4210R CPU @2.40GHz CPUCache: 14080KB Result Measured 10 times each with and without applying bloom filters Latency : micros/op Throughput : MB/s When bloom filters are applied 1 2 3 4 5 6 7 8 9 10 Latency 11.194 11.314 11.178 11.176 11.023 11.374 11.204 11.047 11.117 11.206 Throughput 171.8 169.9 172.0 172.0 174.4 169.0 171.6 174.0 172.9 171.6 > Average Latency = 11.183 micros/op > Average Throughput = 172.92 MB/s When bloom filters are not applied 1 2 3 4 5 6 7 8 9 10 Latency 10.543 11.258 10.280 10.674 10.976 10.537 10.724 10.353 10.619 10.785 Throughput 182.4 170.8 187.0 180.1 175.2 182.5 179.3 185.7 181.1 178.3 > Average Latency = 10.674 micros/op > Average Throughput = 180.24 MB/s Discussion Based on the results above, comparing the average Latency and Throughput when bloom filters are applied versus when they are not, we find the following: Latency Throughput Before the experiment, I hypothesized that not using bloom filters would result in lower Latency and higher Throughput during writes. Indeed, the results show that Latency is lower and Throughput is higher when bloom filters are not applied compared to when they are. However, I expected a more significant difference between applying and not applying bloom filters during write operations, but the actual difference in Latency was only about 0.5 micros per key-value pair, which did not seem substantial. I pondered why the difference was not more pronounced. db_bench Output The output from running db_bench does not strictly represent the time taken to create a single SSTable but rather the time taken to process a single key-value pair. Therefore, the output of db_bench should be viewed as an indicator of the overall write process rather than the time taken to create an SSTable. Since an SSTable is not created for every key-value pair, the time difference in writing an SSTable may not significantly impact the overall write process. Thus, while there may have been a noticeable time difference in processing a single SSTable, it might not have been significant in the overall write process. fillseq The reason for using fillseq instead of fillrandom in this experiment was to ensure the same order of key insertion when applying and not applying bloom filters. fillseq has the characteristic of not triggering compaction , meaning that performance differences only appear for SSTables created by flush . In an environment where both flush and compaction occur, more SSTables are created, potentially leading to more significant performance differences. However, since this experiment was conducted in a flush -only scenario, the number of SSTables created was limited, resulting in smaller performance differences.","title":"SSTable"},{"location":"benchmarks/sstable/#sstable","text":"An SSTable consists of Data Blocks containing key-value pairs and a Filter Block that holds bloom filters for each Data Block. According to the LevelDB Handbook , it states the following regarding the Filter Block: If the user does not specify LevelDB to use filters, LevelDB does not store content in this block. This means that if you do not specify to use bloom filters, data is not stored in the Filter Block. This led to the thought, \"If we don't use bloom filters, wouldn't the performance improve during writes since we don't write to the Filter Block?\" Thus, I decided to conduct an experiment on this. Before conducting the experiment, I used uftrace to confirm that the leveldb::FilterBlockBuilder::Addkey function is only called when bloom filters are applied, verifying that data is not written to the Filter Block if bloom filters are not used.","title":"SSTable"},{"location":"benchmarks/sstable/#hypothesis","text":"If bloom filters are not applied, data is not stored in the Filter Block, which will improve performance during write operations. Latency will decrease, and Throughput will increase.","title":"Hypothesis"},{"location":"benchmarks/sstable/#design","text":"Controlled Variables --value_size : 2,000 --use_existing_db : 0 --compression_ratio : 1 --benchmarks : \"fillseq\" Independent Variables --bloom_bits : Not specified when bloom filters are not applied, specified as 64 when applied Dependent Variables Throughput Latency","title":"Design"},{"location":"benchmarks/sstable/#experiment-environment","text":"CPU: 40*Intel\u00ae Xeon\u00ae Silver 4210R CPU @2.40GHz CPUCache: 14080KB","title":"Experiment Environment"},{"location":"benchmarks/sstable/#result","text":"Measured 10 times each with and without applying bloom filters Latency : micros/op Throughput : MB/s When bloom filters are applied 1 2 3 4 5 6 7 8 9 10 Latency 11.194 11.314 11.178 11.176 11.023 11.374 11.204 11.047 11.117 11.206 Throughput 171.8 169.9 172.0 172.0 174.4 169.0 171.6 174.0 172.9 171.6 > Average Latency = 11.183 micros/op > Average Throughput = 172.92 MB/s When bloom filters are not applied 1 2 3 4 5 6 7 8 9 10 Latency 10.543 11.258 10.280 10.674 10.976 10.537 10.724 10.353 10.619 10.785 Throughput 182.4 170.8 187.0 180.1 175.2 182.5 179.3 185.7 181.1 178.3 > Average Latency = 10.674 micros/op > Average Throughput = 180.24 MB/s","title":"Result"},{"location":"benchmarks/sstable/#discussion","text":"Based on the results above, comparing the average Latency and Throughput when bloom filters are applied versus when they are not, we find the following: Latency Throughput Before the experiment, I hypothesized that not using bloom filters would result in lower Latency and higher Throughput during writes. Indeed, the results show that Latency is lower and Throughput is higher when bloom filters are not applied compared to when they are. However, I expected a more significant difference between applying and not applying bloom filters during write operations, but the actual difference in Latency was only about 0.5 micros per key-value pair, which did not seem substantial. I pondered why the difference was not more pronounced. db_bench Output The output from running db_bench does not strictly represent the time taken to create a single SSTable but rather the time taken to process a single key-value pair. Therefore, the output of db_bench should be viewed as an indicator of the overall write process rather than the time taken to create an SSTable. Since an SSTable is not created for every key-value pair, the time difference in writing an SSTable may not significantly impact the overall write process. Thus, while there may have been a noticeable time difference in processing a single SSTable, it might not have been significant in the overall write process. fillseq The reason for using fillseq instead of fillrandom in this experiment was to ensure the same order of key insertion when applying and not applying bloom filters. fillseq has the characteristic of not triggering compaction , meaning that performance differences only appear for SSTables created by flush . In an environment where both flush and compaction occur, more SSTables are created, potentially leading to more significant performance differences. However, since this experiment was conducted in a flush -only scenario, the number of SSTables created was limited, resulting in smaller performance differences.","title":"Discussion"},{"location":"benchmarks/wal_1/","text":"WAL Option - Disable_WAL In the case of LevelDB, there is no option to decide whether to use WAL when running db_bench . On the other hand, RocksDB, which is based on LevelDB, provides a disable-wal option that allows you to choose whether to use WAL. Please note that the performance (Throughput, Latency) comparisons used in this post are measured using RocksDB, not LevelDB. Hypothesis Enabling WAL will negatively impact latency and throughput. Design Execute the following benchmarks using RocksDB's db_bench : fillseq fillrandom readrandom Experiment Environment CPU Model: Intel(R) Core(TM) i9-7940X CPU @ 3.10GHz Spec: Caches (sum of all): L1d: 448 KiB (14 instances) L1i: 448 KiB (14 instances) L2: 14 MiB (14 instances) L3: 19.3 MiB (1 instance) OS & HW OS: Ubuntu 22.04 LTS (Not VM) Storage: Samsung SSD 860 2TB Result The disable-wal option used in RocksDB determines whether WAL is enabled. Through this, the performance of Throughput, SAF, WAF, and Latency (Average) was measured. The following data is the average of all results after running RocksDB's db_bench 10 times. The above graph is represented in the table below. WAL Throughput (MB/s) SAF WAF Latency (Average) Benchmark Type Disabled 180.22 2.667 2 0.61416 fillseq Disabled 77.33 1.625 1.7 1.43117 fillrandom Enabled 51.73 2.06667 2 2.13972 fillseq Enabled 36.17 1.625 1.7 3.06011 fillrandom Discussion In the case of RocksDB, when writing WAL, it calls rocksdb::DBImpl::WriteToWAL . When this member function is called, the following procedure occurs. The above diagram is not a complete UML class diagram but a diagram to show the overall flow. Through this diagram, we can see that when rocksdb::DBImpl::WriteToWAL is called, it uses std::write or pwrite . Therefore, allowing WAL requires more IO operations, and since these IO operations are heavily used, it significantly impacts the performance of Latency and Throughput. The following graph measures the time taken to write WAL. As shown in the graph, WAL is a task with considerable overhead due to the use of IO. Summary Advantages of WAL Increases the likelihood of not losing data in case of abnormal termination. Disadvantages of WAL It has significant overhead due to the use of IO, negatively affecting performance in terms of Latency and Throughput.","title":"Disable_wal"},{"location":"benchmarks/wal_1/#wal","text":"","title":"WAL"},{"location":"benchmarks/wal_1/#option-disable_wal","text":"In the case of LevelDB, there is no option to decide whether to use WAL when running db_bench . On the other hand, RocksDB, which is based on LevelDB, provides a disable-wal option that allows you to choose whether to use WAL. Please note that the performance (Throughput, Latency) comparisons used in this post are measured using RocksDB, not LevelDB.","title":"Option - Disable_WAL"},{"location":"benchmarks/wal_1/#hypothesis","text":"Enabling WAL will negatively impact latency and throughput.","title":"Hypothesis"},{"location":"benchmarks/wal_1/#design","text":"Execute the following benchmarks using RocksDB's db_bench : fillseq fillrandom readrandom","title":"Design"},{"location":"benchmarks/wal_1/#experiment-environment","text":"","title":"Experiment Environment"},{"location":"benchmarks/wal_1/#cpu","text":"Model: Intel(R) Core(TM) i9-7940X CPU @ 3.10GHz Spec: Caches (sum of all): L1d: 448 KiB (14 instances) L1i: 448 KiB (14 instances) L2: 14 MiB (14 instances) L3: 19.3 MiB (1 instance)","title":"CPU"},{"location":"benchmarks/wal_1/#os-hw","text":"OS: Ubuntu 22.04 LTS (Not VM) Storage: Samsung SSD 860 2TB","title":"OS &amp; HW"},{"location":"benchmarks/wal_1/#result","text":"The disable-wal option used in RocksDB determines whether WAL is enabled. Through this, the performance of Throughput, SAF, WAF, and Latency (Average) was measured. The following data is the average of all results after running RocksDB's db_bench 10 times. The above graph is represented in the table below. WAL Throughput (MB/s) SAF WAF Latency (Average) Benchmark Type Disabled 180.22 2.667 2 0.61416 fillseq Disabled 77.33 1.625 1.7 1.43117 fillrandom Enabled 51.73 2.06667 2 2.13972 fillseq Enabled 36.17 1.625 1.7 3.06011 fillrandom","title":"Result"},{"location":"benchmarks/wal_1/#discussion","text":"In the case of RocksDB, when writing WAL, it calls rocksdb::DBImpl::WriteToWAL . When this member function is called, the following procedure occurs. The above diagram is not a complete UML class diagram but a diagram to show the overall flow. Through this diagram, we can see that when rocksdb::DBImpl::WriteToWAL is called, it uses std::write or pwrite . Therefore, allowing WAL requires more IO operations, and since these IO operations are heavily used, it significantly impacts the performance of Latency and Throughput. The following graph measures the time taken to write WAL. As shown in the graph, WAL is a task with considerable overhead due to the use of IO.","title":"Discussion"},{"location":"benchmarks/wal_1/#summary","text":"Advantages of WAL Increases the likelihood of not losing data in case of abnormal termination. Disadvantages of WAL It has significant overhead due to the use of IO, negatively affecting performance in terms of Latency and Throughput.","title":"Summary"},{"location":"benchmarks/wal_2/","text":"Write-Ahead Logging (WAL) Analysis Overview of max_total_wal_size Option Hypothesis Each column family has its own ssTable, but they share a single WAL. A new WAL is created whenever a column family is flushed, redirecting all writes to the new WAL. WALs can only be deleted once all their data is moved to ssTables, necessitating regular flushes. Without constraints on WAL size, deletion slows, and flushes become infrequent. The max_total_wal_size option triggers the deletion of the oldest live WAL file when the size exceeds a specified value, forcing a flush if live data exists. A smaller WAL size might lead to frequent flushes, potentially degrading performance. Experimental Design Independent Variable : --max_total_wal_size=[int value] Dependent Variables : SAF, WAF, Latency, Throughput Command ./db_bench --benchmarks=\"fillseq\" --max_total_wal_size=[0,1,10,100,1000,10000,100000,1000000,10000000,100000000] --num=10000000 Experiment Environment Operating System : macOS Monterey Processor : 2.3GHz 8-Core Intel Core i9 SSD : APPLE SSD AP1024N 1TB Results Initial experiments showed no significant results in the given environment. The default value of max_total_wal_size is not zero but calculated as: plaintext [sum of all write_buffer_size * max_write_buffer_number] * 4 This option is effective only with two or more column families. Calculations for Column Families 10 Column Families : write_buffer_size = 64 MB (Default) max_write_buffer_number = 2 (Default) max_total_wal_size = [10*64MB*2]*4 = 5.12GB 15 Column Families : write_buffer_size = 64 MB (Default) max_write_buffer_number = 2 (Default) max_total_wal_size = [15*64MB*2]*4 = 7.68GB Experiment Results The results seem to reflect only the default column family analysis. Different results were observed when the --num_column_families= option was not specified, indicating usage of all 10 and 15 families. Discussion Performance appears to degrade as max_total_wal_size decreases from its default value. Testing with a fixed max_total_wal_size of 500MB and varying column family counts (10, 15, 20, 25, 30) suggests performance degradation with more column families sharing the WAL. The threshold for performance degradation in max_total_wal_size increases with more column families. Future Work Further benchmarking with multiple column families, altering their characteristics, and monitoring flush counts could provide deeper insights.","title":"Max_total_wal_size"},{"location":"benchmarks/wal_2/#write-ahead-logging-wal-analysis","text":"","title":"Write-Ahead Logging (WAL) Analysis"},{"location":"benchmarks/wal_2/#overview-of-max_total_wal_size-option","text":"","title":"Overview of max_total_wal_size Option"},{"location":"benchmarks/wal_2/#hypothesis","text":"Each column family has its own ssTable, but they share a single WAL. A new WAL is created whenever a column family is flushed, redirecting all writes to the new WAL. WALs can only be deleted once all their data is moved to ssTables, necessitating regular flushes. Without constraints on WAL size, deletion slows, and flushes become infrequent. The max_total_wal_size option triggers the deletion of the oldest live WAL file when the size exceeds a specified value, forcing a flush if live data exists. A smaller WAL size might lead to frequent flushes, potentially degrading performance.","title":"Hypothesis"},{"location":"benchmarks/wal_2/#experimental-design","text":"Independent Variable : --max_total_wal_size=[int value] Dependent Variables : SAF, WAF, Latency, Throughput","title":"Experimental Design"},{"location":"benchmarks/wal_2/#command","text":"./db_bench --benchmarks=\"fillseq\" --max_total_wal_size=[0,1,10,100,1000,10000,100000,1000000,10000000,100000000] --num=10000000","title":"Command"},{"location":"benchmarks/wal_2/#experiment-environment","text":"Operating System : macOS Monterey Processor : 2.3GHz 8-Core Intel Core i9 SSD : APPLE SSD AP1024N 1TB","title":"Experiment Environment"},{"location":"benchmarks/wal_2/#results","text":"Initial experiments showed no significant results in the given environment. The default value of max_total_wal_size is not zero but calculated as: plaintext [sum of all write_buffer_size * max_write_buffer_number] * 4 This option is effective only with two or more column families.","title":"Results"},{"location":"benchmarks/wal_2/#calculations-for-column-families","text":"10 Column Families : write_buffer_size = 64 MB (Default) max_write_buffer_number = 2 (Default) max_total_wal_size = [10*64MB*2]*4 = 5.12GB 15 Column Families : write_buffer_size = 64 MB (Default) max_write_buffer_number = 2 (Default) max_total_wal_size = [15*64MB*2]*4 = 7.68GB","title":"Calculations for Column Families"},{"location":"benchmarks/wal_2/#experiment-results","text":"The results seem to reflect only the default column family analysis. Different results were observed when the --num_column_families= option was not specified, indicating usage of all 10 and 15 families.","title":"Experiment Results"},{"location":"benchmarks/wal_2/#discussion","text":"Performance appears to degrade as max_total_wal_size decreases from its default value. Testing with a fixed max_total_wal_size of 500MB and varying column family counts (10, 15, 20, 25, 30) suggests performance degradation with more column families sharing the WAL. The threshold for performance degradation in max_total_wal_size increases with more column families.","title":"Discussion"},{"location":"benchmarks/wal_2/#future-work","text":"Further benchmarking with multiple column families, altering their characteristics, and monitoring flush counts could provide deeper insights.","title":"Future Work"},{"location":"benchmarks/wal_3/","text":"WAL Option - Manual_wal_flush When DB::put is called, data is written to the memtable and, if WAL is enabled, also to the WAL. Initially, it is recorded in the application memory buffer, which then calls the fwrite syscall to flush it to the OS buffer. (The OS buffer is later synchronized to permanent storage.) In the event of a crash, RocksDB can only recover data from the memtable. By default, RocksDB automatically flushes the WAL from memory to the OS buffer every time a put is called. However, by disabling the manual_wal_flush option, you can set it to flush only when DB::FlushWAL is called, or when the WAL buffer is full or the FlushWAL API is invoked. Not calling fwrite syscall every time a put is called generally involves a tradeoff between reliability and latency. If data preservation is crucial, set the manual_wal_flush option to True; if performance is prioritized over potential data loss, set it to False. Hypothesis The fewer the flushes, the better the performance (latency). To determine the extent of this difference and consider the tradeoff, we conducted experiments. We measured throughput and latency by setting the manual_wal_flush option to true and false, respectively. Benchmarks were conducted with both fillseq and fillrandom, which are unrelated to WAL, so the results should be consistent with existing fillseq and fillrandom experiments. Additionally, comparisons were made in both sync/async modes. Design Independent Variable : manual_wal_flush (true / false) Dependent Variable : Latency, Throughput Controlled Variable : Benchmark (fillseq, fillrandom), Sync mode (Sync / Async) Experiment Environment Result The results below are the average values from conducting each experiment five times. Async Latency and Throughput in Asynchronous Mode Latency Latency in Synchronous and Asynchronous Modes Discussion Latency and Throughput with manual_wal_flush Option Performance is worse when true compared to false. Benchmark (fillseq, fillrandom) As expected, fillseq performs better than fillrandom, and the difference in manual_wal_flush is around 1000, showing no impact on the benchmark. Performance in Sync and Async Generally, performance should be lower when manual_wal_flush is true compared to false. However, in synchronous mode, latency is observed to be an overwhelmingly high 3000 micros/op, and performance can be better when true than false. This indicates that the manual_wal_flush option does not affect sync mode.","title":"Manual_wal_flush"},{"location":"benchmarks/wal_3/#wal","text":"","title":"WAL"},{"location":"benchmarks/wal_3/#option-manual_wal_flush","text":"When DB::put is called, data is written to the memtable and, if WAL is enabled, also to the WAL. Initially, it is recorded in the application memory buffer, which then calls the fwrite syscall to flush it to the OS buffer. (The OS buffer is later synchronized to permanent storage.) In the event of a crash, RocksDB can only recover data from the memtable. By default, RocksDB automatically flushes the WAL from memory to the OS buffer every time a put is called. However, by disabling the manual_wal_flush option, you can set it to flush only when DB::FlushWAL is called, or when the WAL buffer is full or the FlushWAL API is invoked. Not calling fwrite syscall every time a put is called generally involves a tradeoff between reliability and latency. If data preservation is crucial, set the manual_wal_flush option to True; if performance is prioritized over potential data loss, set it to False.","title":"Option - Manual_wal_flush"},{"location":"benchmarks/wal_3/#hypothesis","text":"The fewer the flushes, the better the performance (latency). To determine the extent of this difference and consider the tradeoff, we conducted experiments. We measured throughput and latency by setting the manual_wal_flush option to true and false, respectively. Benchmarks were conducted with both fillseq and fillrandom, which are unrelated to WAL, so the results should be consistent with existing fillseq and fillrandom experiments. Additionally, comparisons were made in both sync/async modes.","title":"Hypothesis"},{"location":"benchmarks/wal_3/#design","text":"Independent Variable : manual_wal_flush (true / false) Dependent Variable : Latency, Throughput Controlled Variable : Benchmark (fillseq, fillrandom), Sync mode (Sync / Async)","title":"Design"},{"location":"benchmarks/wal_3/#experiment-environment","text":"","title":"Experiment Environment"},{"location":"benchmarks/wal_3/#result","text":"The results below are the average values from conducting each experiment five times. Async Latency and Throughput in Asynchronous Mode Latency Latency in Synchronous and Asynchronous Modes","title":"Result"},{"location":"benchmarks/wal_3/#discussion","text":"Latency and Throughput with manual_wal_flush Option Performance is worse when true compared to false. Benchmark (fillseq, fillrandom) As expected, fillseq performs better than fillrandom, and the difference in manual_wal_flush is around 1000, showing no impact on the benchmark. Performance in Sync and Async Generally, performance should be lower when manual_wal_flush is true compared to false. However, in synchronous mode, latency is observed to be an overwhelmingly high 3000 micros/op, and performance can be better when true than false. This indicates that the manual_wal_flush option does not affect sync mode.","title":"Discussion"},{"location":"practice/answer/","text":"Answers submitted by students homework_answered_by_students(kor).xlsx Homework Solutions Question 1. (PPT - Jongki Park) Why do LSM-tree and LevelDB use leveled structure? # of Level Write performance Read Performance WAF RAF Single Bad Good High Low Multi Good Bad Low High Question 2. (PPT - Jongki Park) In leveldb, max size of level i is 10^iMB. But max size of level 0 is 8MB. Why? We treat level-0 specially by bounding the number of files instead of number of bytes for two reasons: (1) With larger write-buffer sizes, it is nice not to do too many level-0 compactions. (2) The files in level-0 are merged on every read and therefore we wish to avoid too many files when the individual file size is small Practice 1. (PPT - Suhwan Shin) [A] $ ./db_bench --benchmarks=\"fillseq\" [B] $ ./db_bench --benchmarks=\"fillrandom\" Q1. Compare throughput, latency, and stats of two benchmarks and explain why. Q2. Calculate SAF (Space Amplification Factor) for each benchmark. Benchmark duplicate key range Major Compaction Throughput Latency SAF Fillseq No No High Low 1 (0.98) Fillrandom Yes Yes Low High 1.275 * Q3. In benchmark A, SSTs are not written in L0. Why? - Trivial Move Practice 2. (PPT - Zhu Yongjie) [Load] $ ./db_bench --benchmarks=\"fillrandom\" --use_existing_db=0 [A] $ ./db_bench --benchmarks=\"readseq\" --use_existing_db=1 [B] $ ./db_bench --benchmarks=\"readrandom\" --use_existing_db=1 [C] $ ./db_bench --benchmarks=\"seekrandom\" --use_existing_db=1 Q1. Which user key-value interface does each benchmark use? (Put, Get, Iterator, ...) Q2. Compare throughput and latency of each benchmark and explain why. Benchmark Interface Throughput (MB/s) Latency (micros/op) I/O Access Level readseq Get() high low sequential all readrandom Iterator->Next() low high random access one by one until find the key seekrandom Iterator->Seek() lowest highest random all Practice 3. (PPT - Suhwan Shin) [A] $ ./db_bench --benchmarks=\"fillrandom\" --value_size=100 --num=1000000 --compression_ratio=1 [B] $ ./db_bench --benchmarks=\"fillrandom\" --value_size=1000 --num=114173 --compression_ratio=1 Note 1. key_size = 16B Note 2. same total kv pairs size. Note 3. # of B's entries = 114173 = (16+100)/(16+1000) * 1000000 Q. The size of input kv pairs is the same. But One is better in throughput, the other is better in latency. Explain why. Benchmark DB size # of entries Size of entry Throughput (MB/s) Latency (s/op) A same 1,000,000 (many) 116B (small) low low B same 114,173 (few) 1016B (big) high high","title":"Answers"},{"location":"practice/answer/#answers-submitted-by-students","text":"","title":"Answers submitted by students"},{"location":"practice/answer/#homework_answered_by_studentskorxlsx","text":"","title":"homework_answered_by_students(kor).xlsx"},{"location":"practice/answer/#homework-solutions","text":"","title":"Homework Solutions"},{"location":"practice/answer/#question-1-ppt-jongki-park","text":"","title":"Question 1. (PPT - Jongki Park)"},{"location":"practice/answer/#why-do-lsm-tree-and-leveldb-use-leveled-structure","text":"# of Level Write performance Read Performance WAF RAF Single Bad Good High Low Multi Good Bad Low High","title":"Why do LSM-tree and LevelDB use leveled structure?"},{"location":"practice/answer/#question-2-ppt-jongki-park","text":"","title":"Question 2. (PPT - Jongki Park)"},{"location":"practice/answer/#in-leveldb-max-size-of-level-i-is-10imb-but-max-size-of-level-0-is-8mb-why","text":"We treat level-0 specially by bounding the number of files instead of number of bytes for two reasons: (1) With larger write-buffer sizes, it is nice not to do too many level-0 compactions. (2) The files in level-0 are merged on every read and therefore we wish to avoid too many files when the individual file size is small","title":"In leveldb, max size of level i is 10^iMB. But max size of level 0 is 8MB. Why?"},{"location":"practice/answer/#practice-1-ppt-suhwan-shin","text":"[A] $ ./db_bench --benchmarks=\"fillseq\" [B] $ ./db_bench --benchmarks=\"fillrandom\" Q1. Compare throughput, latency, and stats of two benchmarks and explain why. Q2. Calculate SAF (Space Amplification Factor) for each benchmark. Benchmark duplicate key range Major Compaction Throughput Latency SAF Fillseq No No High Low 1 (0.98) Fillrandom Yes Yes Low High 1.275 * Q3. In benchmark A, SSTs are not written in L0. Why? - Trivial Move","title":"Practice 1. (PPT - Suhwan Shin)"},{"location":"practice/answer/#practice-2-ppt-zhu-yongjie","text":"[Load] $ ./db_bench --benchmarks=\"fillrandom\" --use_existing_db=0 [A] $ ./db_bench --benchmarks=\"readseq\" --use_existing_db=1 [B] $ ./db_bench --benchmarks=\"readrandom\" --use_existing_db=1 [C] $ ./db_bench --benchmarks=\"seekrandom\" --use_existing_db=1 Q1. Which user key-value interface does each benchmark use? (Put, Get, Iterator, ...) Q2. Compare throughput and latency of each benchmark and explain why. Benchmark Interface Throughput (MB/s) Latency (micros/op) I/O Access Level readseq Get() high low sequential all readrandom Iterator->Next() low high random access one by one until find the key seekrandom Iterator->Seek() lowest highest random all","title":"Practice 2. (PPT - Zhu Yongjie)"},{"location":"practice/answer/#practice-3-ppt-suhwan-shin","text":"[A] $ ./db_bench --benchmarks=\"fillrandom\" --value_size=100 --num=1000000 --compression_ratio=1 [B] $ ./db_bench --benchmarks=\"fillrandom\" --value_size=1000 --num=114173 --compression_ratio=1 Note 1. key_size = 16B Note 2. same total kv pairs size. Note 3. # of B's entries = 114173 = (16+100)/(16+1000) * 1000000 Q. The size of input kv pairs is the same. But One is better in throughput, the other is better in latency. Explain why. Benchmark DB size # of entries Size of entry Throughput (MB/s) Latency (s/op) A same 1,000,000 (many) 116B (small) low low B same 114,173 (few) 1016B (big) high high","title":"Practice 3. (PPT - Suhwan Shin)"},{"location":"practice/practice/","text":"Assignments Question 1. Why do LSM-tree and LevelDB use leveled structure? Hint 1 - Stackoverflow - Why does LevelDB needs more than two levels? - Why rocksDB needs multiple levels? - Why does LevelDB make its lower level 10 times bigger than upper one? Hint 2 \u2013 Memory hierarchy Hint 3 - Patrick O'Neil, The Log-Structured Merge-Tree (LSM-Tree), 1996 Question 2. In leveldb, max size of level i is 10^iMB. But max size of level 0 is 8MB. Why? Hint 1 - leveldb source code - leveldb/db/version_set.cc:VersionSet::Finalize - leveldb/db/dbformat.h:kL0_CompactionTrigger Hint 2 - leveldb-handbook, Compaction (Use google chrome translator) 3. Practice 1 [A] $ ./db_bench --benchmarks=\"fillseq\" [B] $ ./db_bench --benchmarks=\"fillrandom\" Q1. Compare throughput, latency, and stats of two benchmarks and explain why. Hint - Seek Time, Key Range, Compaction Q2. In benchmark A, SSTs are not written in L0. Why? Hint - Flush, Compaction Trigger Q3. Calculate SAF (Space Amplification Factor) for each benchmark. Hint - db_bench meta operation 4. Practice 2 [Load] $ ./db_bench --benchmarks=\"fillrandom\" --use_existing_db=0 [A] $ ./db_bench --benchmarks=\"readseq\" --use_existing_db=1 [B] $ ./db_bench --benchmarks=\"readrandom\" --use_existing_db=1 [C] $ ./db_bench --benchmarks=\"seekrandom\" --use_existing_db=1 Note - Before running A, B, and C, run db_load benchmark. Q1. Which user key-value interface does each benchmark use? (Put, Get, Iterator, ...) Hint 1 - leveldb/doc/index.md Hint 2 - leveldb/benchmarks/db_bench.cc Q2. Compare throughput and latency of each benchmark and explain why. Hint - Seek Time 5. Practice 3 [A] $ ./db_bench --benchmarks=\"fillrandom\" --value_size=100 --num=1000000 --compression_ratio=1 [B] $ ./db_bench --benchmarks=\"fillrandom\" --value_size=1000 --num=114173 --compression_ratio=1 Note 1. key_size = 16B Note 2. same total kv pairs size. Note 3. # of B's entries = 114173 = (16+100)/(16+1000) * 1000000 Q. The size of input kv pairs is the same. But One is better in throughput, the other is better in latency. Explain why. Hint. Batch Processing","title":"Assignments"},{"location":"practice/practice/#assignments","text":"","title":"Assignments"},{"location":"practice/practice/#question-1-why-do-lsm-tree-and-leveldb-use-leveled-structure","text":"Hint 1 - Stackoverflow - Why does LevelDB needs more than two levels? - Why rocksDB needs multiple levels? - Why does LevelDB make its lower level 10 times bigger than upper one? Hint 2 \u2013 Memory hierarchy Hint 3 - Patrick O'Neil, The Log-Structured Merge-Tree (LSM-Tree), 1996","title":"Question 1. Why do LSM-tree and LevelDB use leveled structure?"},{"location":"practice/practice/#question-2-in-leveldb-max-size-of-level-i-is-10imb-but-max-size-of-level-0-is-8mb-why","text":"Hint 1 - leveldb source code - leveldb/db/version_set.cc:VersionSet::Finalize - leveldb/db/dbformat.h:kL0_CompactionTrigger Hint 2 - leveldb-handbook, Compaction (Use google chrome translator)","title":"Question 2. In leveldb, max size of level i is 10^iMB. But max size of level 0 is 8MB. Why?"},{"location":"practice/practice/#3-practice-1","text":"[A] $ ./db_bench --benchmarks=\"fillseq\" [B] $ ./db_bench --benchmarks=\"fillrandom\" Q1. Compare throughput, latency, and stats of two benchmarks and explain why. Hint - Seek Time, Key Range, Compaction Q2. In benchmark A, SSTs are not written in L0. Why? Hint - Flush, Compaction Trigger Q3. Calculate SAF (Space Amplification Factor) for each benchmark. Hint - db_bench meta operation","title":"3. Practice 1"},{"location":"practice/practice/#4-practice-2","text":"[Load] $ ./db_bench --benchmarks=\"fillrandom\" --use_existing_db=0 [A] $ ./db_bench --benchmarks=\"readseq\" --use_existing_db=1 [B] $ ./db_bench --benchmarks=\"readrandom\" --use_existing_db=1 [C] $ ./db_bench --benchmarks=\"seekrandom\" --use_existing_db=1 Note - Before running A, B, and C, run db_load benchmark. Q1. Which user key-value interface does each benchmark use? (Put, Get, Iterator, ...) Hint 1 - leveldb/doc/index.md Hint 2 - leveldb/benchmarks/db_bench.cc Q2. Compare throughput and latency of each benchmark and explain why. Hint - Seek Time","title":"4. Practice 2"},{"location":"practice/practice/#5-practice-3","text":"[A] $ ./db_bench --benchmarks=\"fillrandom\" --value_size=100 --num=1000000 --compression_ratio=1 [B] $ ./db_bench --benchmarks=\"fillrandom\" --value_size=1000 --num=114173 --compression_ratio=1 Note 1. key_size = 16B Note 2. same total kv pairs size. Note 3. # of B's entries = 114173 = (16+100)/(16+1000) * 1000000 Q. The size of input kv pairs is the same. But One is better in throughput, the other is better in latency. Explain why. Hint. Batch Processing","title":"5. Practice 3"},{"location":"tuning/","text":"LevelDB Tuning Contest 1. Workload (Benchmark): YCSB-cpp The goal of the Yahoo Cloud Serving Benchmark (YCSB) project is to develop a framework and common set of workloads for evaluating the performance of different \"key-value\" and \"cloud\" serving stores. YCSB github YCSB-cpp github Cooper, Brian F., et al. \"Benchmarking cloud serving systems with YCSB.\" Proceedings of the 1st ACM symposium on Cloud computing. 2010. Contest Workload YCSB Workload Record Count = 2,000,000 Operation Count = 2,000,000 Workload file Workload A Workload B Workload D Load A -> Run A -> Run B -> Run D 2. Tuning Options Team write_buffer_size max_file_size compression block_cache filter_policy block_size block_restart_interval WAL/Manifest 128MB 64MB snappy 128MB 10 8KB 16 Memtable 32MB 16MB snappy 32MB 10 8KB 16 Compaction 32MB 4MB snappy 8MB 10 2KB 16 SSTable 64MB 32MB snappy 13.75MB 10 2MB 4 Bloom Filter 128MB 64MB snappy 64MB 9 8KB 4 Cache 47.68MB 4MB snappy 40MB 10 8KB 32 3. Result Avg. Throughput (ops/sec) WAL/Manifest Memtable Compaction SSTable Bloom Filter Cache Load A 71065 34800 23336 57734 71972 35522 Run A 73928 58610 40784 63753 72253 47830 Run B 131709 130014 114509 147205 136455 125638 Run D 191939 200841 177168 207546 194346 193820 - Tuning Benchmark shell script - Tuning Benchmark excel data - Tuning Benchmark raw data 4. Ranking & Report Total Rank Team Load A Run A Run B Run D Report 1 SSTable 3 3 1 1 Link (KOR) 1 Bloom Filter 1 2 2 3 Link (KOR) 3 WAL/Manifest 2 1 3 5 Link (KOR) 4 Memtable 5 4 4 2 Link (KOR) 5 Cache 4 5 5 4 Link (KOR) 6 Compaction 6 6 6 6 Link (ENG) 5. Notice Study the db options and their relationships. Analyze workloads such as key/value size and key/operations distribution. Hypothesize the best option set and verify it by experiment. Submit your best option set to the assistant by e-mail. Send your YCSB-cpp/leveldb/leveldb.properties to koreachoi96@gmail.com until Monday, 15 August 2022, 9:00 AM Write a report about your hypothesis, experiment, and final decision in markdown format. Pull request at tuning directory until Tuesday, 16 August 2022, 2:00 PM Title: [Tuning]team_ _report.md Check your ranking at Tuesday, 16 August 2022, 4:00 PM Please refer to LevelDB options.h and RocksDB tuning guide before you start! leveldb/include/leveldb/options.h leveldb/doc/index.md RocksDB Tuning Guide RocksDB Setup Options and Basice Tuning 6. LevelDB options and restrictions Modify YCSB-cpp/leveldb/leveldb.properties like below. # YCSB-cpp/leveldb/leveldb.properties # ---------------------------Restriction---------------------------- # max_memory size (= write_buffer_size + block_cache_size) <= 1024 * 1024 * 1024 (1GB) # max_file_size <= 1024 * 1024 * 1024 (1GB) leveldb.max_open_files=10000 # -------------LevelDB Options, Tune them!------------------ leveldb.dbname=/tmp/ycsb-leveldb leveldb.format=single leveldb.destroy=false leveldb.write_buffer_size=2097152 leveldb.max_file_size=4194304 leveldb.compression=snappy leveldb.cache_size=4194304 leveldb.filter_bits=10 leveldb.block_size=4096 leveldb.block_restart_interval=16 7. Install YCSB-cpp Install and build leveldb in release mode git clone https://github.com/ls4154/YCSB-cpp.git Modify config section in Makefile #---------------------build config------------------------- DEBUG_BUILD ?= 0 # put your leveldb directory EXTRA_CXXFLAGS ?= -I/example/leveldb/include EXTRA_LDFLAGS ?= -L/example/leveldb/build -lsnappy BIND_LEVELDB ?= 1 BIND_ROCKSDB ?= 0 BIND_LMDB ?= 0 make Modify YCSB-cpp/leveldb/leveldb.properties Run benchmarks # Command ./ycsb -load -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s ./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s ./ycsb -run -db leveldb -P workloads/workloadb -P leveldb/leveldb.properties -s ./ycsb -run -db leveldb -P workloads/workloadd -P leveldb/leveldb.properties -s You can make copies of leveldb.properties and use them for benchmarks. # Command ./ycsb -load -db leveldb -P workloads/workloada -P leveldb/leveldb.properties1 -s ./ycsb -load -db leveldb -P workloads/workloada -P leveldb/leveldb.properties2 -s 8. Tuning Enviornment DKU Linux Server System Specification CPU Memory Storage Linux Ubuntu","title":"Tuning Contest Overview"},{"location":"tuning/#leveldb-tuning-contest","text":"","title":"LevelDB Tuning Contest"},{"location":"tuning/#1-workload-benchmark-ycsb-cpp","text":"The goal of the Yahoo Cloud Serving Benchmark (YCSB) project is to develop a framework and common set of workloads for evaluating the performance of different \"key-value\" and \"cloud\" serving stores. YCSB github YCSB-cpp github Cooper, Brian F., et al. \"Benchmarking cloud serving systems with YCSB.\" Proceedings of the 1st ACM symposium on Cloud computing. 2010.","title":"1. Workload (Benchmark): YCSB-cpp"},{"location":"tuning/#contest-workload","text":"YCSB Workload Record Count = 2,000,000 Operation Count = 2,000,000 Workload file Workload A Workload B Workload D Load A -> Run A -> Run B -> Run D","title":"Contest Workload"},{"location":"tuning/#2-tuning-options","text":"Team write_buffer_size max_file_size compression block_cache filter_policy block_size block_restart_interval WAL/Manifest 128MB 64MB snappy 128MB 10 8KB 16 Memtable 32MB 16MB snappy 32MB 10 8KB 16 Compaction 32MB 4MB snappy 8MB 10 2KB 16 SSTable 64MB 32MB snappy 13.75MB 10 2MB 4 Bloom Filter 128MB 64MB snappy 64MB 9 8KB 4 Cache 47.68MB 4MB snappy 40MB 10 8KB 32","title":"2. Tuning Options"},{"location":"tuning/#3-result","text":"Avg. Throughput (ops/sec) WAL/Manifest Memtable Compaction SSTable Bloom Filter Cache Load A 71065 34800 23336 57734 71972 35522 Run A 73928 58610 40784 63753 72253 47830 Run B 131709 130014 114509 147205 136455 125638 Run D 191939 200841 177168 207546 194346 193820 - Tuning Benchmark shell script - Tuning Benchmark excel data - Tuning Benchmark raw data","title":"3. Result"},{"location":"tuning/#4-ranking-report","text":"Total Rank Team Load A Run A Run B Run D Report 1 SSTable 3 3 1 1 Link (KOR) 1 Bloom Filter 1 2 2 3 Link (KOR) 3 WAL/Manifest 2 1 3 5 Link (KOR) 4 Memtable 5 4 4 2 Link (KOR) 5 Cache 4 5 5 4 Link (KOR) 6 Compaction 6 6 6 6 Link (ENG)","title":"4. Ranking &amp; Report"},{"location":"tuning/#5-notice","text":"Study the db options and their relationships. Analyze workloads such as key/value size and key/operations distribution. Hypothesize the best option set and verify it by experiment. Submit your best option set to the assistant by e-mail. Send your YCSB-cpp/leveldb/leveldb.properties to koreachoi96@gmail.com until Monday, 15 August 2022, 9:00 AM Write a report about your hypothesis, experiment, and final decision in markdown format. Pull request at tuning directory until Tuesday, 16 August 2022, 2:00 PM Title: [Tuning]team_ _report.md Check your ranking at Tuesday, 16 August 2022, 4:00 PM Please refer to LevelDB options.h and RocksDB tuning guide before you start! leveldb/include/leveldb/options.h leveldb/doc/index.md RocksDB Tuning Guide RocksDB Setup Options and Basice Tuning","title":"5. Notice"},{"location":"tuning/#6-leveldb-options-and-restrictions","text":"Modify YCSB-cpp/leveldb/leveldb.properties like below. # YCSB-cpp/leveldb/leveldb.properties # ---------------------------Restriction---------------------------- # max_memory size (= write_buffer_size + block_cache_size) <= 1024 * 1024 * 1024 (1GB) # max_file_size <= 1024 * 1024 * 1024 (1GB) leveldb.max_open_files=10000 # -------------LevelDB Options, Tune them!------------------ leveldb.dbname=/tmp/ycsb-leveldb leveldb.format=single leveldb.destroy=false leveldb.write_buffer_size=2097152 leveldb.max_file_size=4194304 leveldb.compression=snappy leveldb.cache_size=4194304 leveldb.filter_bits=10 leveldb.block_size=4096 leveldb.block_restart_interval=16","title":"6. LevelDB options and restrictions"},{"location":"tuning/#7-install-ycsb-cpp","text":"Install and build leveldb in release mode git clone https://github.com/ls4154/YCSB-cpp.git Modify config section in Makefile #---------------------build config------------------------- DEBUG_BUILD ?= 0 # put your leveldb directory EXTRA_CXXFLAGS ?= -I/example/leveldb/include EXTRA_LDFLAGS ?= -L/example/leveldb/build -lsnappy BIND_LEVELDB ?= 1 BIND_ROCKSDB ?= 0 BIND_LMDB ?= 0 make Modify YCSB-cpp/leveldb/leveldb.properties Run benchmarks # Command ./ycsb -load -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s ./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s ./ycsb -run -db leveldb -P workloads/workloadb -P leveldb/leveldb.properties -s ./ycsb -run -db leveldb -P workloads/workloadd -P leveldb/leveldb.properties -s You can make copies of leveldb.properties and use them for benchmarks. # Command ./ycsb -load -db leveldb -P workloads/workloada -P leveldb/leveldb.properties1 -s ./ycsb -load -db leveldb -P workloads/workloada -P leveldb/leveldb.properties2 -s","title":"7. Install YCSB-cpp"},{"location":"tuning/#8-tuning-enviornment","text":"DKU Linux Server System Specification CPU Memory Storage Linux Ubuntu","title":"8. Tuning Enviornment"},{"location":"tuning/Tuning_team_Compaction_report/","text":"[Team Compaction] YCSB Tuning Report DB options for tunning options default Explanation leveldb.write_buffer_size 2MB single memtable size leveldb.max_file_size 4MB sstable size leveldb.compression snappy compression method leveldb.cache_size 4M cache size leveldb.filter_bits 10 number of filter block bits leveldb.block_size 4KB size of data block in one file leveldb.block_restart_interval 16 number of keys between restart points(delta encoding) Analyze workloads A: Read/update ratio: 50/50 B: Read/update ratio: 95/5 D: Read/update/insert ratio: 95/0/5 Because of the high read proportion, consideration is given to ways to maximize read performance. Of course, consider write performance and choose the best option. Hypothesis and experiment Considering that it is run at random, it is measured with an average of 3 times. Hypothesis Based on what was studied in the study, it is expected that writing performance will increase as the size of the buffer increases the amount of writing at once. In addition, it is expected that read performance will improve if the size of the file is 1/4 of the size of the buffer, allowing four files, which are the thresholds of level0, to be read at once, and increasing the cache size. Finally, by reducing the size of the block, increasing the number of bits in the index of the file, and increasing the number of bits in the filter block, the read performance is expected to be improved Default workload runtime(sec) throughput(ops/sec) load 6.22135 16073.7 A 2.88199 34698.2 B 0.753103 132784 D 0.545697 183252 write_buffer_size 8MB workload runtime(sec) throughput(ops/sec) -> runtime(sec) throughput(ops/sec) load 6.22135 16073.7 -> 4.58351 21817.4 A 2.88199 34698.2 -> 2.30625 43360.4 B 0.753103 132784 -> 0.738974 135323 D 0.545697 183252 -> 0.50175 199302 As the write performance improved, the performance of load and A improved. write_buffer_size 32MB, max_file_size 8MB workload runtime(sec) throughput(ops/sec) -> runtime(sec) throughput(ops/sec) load 6.22135 16073.7 -> 1.33948 74655.9 A 2.88199 34698.2 -> 1.44349 69276.5 B 0.753103 132784 -> 0.708532 141137 D 0.545697 183252 -> 0.502773 198897 The buffer size was further increased to 32MB(no performance changes after increasing more than 32MB), and the file size was 8MB, so that only Level 0 could be filled first, and it could be seen that the write performance was improved. cacahe_size 8MB, block_size 2KB workload runtime(sec) throughput(ops/sec) -> runtime(sec) throughput(ops/sec) load 6.22135 16073.7 -> 1.33054 75157.5 A 2.88199 34698.2 -> 1.38317 72297.5 B 0.753103 132784 -> 0.690946 144729 D 0.545697 183252 -> 0.484127 206557 For read performance, the cache size was increased and the block size was reduced. (More cache than 8MB or less block size makes little difference in performance) It was found that the performance of workload B and C, which had a high reading proportion, improved slightly. Here, increasing the filter block and changing block_restart_interval does not cause any worse performance or make any difference. Therefore, the best options the best options leveldb.write_buffer_size 32MB leveldb.max_file_size 8MB leveldb.compression snappy leveldb.cache_size 8MB leveldb.filter_bits 10 leveldb.block_size 2KB leveldb.block_restart_interval 16 Conclusion and discussion We selected the best option by properly increasing the buffer size, file size, and cache size to 32MB, 8MB, and 8MB, respectively, and the block size to half 2KB. It was clear that writing performance improved. However, it was predicted that read performance would improve if the cache size was increased to increase the hit rate and the block size was reduced to increase the items included in the index, but read performance did not improve significantly. There may be several factors for this reason, but the unpredictable factor is believed to be the main cause because data is accessed randomly. Using one-third of the memory as a cache is good in terms of tradeoff, which can leave a large amount of OS page cache, so it is expensive to avoid memory budgeting, but in terms of performance, it is expected that the performance will be better if the size is increased by utilizing the remaining cache well.","title":"(Rank 6) Team Compaction"},{"location":"tuning/Tuning_team_Compaction_report/#team-compaction-ycsb-tuning-report","text":"","title":"[Team Compaction] YCSB Tuning Report"},{"location":"tuning/Tuning_team_Compaction_report/#db-options-for-tunning","text":"options default Explanation leveldb.write_buffer_size 2MB single memtable size leveldb.max_file_size 4MB sstable size leveldb.compression snappy compression method leveldb.cache_size 4M cache size leveldb.filter_bits 10 number of filter block bits leveldb.block_size 4KB size of data block in one file leveldb.block_restart_interval 16 number of keys between restart points(delta encoding)","title":"DB options for tunning"},{"location":"tuning/Tuning_team_Compaction_report/#analyze-workloads","text":"A: Read/update ratio: 50/50 B: Read/update ratio: 95/5 D: Read/update/insert ratio: 95/0/5 Because of the high read proportion, consideration is given to ways to maximize read performance. Of course, consider write performance and choose the best option.","title":"Analyze workloads"},{"location":"tuning/Tuning_team_Compaction_report/#hypothesis-and-experiment","text":"Considering that it is run at random, it is measured with an average of 3 times.","title":"Hypothesis and experiment"},{"location":"tuning/Tuning_team_Compaction_report/#hypothesis","text":"Based on what was studied in the study, it is expected that writing performance will increase as the size of the buffer increases the amount of writing at once. In addition, it is expected that read performance will improve if the size of the file is 1/4 of the size of the buffer, allowing four files, which are the thresholds of level0, to be read at once, and increasing the cache size. Finally, by reducing the size of the block, increasing the number of bits in the index of the file, and increasing the number of bits in the filter block, the read performance is expected to be improved","title":"Hypothesis"},{"location":"tuning/Tuning_team_Compaction_report/#default","text":"workload runtime(sec) throughput(ops/sec) load 6.22135 16073.7 A 2.88199 34698.2 B 0.753103 132784 D 0.545697 183252","title":"Default"},{"location":"tuning/Tuning_team_Compaction_report/#write_buffer_size-8mb","text":"workload runtime(sec) throughput(ops/sec) -> runtime(sec) throughput(ops/sec) load 6.22135 16073.7 -> 4.58351 21817.4 A 2.88199 34698.2 -> 2.30625 43360.4 B 0.753103 132784 -> 0.738974 135323 D 0.545697 183252 -> 0.50175 199302 As the write performance improved, the performance of load and A improved.","title":"write_buffer_size 8MB"},{"location":"tuning/Tuning_team_Compaction_report/#write_buffer_size-32mb-max_file_size-8mb","text":"workload runtime(sec) throughput(ops/sec) -> runtime(sec) throughput(ops/sec) load 6.22135 16073.7 -> 1.33948 74655.9 A 2.88199 34698.2 -> 1.44349 69276.5 B 0.753103 132784 -> 0.708532 141137 D 0.545697 183252 -> 0.502773 198897 The buffer size was further increased to 32MB(no performance changes after increasing more than 32MB), and the file size was 8MB, so that only Level 0 could be filled first, and it could be seen that the write performance was improved.","title":"write_buffer_size 32MB, max_file_size 8MB"},{"location":"tuning/Tuning_team_Compaction_report/#cacahe_size-8mb-block_size-2kb","text":"workload runtime(sec) throughput(ops/sec) -> runtime(sec) throughput(ops/sec) load 6.22135 16073.7 -> 1.33054 75157.5 A 2.88199 34698.2 -> 1.38317 72297.5 B 0.753103 132784 -> 0.690946 144729 D 0.545697 183252 -> 0.484127 206557 For read performance, the cache size was increased and the block size was reduced. (More cache than 8MB or less block size makes little difference in performance) It was found that the performance of workload B and C, which had a high reading proportion, improved slightly. Here, increasing the filter block and changing block_restart_interval does not cause any worse performance or make any difference.","title":"cacahe_size 8MB, block_size 2KB"},{"location":"tuning/Tuning_team_Compaction_report/#therefore-the-best-options","text":"the best options leveldb.write_buffer_size 32MB leveldb.max_file_size 8MB leveldb.compression snappy leveldb.cache_size 8MB leveldb.filter_bits 10 leveldb.block_size 2KB leveldb.block_restart_interval 16","title":"Therefore, the best options"},{"location":"tuning/Tuning_team_Compaction_report/#conclusion-and-discussion","text":"We selected the best option by properly increasing the buffer size, file size, and cache size to 32MB, 8MB, and 8MB, respectively, and the block size to half 2KB. It was clear that writing performance improved. However, it was predicted that read performance would improve if the cache size was increased to increase the hit rate and the block size was reduced to increase the items included in the index, but read performance did not improve significantly. There may be several factors for this reason, but the unpredictable factor is believed to be the main cause because data is accessed randomly. Using one-third of the memory as a cache is good in terms of tradeoff, which can leave a large amount of OS page cache, so it is expensive to avoid memory budgeting, but in terms of performance, it is expected that the performance will be better if the size is increased by utilizing the remaining cache well.","title":"Conclusion and discussion"},{"location":"tuning/Tuning_team_SSTable_report/","text":"[Team SSTable] Tuning Report write_buffer_size Increasing this option improves write performance, thus speeding up bulk loads. However, this comes with a trade-off as the time to search for a desired key in the MemTable may increase. Nevertheless, since the search time in a Skiplist is O(log n), the time taken for searching is not significantly large compared to the increase in write_buffer size. Moreover, searching in memory is faster than searching on disk, so we decided to increase the write_buffer_size . max_file_size This option concerns the size of the SSTable. If this is smaller than the write_buffer_size, more SSTables will be created through minor compaction, which is disadvantageous for write performance. Additionally, if max_file_size is less than a quarter of write_buffer_size , level 0 compaction may occur every time a minor compaction happens, leading to performance degradation. Therefore, we decided to set max_file_size to be larger than a quarter of write_buffer_size , and thought it appropriate to set it to about half of write_buffer_size . compression According to the content in options.h , it is generally better to use snappy, so we left it as is. cache_size A larger cache size results in more hits, improving read performance. Especially for workload D, which involves reading recent data frequently, increasing the cache size can lead to significant performance improvements. However, we cannot increase the cache size indefinitely, so we considered how large it should be. The cache size of the environment (school server) where the experiment is run is 14080KB, so we decided to set the cache size accordingly. filter_bits Whether to use a Bloom Filter Not using a Bloom Filter improves write performance, while using it enhances read performance. However, based on our team's experimental results, the improvement in write performance without a Bloom Filter was negligible. Since LSM-Tree is inherently optimized for writing, we decided that enhancing read performance is more beneficial and chose to use a Bloom Filter. How many bits to use per key Referring to the previous presentation by the Bloom Filter team, we found that using 10 bits yielded the most optimal speed. Therefore, we decided to keep it at 10 bits. block_size According to our team's analysis, when creating an SSTable, Data Blocks are written directly to disk, while other blocks, including Filter Blocks, are gathered in a buffer and written at once. Thus, if the number of Data Blocks within an SSTable is small, the disk I/O required to create that SSTable can be reduced, potentially improving write performance. However, this comes with a trade-off of potentially degrading read performance. Since the process of finding a key within an SSTable uses binary search, and the time complexity of binary search is O(log n), we thought the increase in block size would not significantly increase search time. Therefore, we decided to increase the block_size. block_restart_interval Through block_restart_interval , key-value pairs within a Data Block form a kind of section. Analyzing the code, we observed that the target key is found using binary search for the section, and linear search within the section. Therefore, we judged that setting this option smaller is advantageous for reading and decided to make it smaller. Although this may use more space, we decided not to consider this as long as runtime and throughput are the only concerns. Result The average values obtained by entering each command three times are as follows. Default set leveldb.write_buffer_size=2097152 leveldb.max_file_size=4194304 leveldb.compression=snappy leveldb.cache_size=4194304 leveldb.filter_bits=10 leveldb.block_size=4096 leveldb.block_restart_interval=16 Load A Average Load Runtime(sec): 4.4636 Average Load Throughput(ops/sec): 22598.2 Run A Average Run Runtime(sec): 1.98984 Average Run Throughput(ops/sec): 51221.6 Run B Average Run Runtime(sec): 0.60909 Average Run Throughput(ops/sec): 164452.7 Run D Average Run Runtime(sec): 0.43340 Average Run Throughput(ops/sec): 230799.3 Our Best set leveldb.write_buffer_size=67108864 leveldb.max_file_size=33554432 leveldb.compression=snappy leveldb.cache_size=14417920 leveldb.filter_bits=10 leveldb.block_size=2097152 leveldb.block_restart_interval=4 Load A Average Load Runtime(sec): 1.32495 Average Load Throughput(ops/sec): 77013.1 Run A Average Run Runtime(sec): 1.16803 Average Run Throughput(ops/sec): 85921.1 Run B Average Run Runtime(sec): 0.55706 Average Run Throughput(ops/sec): 184068 Run D Average Run Runtime(sec): 0.42554 Average Run Throughput(ops/sec): 240711.3","title":"(Rank 1) Team SSTable"},{"location":"tuning/Tuning_team_SSTable_report/#team-sstable-tuning-report","text":"","title":"[Team SSTable] Tuning Report"},{"location":"tuning/Tuning_team_SSTable_report/#write_buffer_size","text":"Increasing this option improves write performance, thus speeding up bulk loads. However, this comes with a trade-off as the time to search for a desired key in the MemTable may increase. Nevertheless, since the search time in a Skiplist is O(log n), the time taken for searching is not significantly large compared to the increase in write_buffer size. Moreover, searching in memory is faster than searching on disk, so we decided to increase the write_buffer_size .","title":"write_buffer_size"},{"location":"tuning/Tuning_team_SSTable_report/#max_file_size","text":"This option concerns the size of the SSTable. If this is smaller than the write_buffer_size, more SSTables will be created through minor compaction, which is disadvantageous for write performance. Additionally, if max_file_size is less than a quarter of write_buffer_size , level 0 compaction may occur every time a minor compaction happens, leading to performance degradation. Therefore, we decided to set max_file_size to be larger than a quarter of write_buffer_size , and thought it appropriate to set it to about half of write_buffer_size .","title":"max_file_size"},{"location":"tuning/Tuning_team_SSTable_report/#compression","text":"According to the content in options.h , it is generally better to use snappy, so we left it as is.","title":"compression"},{"location":"tuning/Tuning_team_SSTable_report/#cache_size","text":"A larger cache size results in more hits, improving read performance. Especially for workload D, which involves reading recent data frequently, increasing the cache size can lead to significant performance improvements. However, we cannot increase the cache size indefinitely, so we considered how large it should be. The cache size of the environment (school server) where the experiment is run is 14080KB, so we decided to set the cache size accordingly.","title":"cache_size"},{"location":"tuning/Tuning_team_SSTable_report/#filter_bits","text":"Whether to use a Bloom Filter Not using a Bloom Filter improves write performance, while using it enhances read performance. However, based on our team's experimental results, the improvement in write performance without a Bloom Filter was negligible. Since LSM-Tree is inherently optimized for writing, we decided that enhancing read performance is more beneficial and chose to use a Bloom Filter. How many bits to use per key Referring to the previous presentation by the Bloom Filter team, we found that using 10 bits yielded the most optimal speed. Therefore, we decided to keep it at 10 bits.","title":"filter_bits"},{"location":"tuning/Tuning_team_SSTable_report/#block_size","text":"According to our team's analysis, when creating an SSTable, Data Blocks are written directly to disk, while other blocks, including Filter Blocks, are gathered in a buffer and written at once. Thus, if the number of Data Blocks within an SSTable is small, the disk I/O required to create that SSTable can be reduced, potentially improving write performance. However, this comes with a trade-off of potentially degrading read performance. Since the process of finding a key within an SSTable uses binary search, and the time complexity of binary search is O(log n), we thought the increase in block size would not significantly increase search time. Therefore, we decided to increase the block_size.","title":"block_size"},{"location":"tuning/Tuning_team_SSTable_report/#block_restart_interval","text":"Through block_restart_interval , key-value pairs within a Data Block form a kind of section. Analyzing the code, we observed that the target key is found using binary search for the section, and linear search within the section. Therefore, we judged that setting this option smaller is advantageous for reading and decided to make it smaller. Although this may use more space, we decided not to consider this as long as runtime and throughput are the only concerns.","title":"block_restart_interval"},{"location":"tuning/Tuning_team_SSTable_report/#result","text":"The average values obtained by entering each command three times are as follows.","title":"Result"},{"location":"tuning/Tuning_team_SSTable_report/#default-set","text":"leveldb.write_buffer_size=2097152 leveldb.max_file_size=4194304 leveldb.compression=snappy leveldb.cache_size=4194304 leveldb.filter_bits=10 leveldb.block_size=4096 leveldb.block_restart_interval=16 Load A Average Load Runtime(sec): 4.4636 Average Load Throughput(ops/sec): 22598.2 Run A Average Run Runtime(sec): 1.98984 Average Run Throughput(ops/sec): 51221.6 Run B Average Run Runtime(sec): 0.60909 Average Run Throughput(ops/sec): 164452.7 Run D Average Run Runtime(sec): 0.43340 Average Run Throughput(ops/sec): 230799.3","title":"Default set"},{"location":"tuning/Tuning_team_SSTable_report/#our-best-set","text":"leveldb.write_buffer_size=67108864 leveldb.max_file_size=33554432 leveldb.compression=snappy leveldb.cache_size=14417920 leveldb.filter_bits=10 leveldb.block_size=2097152 leveldb.block_restart_interval=4 Load A Average Load Runtime(sec): 1.32495 Average Load Throughput(ops/sec): 77013.1 Run A Average Run Runtime(sec): 1.16803 Average Run Throughput(ops/sec): 85921.1 Run B Average Run Runtime(sec): 0.55706 Average Run Throughput(ops/sec): 184068 Run D Average Run Runtime(sec): 0.42554 Average Run Throughput(ops/sec): 240711.3","title":"Our Best set"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/","text":"[Team WAL/Manifest] Tuning Report Hypothesis write_buffer_size Increasing the write_buffer_size improves performance during bulk loads. Since max_memory_size (= write_buffer_size + block_cache_size) <= 1GB, the performance of Load is expected to be maximized at 1GB. WorkloadA: Read(50) + Modify(50) WorkloadB: Read(95) + Modify(5) WorkloadD: Read(95) + Modify(5) Therefore, as all workloads involve modifications, increasing the write_buffer_size is expected to improve performance in LoadA, RunA, RunB, and RunD. max_file_size Increasing the maximum file size may lengthen compaction time, potentially slowing latency. It is important to find a suitable value for the file system. If the file system is efficient with relatively large files, the file size can be increased. block_size This option sets the approximate size of packed data per block. Since the block size here refers to uncompressed data, it was anticipated that smaller sizes would be advantageous for performance due to faster compression. block_restart_interval The exact function of this option is not well understood, but // Most clients should leave this parameter alone. As recommended by leveldb, most clients do not change this parameter, so experiments were conducted accordingly. block_cache_size Generally, a larger cache size is thought to increase the hit rate. However, due to the cost of cache and other factors, the performance may not scale linearly with size. compression The compression options are kNoCompression and kSnappyCompression. // Note that these speeds are significantly faster than most // persistent storage speeds, and therefore it is typically never // worth switching to kNoCompression. Even if the input data is // incompressible, the kSnappyCompression implementation will // efficiently detect that and will switch to uncompressed mode. According to the description of leveldb's compression options, there is no need to switch to kNoCompression, and performance is expected to be better with kSnappyCompression. filter_policy The default is set to NewBloomFilterPolicy, and based on the bloomfilter team's benchmark presentation, it was reported to be most efficient at 10 bits, so experiments were conducted without changing this setting. Experiments write_buffer_size Experiments were conducted on three different servers with 64, 128, 256, and 512MB. It was found that there was no significant change beyond 64MB. max_file_size Experiments were conducted by doubling the default max_file_size value of 2MB on three different server environments, testing from 64MB to 1GB. block_size Experiments were conducted by setting multiple values, doubling or halving from the default value of 4kB. block_restart_interval Experiments were conducted with the default value of 16. block_cache_size The goal was to find the value at which performance no longer improved by increasing the size as much as possible. compression Experiments were conducted with both kNoCompression and kSnappyCompression, but there was no significant difference, and performance did not improve with kNoCompression. filter_policy Experiments were conducted with 10 bits while changing other options. Results write_buffer_size It seems that beyond a certain value, it is fixed to a predetermined value. Although it needs to be confirmed whether different servers affect the results, the average value was taken as the result since the same results were not obtained on all three servers. max_file_size There was no significant difference beyond 64MB, and since larger file sizes can generally be detrimental to performance, 64MB was chosen as the result among similar values. block_size In our experiments, consistent and good results were observed at 8kB, so 8kB was chosen as the result. block_restart_interval 16 block_cache_size Performance did not improve beyond 128MB, and repeated experiments confirmed high performance, so 128MB was chosen as the result. compression kSnappyCompression filter_policy 10 bits","title":"(Rank 3) Team WAL/Manifest"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#team-walmanifest-tuning-report","text":"","title":"[Team WAL/Manifest] Tuning Report"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#hypothesis","text":"","title":"Hypothesis"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#write_buffer_size","text":"Increasing the write_buffer_size improves performance during bulk loads. Since max_memory_size (= write_buffer_size + block_cache_size) <= 1GB, the performance of Load is expected to be maximized at 1GB. WorkloadA: Read(50) + Modify(50) WorkloadB: Read(95) + Modify(5) WorkloadD: Read(95) + Modify(5) Therefore, as all workloads involve modifications, increasing the write_buffer_size is expected to improve performance in LoadA, RunA, RunB, and RunD.","title":"write_buffer_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#max_file_size","text":"Increasing the maximum file size may lengthen compaction time, potentially slowing latency. It is important to find a suitable value for the file system. If the file system is efficient with relatively large files, the file size can be increased.","title":"max_file_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#block_size","text":"This option sets the approximate size of packed data per block. Since the block size here refers to uncompressed data, it was anticipated that smaller sizes would be advantageous for performance due to faster compression.","title":"block_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#block_restart_interval","text":"The exact function of this option is not well understood, but // Most clients should leave this parameter alone. As recommended by leveldb, most clients do not change this parameter, so experiments were conducted accordingly.","title":"block_restart_interval"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#block_cache_size","text":"Generally, a larger cache size is thought to increase the hit rate. However, due to the cost of cache and other factors, the performance may not scale linearly with size.","title":"block_cache_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#compression","text":"The compression options are kNoCompression and kSnappyCompression. // Note that these speeds are significantly faster than most // persistent storage speeds, and therefore it is typically never // worth switching to kNoCompression. Even if the input data is // incompressible, the kSnappyCompression implementation will // efficiently detect that and will switch to uncompressed mode. According to the description of leveldb's compression options, there is no need to switch to kNoCompression, and performance is expected to be better with kSnappyCompression.","title":"compression"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#filter_policy","text":"The default is set to NewBloomFilterPolicy, and based on the bloomfilter team's benchmark presentation, it was reported to be most efficient at 10 bits, so experiments were conducted without changing this setting.","title":"filter_policy"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#experiments","text":"","title":"Experiments"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#write_buffer_size_1","text":"Experiments were conducted on three different servers with 64, 128, 256, and 512MB. It was found that there was no significant change beyond 64MB.","title":"write_buffer_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#max_file_size_1","text":"Experiments were conducted by doubling the default max_file_size value of 2MB on three different server environments, testing from 64MB to 1GB.","title":"max_file_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#block_size_1","text":"Experiments were conducted by setting multiple values, doubling or halving from the default value of 4kB.","title":"block_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#block_restart_interval_1","text":"Experiments were conducted with the default value of 16.","title":"block_restart_interval"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#block_cache_size_1","text":"The goal was to find the value at which performance no longer improved by increasing the size as much as possible.","title":"block_cache_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#compression_1","text":"Experiments were conducted with both kNoCompression and kSnappyCompression, but there was no significant difference, and performance did not improve with kNoCompression.","title":"compression"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#filter_policy_1","text":"Experiments were conducted with 10 bits while changing other options.","title":"filter_policy"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#results","text":"","title":"Results"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#write_buffer_size_2","text":"It seems that beyond a certain value, it is fixed to a predetermined value. Although it needs to be confirmed whether different servers affect the results, the average value was taken as the result since the same results were not obtained on all three servers.","title":"write_buffer_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#max_file_size_2","text":"There was no significant difference beyond 64MB, and since larger file sizes can generally be detrimental to performance, 64MB was chosen as the result among similar values.","title":"max_file_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#block_size_2","text":"In our experiments, consistent and good results were observed at 8kB, so 8kB was chosen as the result.","title":"block_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#block_restart_interval_2","text":"16","title":"block_restart_interval"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#block_cache_size_2","text":"Performance did not improve beyond 128MB, and repeated experiments confirmed high performance, so 128MB was chosen as the result.","title":"block_cache_size"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#compression_2","text":"kSnappyCompression","title":"compression"},{"location":"tuning/Tuning_team_WAL%2CManifest_report/#filter_policy_2","text":"10 bits","title":"filter_policy"},{"location":"tuning/Tuning_team_bloomfilter_report/","text":"[Team BloomFilter] YCSB Tuning Report 1) Write Buffer Size Hypothesis: Increasing the write buffer size is expected to improve write performance. However, if increased excessively, it may lead to significant overhead, resulting in performance degradation. The goal is to maximize the size while minimizing performance loss to find the optimal value. Measurement: Using the original value of 64MB, the time taken for LOAD was mostly similar, and the LOAD and RUN times were proportional. Increasing it to 128MB significantly reduced the LOAD time, and the LOAD and RUN times were no longer proportional. Runtime either increased or decreased compared to the original, with decreases being more significant. This option had the most impact on performance changes, and efforts were made to minimize cases where time increased using other options. Increasing beyond 128MB showed no significant difference or even reduced performance. 2) Max File Size Hypothesis: Limiting the maximum file size is expected to improve performance, but it may impose restrictions on database usage. Measurement: Changing the value multiple times did not yield significant differences in measurements. 3) Cache Size Hypothesis: Since cache memory is high-speed, increasing the cache size is expected to enhance performance. Measurement: Contrary to expectations, doubling the original value of 128MB decreased performance, while reducing it to 64MB increased performance. A cache size of 64MB is sufficient, and increasing beyond this seems to increase overhead. 4) Filter Bits Hypothesis: The default Bloom filter bit value is 10 bits, with the optimal number of hashes being 10 * 0.69 = 6.9. However, since the actual value used is 6, using 9 filter bits with 6 hashes is hypothesized to reduce false positives and improve performance. Measurement: Reducing the bits from 10 to 9 resulted in slightly more consistent values, though not significantly different. Increasing the max file size made the results irregular, and significantly increasing filter bits did not have a major effect. 5) Block Size Hypothesis: The block size is considered to have a clear tradeoff, and the default value is assumed to be the most appropriate. Measurement: No significant differences were observed, but the default value generally showed the best performance. 6) Block Restart Interval Hypothesis: Reducing the interval between blocks is expected to improve performance. Measurement: Reducing the value showed a slight tendency to improve performance. Final Results Options Result Write Buffer Size 128MB Max File Size 64MB Cache Size 64MB Filter Bits 9 Block Size 8KB Block Restart Interval 4","title":"(Rank 1) Team Bloom Filter"},{"location":"tuning/Tuning_team_bloomfilter_report/#team-bloomfilter-ycsb-tuning-report","text":"","title":"[Team BloomFilter] YCSB Tuning Report"},{"location":"tuning/Tuning_team_bloomfilter_report/#1-write-buffer-size","text":"Hypothesis: Increasing the write buffer size is expected to improve write performance. However, if increased excessively, it may lead to significant overhead, resulting in performance degradation. The goal is to maximize the size while minimizing performance loss to find the optimal value. Measurement: Using the original value of 64MB, the time taken for LOAD was mostly similar, and the LOAD and RUN times were proportional. Increasing it to 128MB significantly reduced the LOAD time, and the LOAD and RUN times were no longer proportional. Runtime either increased or decreased compared to the original, with decreases being more significant. This option had the most impact on performance changes, and efforts were made to minimize cases where time increased using other options. Increasing beyond 128MB showed no significant difference or even reduced performance.","title":"1) Write Buffer Size"},{"location":"tuning/Tuning_team_bloomfilter_report/#2-max-file-size","text":"Hypothesis: Limiting the maximum file size is expected to improve performance, but it may impose restrictions on database usage. Measurement: Changing the value multiple times did not yield significant differences in measurements.","title":"2) Max File Size"},{"location":"tuning/Tuning_team_bloomfilter_report/#3-cache-size","text":"Hypothesis: Since cache memory is high-speed, increasing the cache size is expected to enhance performance. Measurement: Contrary to expectations, doubling the original value of 128MB decreased performance, while reducing it to 64MB increased performance. A cache size of 64MB is sufficient, and increasing beyond this seems to increase overhead.","title":"3) Cache Size"},{"location":"tuning/Tuning_team_bloomfilter_report/#4-filter-bits","text":"Hypothesis: The default Bloom filter bit value is 10 bits, with the optimal number of hashes being 10 * 0.69 = 6.9. However, since the actual value used is 6, using 9 filter bits with 6 hashes is hypothesized to reduce false positives and improve performance. Measurement: Reducing the bits from 10 to 9 resulted in slightly more consistent values, though not significantly different. Increasing the max file size made the results irregular, and significantly increasing filter bits did not have a major effect.","title":"4) Filter Bits"},{"location":"tuning/Tuning_team_bloomfilter_report/#5-block-size","text":"Hypothesis: The block size is considered to have a clear tradeoff, and the default value is assumed to be the most appropriate. Measurement: No significant differences were observed, but the default value generally showed the best performance.","title":"5) Block Size"},{"location":"tuning/Tuning_team_bloomfilter_report/#6-block-restart-interval","text":"Hypothesis: Reducing the interval between blocks is expected to improve performance. Measurement: Reducing the value showed a slight tendency to improve performance.","title":"6) Block Restart Interval"},{"location":"tuning/Tuning_team_bloomfilter_report/#final-results","text":"Options Result Write Buffer Size 128MB Max File Size 64MB Cache Size 64MB Filter Bits 9 Block Size 8KB Block Restart Interval 4","title":"Final Results"},{"location":"tuning/Tuning_team_cache_report/","text":"[Team Cache] YCSB Tuning Report Hypothesis and Experiment Options Default Options Restrictions: Maximum memory size (write_buffer_size + block_cache_size) must be \u2264 1GB Maximum file size must be \u2264 1GB leveldb.max_open_files=10000 LevelDB Options: leveldb.dbname=/tmp/ycsb-leveldb leveldb.format=single leveldb.destroy=false leveldb.write_buffer_size=2097152 leveldb.max_file_size=4194304 leveldb.compression=snappy leveldb.cache_size=4194304 leveldb.filter_bits=10 leveldb.block_size=4096 leveldb.block_restart_interval=16 Experiment Options Different shell scripts were created for each workload to handle varying read and write conditions, and experiments were conducted accordingly. YCSB Workload: Record Count: 2,000,000 Operation Count: 2,000,000 Workload Files: Workload A, Workload B, Workload D Sequence: Load A -> Run A -> Run B -> Run D Workload Details: Workload A: Read/update ratio: 50/50 Workload B: Read/update ratio: 95/5 Workload D: Read/update/insert ratio: 95/0/5 echo \"ex 1\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties1 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties2 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties3 -s\" RESULT=$($CMD) echo \"$RESULT\" echo \"ex 2\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties1 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties2 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties3 -s\" RESULT=$($CMD) echo \"$RESULT\" echo \"ex 3\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties1 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties2 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties3 -s\" RESULT=$($CMD) echo \"$RESULT\" echo \"ex 4\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties1 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties2 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties3 -s\" RESULT=$($CMD) echo \"$RESULT\" The results are the averages of the results from each experiment. write_buffer_size The value was increased by a factor of 2, and the point at which performance no longer improved was found. max_file_size The value was increased along with write_buffer_size, and the optimal option was selected. cache_size While no significant performance differences were found with changes in other options, the optimal option was selected by repeating experiments multiple times and averaging the results. block_size Both write and read performance were compared and measured. block_restart_interval The number of keys between restart points for delta encoding. Result Result leveldb.write_buffer_size 47.68MB leveldb.max_file_size 4MB leveldb.compression snappy leveldb.cache_size 40MB leveldb.filter_bits 10 leveldb.block_size 8KB leveldb.block_restart_interval 32","title":"(Rank 5) Team Cache"},{"location":"tuning/Tuning_team_cache_report/#team-cache-ycsb-tuning-report","text":"","title":"[Team Cache] YCSB Tuning Report"},{"location":"tuning/Tuning_team_cache_report/#hypothesis-and-experiment-options","text":"","title":"Hypothesis and Experiment Options"},{"location":"tuning/Tuning_team_cache_report/#default-options","text":"Restrictions: Maximum memory size (write_buffer_size + block_cache_size) must be \u2264 1GB Maximum file size must be \u2264 1GB leveldb.max_open_files=10000 LevelDB Options: leveldb.dbname=/tmp/ycsb-leveldb leveldb.format=single leveldb.destroy=false leveldb.write_buffer_size=2097152 leveldb.max_file_size=4194304 leveldb.compression=snappy leveldb.cache_size=4194304 leveldb.filter_bits=10 leveldb.block_size=4096 leveldb.block_restart_interval=16","title":"Default Options"},{"location":"tuning/Tuning_team_cache_report/#experiment-options","text":"Different shell scripts were created for each workload to handle varying read and write conditions, and experiments were conducted accordingly. YCSB Workload: Record Count: 2,000,000 Operation Count: 2,000,000 Workload Files: Workload A, Workload B, Workload D Sequence: Load A -> Run A -> Run B -> Run D Workload Details: Workload A: Read/update ratio: 50/50 Workload B: Read/update ratio: 95/5 Workload D: Read/update/insert ratio: 95/0/5 echo \"ex 1\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties1 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties2 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties3 -s\" RESULT=$($CMD) echo \"$RESULT\" echo \"ex 2\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties1 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties2 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties3 -s\" RESULT=$($CMD) echo \"$RESULT\" echo \"ex 3\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties1 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties2 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties3 -s\" RESULT=$($CMD) echo \"$RESULT\" echo \"ex 4\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties1 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties2 -s\" RESULT=$($CMD) echo \"$RESULT\" CMD=\"./ycsb -run -db leveldb -P workloads/workloada -P leveldb/leveldb.properties3 -s\" RESULT=$($CMD) echo \"$RESULT\" The results are the averages of the results from each experiment.","title":"Experiment Options"},{"location":"tuning/Tuning_team_cache_report/#write_buffer_size","text":"The value was increased by a factor of 2, and the point at which performance no longer improved was found.","title":"write_buffer_size"},{"location":"tuning/Tuning_team_cache_report/#max_file_size","text":"The value was increased along with write_buffer_size, and the optimal option was selected.","title":"max_file_size"},{"location":"tuning/Tuning_team_cache_report/#cache_size","text":"While no significant performance differences were found with changes in other options, the optimal option was selected by repeating experiments multiple times and averaging the results.","title":"cache_size"},{"location":"tuning/Tuning_team_cache_report/#block_size","text":"Both write and read performance were compared and measured.","title":"block_size"},{"location":"tuning/Tuning_team_cache_report/#block_restart_interval","text":"The number of keys between restart points for delta encoding.","title":"block_restart_interval"},{"location":"tuning/Tuning_team_cache_report/#result","text":"Result leveldb.write_buffer_size 47.68MB leveldb.max_file_size 4MB leveldb.compression snappy leveldb.cache_size 40MB leveldb.filter_bits 10 leveldb.block_size 8KB leveldb.block_restart_interval 32","title":"Result"},{"location":"tuning/Tuning_team_memtable_report/","text":"[Team Memtable] YCSB Tuning Report Configuration Options Overview leveldb.write_buffer_size Controls the size of memtable & immutable memtable Default: 33554432 (32MB) Impact: Larger size improves write performance However, increases recovery time during DB restart due to WAL replay Recommended to adjust based on your write patterns leveldb.max_file_size Determines maximum size of a single SSTable file Default: 16777216 (16MB) Considerations: Should be proportional to key/value sizes Larger values may increase read/write amplification Need to balance between performance and resource usage leveldb.compression Uses Snappy compression algorithm Characteristics: Good performance-to-compression ratio Popular choice for databases Trade-off between CPU usage and storage space Recommendations: Enable for large datasets Disable for small datasets where CPU is more critical leveldb.cache_size Controls block cache size Default: 33554432 (32MB) Effects: Larger cache improves read performance Increases memory usage Caches blocks to reduce disk I/O leveldb.block_size Basic unit for read/write operations Default: 8192 (8KB) Should be configured according to key/value sizes leveldb.block_restart_interval Default: 16 Controls prefix compression interval for keys Determines how many keys share prefix compression Performance Test Results Test Timeline Start: 2022-08-15 20:13:09 (0 operations) End: 2022-08-15 20:13:10 (100,000 operations) Performance Metrics Total Runtime: 0.831485 seconds Total Operations: 100,000 Throughput: 120,267 ops/sec Operation Breakdown READ Operations Count: 49,912 Maximum Latency: 4,231.61ms Minimum Latency: 1.14ms Average Latency: 4.24ms UPDATE Operations Count: 50,088 Maximum Latency: 3,125.45ms Minimum Latency: 3.94ms Average Latency: 8.25ms Configuration Settings Note: Options are configured for a small database due to the limited number of operations. Database Parameters leveldb.write_buffer_size=33554432 leveldb.max_file_size=16777216 leveldb.compression=snappy leveldb.cache_size=33554432 leveldb.filter_bits=10 leveldb.block_size=8192 leveldb.block_restart_interval=16","title":"(Rank 4) Team Memtable"},{"location":"tuning/Tuning_team_memtable_report/#team-memtable-ycsb-tuning-report","text":"","title":"[Team Memtable] YCSB Tuning Report"},{"location":"tuning/Tuning_team_memtable_report/#configuration-options-overview","text":"","title":"Configuration Options Overview"},{"location":"tuning/Tuning_team_memtable_report/#leveldbwrite_buffer_size","text":"Controls the size of memtable & immutable memtable Default: 33554432 (32MB) Impact: Larger size improves write performance However, increases recovery time during DB restart due to WAL replay Recommended to adjust based on your write patterns","title":"leveldb.write_buffer_size"},{"location":"tuning/Tuning_team_memtable_report/#leveldbmax_file_size","text":"Determines maximum size of a single SSTable file Default: 16777216 (16MB) Considerations: Should be proportional to key/value sizes Larger values may increase read/write amplification Need to balance between performance and resource usage","title":"leveldb.max_file_size"},{"location":"tuning/Tuning_team_memtable_report/#leveldbcompression","text":"Uses Snappy compression algorithm Characteristics: Good performance-to-compression ratio Popular choice for databases Trade-off between CPU usage and storage space Recommendations: Enable for large datasets Disable for small datasets where CPU is more critical","title":"leveldb.compression"},{"location":"tuning/Tuning_team_memtable_report/#leveldbcache_size","text":"Controls block cache size Default: 33554432 (32MB) Effects: Larger cache improves read performance Increases memory usage Caches blocks to reduce disk I/O","title":"leveldb.cache_size"},{"location":"tuning/Tuning_team_memtable_report/#leveldbblock_size","text":"Basic unit for read/write operations Default: 8192 (8KB) Should be configured according to key/value sizes","title":"leveldb.block_size"},{"location":"tuning/Tuning_team_memtable_report/#leveldbblock_restart_interval","text":"Default: 16 Controls prefix compression interval for keys Determines how many keys share prefix compression","title":"leveldb.block_restart_interval"},{"location":"tuning/Tuning_team_memtable_report/#performance-test-results","text":"","title":"Performance Test Results"},{"location":"tuning/Tuning_team_memtable_report/#test-timeline","text":"Start: 2022-08-15 20:13:09 (0 operations) End: 2022-08-15 20:13:10 (100,000 operations)","title":"Test Timeline"},{"location":"tuning/Tuning_team_memtable_report/#performance-metrics","text":"Total Runtime: 0.831485 seconds Total Operations: 100,000 Throughput: 120,267 ops/sec","title":"Performance Metrics"},{"location":"tuning/Tuning_team_memtable_report/#operation-breakdown","text":"","title":"Operation Breakdown"},{"location":"tuning/Tuning_team_memtable_report/#read-operations","text":"Count: 49,912 Maximum Latency: 4,231.61ms Minimum Latency: 1.14ms Average Latency: 4.24ms","title":"READ Operations"},{"location":"tuning/Tuning_team_memtable_report/#update-operations","text":"Count: 50,088 Maximum Latency: 3,125.45ms Minimum Latency: 3.94ms Average Latency: 8.25ms","title":"UPDATE Operations"},{"location":"tuning/Tuning_team_memtable_report/#configuration-settings","text":"Note: Options are configured for a small database due to the limited number of operations.","title":"Configuration Settings"},{"location":"tuning/Tuning_team_memtable_report/#database-parameters","text":"leveldb.write_buffer_size=33554432 leveldb.max_file_size=16777216 leveldb.compression=snappy leveldb.cache_size=33554432 leveldb.filter_bits=10 leveldb.block_size=8192 leveldb.block_restart_interval=16","title":"Database Parameters"}]}